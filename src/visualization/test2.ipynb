{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 10:52:01.603175: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-05 10:52:01.629617: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-05 10:52:01.629637: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-05 10:52:01.630418: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-05 10:52:01.634643: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-05 10:52:01.635110: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-05 10:52:07.116567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.models.train_emos import train_emos, train_and_test_emos\n",
    "from pit import make_cpit_diagram_emos, make_cpit_hist_emos \n",
    "from brier_score import brier_skill_plot, brier_plot\n",
    "from src.models.get_data import get_tensors, get_normalized_tensor\n",
    "from src.models.emos import EMOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15733, 5)\n"
     ]
    }
   ],
   "source": [
    "neighbourhood_size = 11\n",
    "parameter_names = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "ignore = ['229', '285', '323']\n",
    "train_folds = [1, 2]\n",
    "train_data = get_normalized_tensor(neighbourhood_size, parameter_names, train_folds, ignore)\n",
    "\n",
    "X_train = train_data['X']\n",
    "y_train = train_data['y']\n",
    "variances_train = train_data['variances']\n",
    "mean_train = train_data['mean']\n",
    "std_train = train_data['std']\n",
    "\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7780, 5)\n"
     ]
    }
   ],
   "source": [
    "test_fold = 3\n",
    "\n",
    "X_test, y_test, variances_test = get_tensors(neighbourhood_size, parameter_names, test_fold, ignore)\n",
    "X_test = (X_test - mean_train) / std_train\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = {}\n",
    "\n",
    "setup[\"num_features\"] = len(parameter_names)\n",
    "setup[\"feature_mean\"] = mean_train\n",
    "setup[\"feature_std\"] = std_train\n",
    "setup[\"features\"] = parameter_names\n",
    "setup[\"neighbourhood_size\"] = neighbourhood_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n"
     ]
    }
   ],
   "source": [
    "setup1 = setup\n",
    "\n",
    "setup1[\"loss\"] = \"loss_CRPS_sample\"\n",
    "setup1[\"samples\"] = 100\n",
    "setup1[\"optimizer\"] = \"Adam\"\n",
    "setup1[\"learning_rate\"] = 0.01\n",
    "setup1[\"forecast_distribution\"] = \"distr_trunc_normal\"\n",
    "\n",
    "trunc_normal_crps = EMOS(setup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.6883008480072021\n",
      "Step: 1, Loss: 1.6662862300872803\n",
      "Step: 2, Loss: 1.6432406902313232\n",
      "Step: 3, Loss: 1.6200432777404785\n",
      "Step: 4, Loss: 1.5990031957626343\n",
      "Step: 5, Loss: 1.5782463550567627\n",
      "Step: 6, Loss: 1.5610617399215698\n",
      "Step: 7, Loss: 1.5437023639678955\n",
      "Step: 8, Loss: 1.5264198780059814\n",
      "Step: 9, Loss: 1.510314702987671\n",
      "Step: 10, Loss: 1.4963035583496094\n",
      "Step: 11, Loss: 1.4833961725234985\n",
      "Step: 12, Loss: 1.4701125621795654\n",
      "Step: 13, Loss: 1.4571635723114014\n",
      "Step: 14, Loss: 1.4432464838027954\n",
      "Step: 15, Loss: 1.4332988262176514\n",
      "Step: 16, Loss: 1.4216610193252563\n",
      "Step: 17, Loss: 1.410881757736206\n",
      "Step: 18, Loss: 1.3993719816207886\n",
      "Step: 19, Loss: 1.3881255388259888\n",
      "Step: 20, Loss: 1.3777860403060913\n",
      "Step: 21, Loss: 1.3673980236053467\n",
      "Step: 22, Loss: 1.3564146757125854\n",
      "Step: 23, Loss: 1.3431036472320557\n",
      "Step: 24, Loss: 1.3342841863632202\n",
      "Step: 25, Loss: 1.3214737176895142\n",
      "Step: 26, Loss: 1.3128485679626465\n",
      "Step: 27, Loss: 1.3033281564712524\n",
      "Step: 28, Loss: 1.2929871082305908\n",
      "Step: 29, Loss: 1.2832266092300415\n",
      "Step: 30, Loss: 1.2721956968307495\n",
      "Step: 31, Loss: 1.2632734775543213\n",
      "Step: 32, Loss: 1.2536817789077759\n",
      "Step: 33, Loss: 1.2443947792053223\n",
      "Step: 34, Loss: 1.235895037651062\n",
      "Step: 35, Loss: 1.2277300357818604\n",
      "Step: 36, Loss: 1.2189908027648926\n",
      "Step: 37, Loss: 1.2087610960006714\n",
      "Step: 38, Loss: 1.2023229598999023\n",
      "Step: 39, Loss: 1.196665644645691\n",
      "Step: 40, Loss: 1.187252402305603\n",
      "Step: 41, Loss: 1.1798471212387085\n",
      "Step: 42, Loss: 1.1707526445388794\n",
      "Step: 43, Loss: 1.1628941297531128\n",
      "Step: 44, Loss: 1.1556118726730347\n",
      "Step: 45, Loss: 1.1484297513961792\n",
      "Step: 46, Loss: 1.1395161151885986\n",
      "Step: 47, Loss: 1.1332924365997314\n",
      "Step: 48, Loss: 1.126422643661499\n",
      "Step: 49, Loss: 1.1185189485549927\n",
      "Step: 50, Loss: 1.1134979724884033\n",
      "Step: 51, Loss: 1.1047574281692505\n",
      "Step: 52, Loss: 1.0996800661087036\n",
      "Step: 53, Loss: 1.091922402381897\n",
      "Step: 54, Loss: 1.086463212966919\n",
      "Step: 55, Loss: 1.0825034379959106\n",
      "Step: 56, Loss: 1.0761717557907104\n",
      "Step: 57, Loss: 1.071563482284546\n",
      "Step: 58, Loss: 1.0652289390563965\n",
      "Step: 59, Loss: 1.05803382396698\n",
      "Step: 60, Loss: 1.0525280237197876\n",
      "Step: 61, Loss: 1.0487180948257446\n",
      "Step: 62, Loss: 1.0438905954360962\n",
      "Step: 63, Loss: 1.0407748222351074\n",
      "Step: 64, Loss: 1.037577748298645\n",
      "Step: 65, Loss: 1.0292795896530151\n",
      "Step: 66, Loss: 1.0256247520446777\n",
      "Step: 67, Loss: 1.0218448638916016\n",
      "Step: 68, Loss: 1.0160893201828003\n",
      "Step: 69, Loss: 1.0124597549438477\n",
      "Step: 70, Loss: 1.0088505744934082\n",
      "Step: 71, Loss: 1.0056170225143433\n",
      "Step: 72, Loss: 0.9993909597396851\n",
      "Step: 73, Loss: 0.9996674060821533\n",
      "Step: 74, Loss: 0.9931091070175171\n",
      "Step: 75, Loss: 0.9905707836151123\n",
      "Step: 76, Loss: 0.9881187677383423\n",
      "Step: 77, Loss: 0.9842132329940796\n",
      "Step: 78, Loss: 0.978948712348938\n",
      "Step: 79, Loss: 0.9779198169708252\n",
      "Step: 80, Loss: 0.9768511056900024\n",
      "Step: 81, Loss: 0.9732064604759216\n",
      "Step: 82, Loss: 0.9679802060127258\n",
      "Step: 83, Loss: 0.968618631362915\n",
      "Step: 84, Loss: 0.9640383124351501\n",
      "Step: 85, Loss: 0.9634473919868469\n",
      "Step: 86, Loss: 0.9613333940505981\n",
      "Step: 87, Loss: 0.9589179754257202\n",
      "Step: 88, Loss: 0.9550699591636658\n",
      "Step: 89, Loss: 0.9532297849655151\n",
      "Step: 90, Loss: 0.9519087672233582\n",
      "Step: 91, Loss: 0.949008047580719\n",
      "Step: 92, Loss: 0.9472055435180664\n",
      "Step: 93, Loss: 0.9442539215087891\n",
      "Step: 94, Loss: 0.9446405172348022\n",
      "Step: 95, Loss: 0.943405032157898\n",
      "Step: 96, Loss: 0.9408349394798279\n",
      "Step: 97, Loss: 0.9381186962127686\n",
      "Step: 98, Loss: 0.9367765784263611\n",
      "Step: 99, Loss: 0.937079906463623\n",
      "Step: 100, Loss: 0.9338374137878418\n",
      "Step: 101, Loss: 0.9335238337516785\n",
      "Step: 102, Loss: 0.9314683079719543\n",
      "Step: 103, Loss: 0.9291380643844604\n",
      "Step: 104, Loss: 0.9303454756736755\n",
      "Step: 105, Loss: 0.9288776516914368\n",
      "Step: 106, Loss: 0.928064227104187\n",
      "Step: 107, Loss: 0.9257048964500427\n",
      "Step: 108, Loss: 0.9269102215766907\n",
      "Step: 109, Loss: 0.9238567352294922\n",
      "Step: 110, Loss: 0.9249820113182068\n",
      "Step: 111, Loss: 0.9218902587890625\n",
      "Step: 112, Loss: 0.9216076135635376\n",
      "Step: 113, Loss: 0.9202118515968323\n",
      "Step: 114, Loss: 0.9209132790565491\n",
      "Step: 115, Loss: 0.9203075766563416\n",
      "Step: 116, Loss: 0.9183975458145142\n",
      "Step: 117, Loss: 0.9186397790908813\n",
      "Step: 118, Loss: 0.9187101721763611\n",
      "Step: 119, Loss: 0.9165839552879333\n",
      "Step: 120, Loss: 0.9183563590049744\n",
      "Step: 121, Loss: 0.9140990972518921\n",
      "Step: 122, Loss: 0.9157315492630005\n",
      "Step: 123, Loss: 0.9161264896392822\n",
      "Step: 124, Loss: 0.916260838508606\n",
      "Step: 125, Loss: 0.9133421182632446\n",
      "Step: 126, Loss: 0.9133545160293579\n",
      "Step: 127, Loss: 0.9135067462921143\n",
      "Step: 128, Loss: 0.9101591110229492\n",
      "Step: 129, Loss: 0.9122625589370728\n",
      "Step: 130, Loss: 0.9103884100914001\n",
      "Step: 131, Loss: 0.9096411466598511\n",
      "Step: 132, Loss: 0.911653459072113\n",
      "Step: 133, Loss: 0.9097642302513123\n",
      "Step: 134, Loss: 0.9088944792747498\n",
      "Step: 135, Loss: 0.909572184085846\n",
      "Step: 136, Loss: 0.9107782244682312\n",
      "Step: 137, Loss: 0.9109931588172913\n",
      "Step: 138, Loss: 0.9122653007507324\n",
      "Step: 139, Loss: 0.9110147953033447\n",
      "Step: 140, Loss: 0.9097083806991577\n",
      "Step: 141, Loss: 0.9092739820480347\n",
      "Step: 142, Loss: 0.9091068506240845\n",
      "Step: 143, Loss: 0.9088169932365417\n",
      "Step: 144, Loss: 0.908433198928833\n",
      "Step: 145, Loss: 0.9099929928779602\n",
      "Step: 146, Loss: 0.9099617600440979\n",
      "Step: 147, Loss: 0.9096108078956604\n",
      "Step: 148, Loss: 0.9098110198974609\n",
      "Step: 149, Loss: 0.9085754156112671\n",
      "Step: 150, Loss: 0.9095516204833984\n",
      "Step: 151, Loss: 0.9096381068229675\n",
      "Step: 152, Loss: 0.9083967804908752\n",
      "Step: 153, Loss: 0.9077576398849487\n",
      "Step: 154, Loss: 0.9060836434364319\n",
      "Step: 155, Loss: 0.9068413376808167\n",
      "Step: 156, Loss: 0.9083045721054077\n",
      "Step: 157, Loss: 0.9071683883666992\n",
      "Step: 158, Loss: 0.9083646535873413\n",
      "Step: 159, Loss: 0.9058911204338074\n",
      "Step: 160, Loss: 0.9069469571113586\n",
      "Step: 161, Loss: 0.9080486297607422\n",
      "Step: 162, Loss: 0.9083678722381592\n",
      "Step: 163, Loss: 0.9076711535453796\n",
      "Step: 164, Loss: 0.9070451259613037\n",
      "Step: 165, Loss: 0.9089861512184143\n",
      "Step: 166, Loss: 0.9076629877090454\n",
      "Step: 167, Loss: 0.906688392162323\n",
      "Step: 168, Loss: 0.9065362811088562\n",
      "Step: 169, Loss: 0.9079749584197998\n",
      "Step: 170, Loss: 0.9086669087409973\n",
      "Step: 171, Loss: 0.9066409468650818\n",
      "Step: 172, Loss: 0.9070297479629517\n",
      "Step: 173, Loss: 0.9080989956855774\n",
      "Step: 174, Loss: 0.9074241518974304\n",
      "Step: 175, Loss: 0.9074966311454773\n",
      "Step: 176, Loss: 0.908650279045105\n",
      "Step: 177, Loss: 0.9081657528877258\n",
      "Step: 178, Loss: 0.905368447303772\n",
      "Step: 179, Loss: 0.9079133868217468\n",
      "Step: 180, Loss: 0.9066424369812012\n",
      "Step: 181, Loss: 0.9065343141555786\n",
      "Step: 182, Loss: 0.9053148031234741\n",
      "Step: 183, Loss: 0.9075140357017517\n",
      "Step: 184, Loss: 0.9046110510826111\n",
      "Step: 185, Loss: 0.9066374897956848\n",
      "Step: 186, Loss: 0.9062640070915222\n",
      "Step: 187, Loss: 0.9063915014266968\n",
      "Step: 188, Loss: 0.9077780842781067\n",
      "Step: 189, Loss: 0.9075442552566528\n",
      "Step: 190, Loss: 0.9063516855239868\n",
      "Step: 191, Loss: 0.9080896377563477\n",
      "Step: 192, Loss: 0.9061620831489563\n",
      "Step: 193, Loss: 0.907393217086792\n",
      "Step: 194, Loss: 0.9063425064086914\n",
      "Step: 195, Loss: 0.907287061214447\n",
      "Step: 196, Loss: 0.9069271683692932\n",
      "Step: 197, Loss: 0.9069901704788208\n",
      "Step: 198, Loss: 0.9066352844238281\n",
      "Step: 199, Loss: 0.9074951410293579\n",
      "Step: 200, Loss: 0.9064194560050964\n",
      "Step: 201, Loss: 0.9062080383300781\n",
      "Step: 202, Loss: 0.9049270749092102\n",
      "Step: 203, Loss: 0.9079447984695435\n",
      "Step: 204, Loss: 0.9072237610816956\n",
      "Step: 205, Loss: 0.9066542387008667\n",
      "Step: 206, Loss: 0.9060074090957642\n",
      "Step: 207, Loss: 0.9061345458030701\n",
      "Step: 208, Loss: 0.9055109620094299\n",
      "Step: 209, Loss: 0.9051041603088379\n",
      "Step: 210, Loss: 0.9060968160629272\n",
      "Step: 211, Loss: 0.9081845879554749\n",
      "Step: 212, Loss: 0.9080870747566223\n",
      "Step: 213, Loss: 0.9068083763122559\n",
      "Step: 214, Loss: 0.9076140522956848\n",
      "Step: 215, Loss: 0.9059402942657471\n",
      "Step: 216, Loss: 0.907383382320404\n",
      "Step: 217, Loss: 0.9049944281578064\n",
      "Step: 218, Loss: 0.9085097312927246\n",
      "Step: 219, Loss: 0.9062745571136475\n",
      "Step: 220, Loss: 0.9066323637962341\n",
      "Step: 221, Loss: 0.9069488644599915\n",
      "Step: 222, Loss: 0.9070555567741394\n",
      "Step: 223, Loss: 0.9059773683547974\n",
      "Step: 224, Loss: 0.9070245027542114\n",
      "Step: 225, Loss: 0.9083942174911499\n",
      "Step: 226, Loss: 0.9070481061935425\n",
      "Step: 227, Loss: 0.907681941986084\n",
      "Step: 228, Loss: 0.9067047834396362\n",
      "Step: 229, Loss: 0.9086537957191467\n",
      "Step: 230, Loss: 0.9070020318031311\n",
      "Step: 231, Loss: 0.9069432020187378\n",
      "Step: 232, Loss: 0.9082178473472595\n",
      "Step: 233, Loss: 0.9074249863624573\n",
      "Step: 234, Loss: 0.9062489867210388\n",
      "Step: 235, Loss: 0.9073326587677002\n",
      "Step: 236, Loss: 0.9083534479141235\n",
      "Step: 237, Loss: 0.9055457711219788\n",
      "Step: 238, Loss: 0.9067409634590149\n",
      "Step: 239, Loss: 0.9046911597251892\n",
      "Step: 240, Loss: 0.9057838320732117\n",
      "Step: 241, Loss: 0.9065688252449036\n",
      "Step: 242, Loss: 0.9074265956878662\n",
      "Step: 243, Loss: 0.9082614779472351\n",
      "Step: 244, Loss: 0.9070658683776855\n",
      "Step: 245, Loss: 0.9070325493812561\n",
      "Step: 246, Loss: 0.9060676097869873\n",
      "Step: 247, Loss: 0.9069624543190002\n",
      "Step: 248, Loss: 0.9065618515014648\n",
      "Step: 249, Loss: 0.9068016409873962\n",
      "Step: 250, Loss: 0.9068683981895447\n",
      "Step: 251, Loss: 0.9076453447341919\n",
      "Step: 252, Loss: 0.9063045978546143\n",
      "Step: 253, Loss: 0.9063704013824463\n",
      "Step: 254, Loss: 0.9069012403488159\n",
      "Step: 255, Loss: 0.9065749049186707\n",
      "Step: 256, Loss: 0.9061461091041565\n",
      "Step: 257, Loss: 0.9068466424942017\n",
      "Step: 258, Loss: 0.9054334163665771\n",
      "Step: 259, Loss: 0.905342161655426\n",
      "Step: 260, Loss: 0.9069742560386658\n",
      "Step: 261, Loss: 0.9065213203430176\n",
      "Step: 262, Loss: 0.9063292145729065\n",
      "Step: 263, Loss: 0.9079030752182007\n",
      "Step: 264, Loss: 0.9063867330551147\n",
      "Step: 265, Loss: 0.9084935784339905\n",
      "Step: 266, Loss: 0.9064860939979553\n",
      "Step: 267, Loss: 0.907300591468811\n",
      "Step: 268, Loss: 0.907686173915863\n",
      "Step: 269, Loss: 0.906562089920044\n",
      "Step: 270, Loss: 0.9075152277946472\n",
      "Step: 271, Loss: 0.9070984125137329\n",
      "Step: 272, Loss: 0.9048060178756714\n",
      "Step: 273, Loss: 0.9063103199005127\n",
      "Step: 274, Loss: 0.906427264213562\n",
      "Step: 275, Loss: 0.9073567390441895\n",
      "Step: 276, Loss: 0.9069697856903076\n",
      "Step: 277, Loss: 0.9086217284202576\n",
      "Step: 278, Loss: 0.9071540236473083\n",
      "Step: 279, Loss: 0.9069341421127319\n",
      "Step: 280, Loss: 0.9063776731491089\n",
      "Step: 281, Loss: 0.9083586931228638\n",
      "Step: 282, Loss: 0.9075623154640198\n",
      "Step: 283, Loss: 0.9071856737136841\n",
      "Step: 284, Loss: 0.9059419631958008\n",
      "Step: 285, Loss: 0.9062219262123108\n",
      "Step: 286, Loss: 0.9047365188598633\n",
      "Step: 287, Loss: 0.9068605899810791\n",
      "Step: 288, Loss: 0.9081482291221619\n",
      "Step: 289, Loss: 0.9074199199676514\n",
      "Step: 290, Loss: 0.9065591096878052\n",
      "Step: 291, Loss: 0.9074273705482483\n",
      "Step: 292, Loss: 0.906760573387146\n",
      "Step: 293, Loss: 0.9070897102355957\n",
      "Step: 294, Loss: 0.9059813618659973\n",
      "Step: 295, Loss: 0.9060482978820801\n",
      "Step: 296, Loss: 0.9057140350341797\n",
      "Step: 297, Loss: 0.9065057039260864\n",
      "Step: 298, Loss: 0.9077387452125549\n",
      "Step: 299, Loss: 0.9054677486419678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=1.6883008>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.6662862>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.6432407>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.6200433>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.5990032>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.5782464>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.5610617>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.5437024>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.5264199>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.5103147>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.4963036>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.4833962>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.4701126>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.4571636>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.4432465>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.4332988>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.421661>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.4108818>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.399372>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.3881255>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.377786>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.367398>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.3564147>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.3431036>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.3342842>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.3214737>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.3128486>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.3033282>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2929871>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2832266>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2721957>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2632735>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2536818>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2443948>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.235895>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.22773>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2189908>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.2087611>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.202323>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1966656>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1872524>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1798471>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1707526>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1628941>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1556119>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1484298>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1395161>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1332924>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1264226>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.118519>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.113498>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.1047574>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0996801>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0919224>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0864632>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0825034>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0761718>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0715635>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0652289>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0580338>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.052528>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0487181>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0438906>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0407748>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0375777>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0292796>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0256248>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0218449>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0160893>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0124598>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0088506>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.005617>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.99939096>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9996674>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9931091>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9905708>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.98811877>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.98421323>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9789487>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9779198>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9768511>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.97320646>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9679802>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.96861863>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9640383>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9634474>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9613334>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.958918>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.95506996>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9532298>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.95190877>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.94900805>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.94720554>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9442539>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9446405>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.94340503>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.94083494>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9381187>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9367766>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9370799>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9338374>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.93352383>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9314683>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.92913806>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9303455>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.92887765>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9280642>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9257049>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9269102>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.92385674>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.924982>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.92189026>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9216076>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.92021185>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9209133>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9203076>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.91839755>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9186398>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9187102>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.91658396>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.91835636>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9140991>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.91573155>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9161265>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.91626084>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9133421>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9133545>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.91350675>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9101591>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.91226256>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9103884>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90964115>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.91165346>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90976423>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9088945>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9095722>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9107782>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.91099316>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9122653>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9110148>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9097084>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.909274>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90910685>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.908817>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9084332>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.909993>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90996176>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9096108>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.909811>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9085754>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9095516>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9096381>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9083968>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90775764>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90608364>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90684134>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9083046>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9071684>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90836465>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9058911>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90694696>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9080486>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9083679>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90767115>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9070451>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90898615>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.907663>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9066884>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9065363>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90797496>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9086669>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90664095>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90702975>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.908099>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90742415>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90749663>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9086503>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90816575>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90536845>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9079134>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90664244>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9065343>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9053148>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90751404>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90461105>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9066375>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.906264>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9063915>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9077781>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90754426>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9063517>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90808964>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9061621>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9073932>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9063425>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90728706>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90692717>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9069902>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9066353>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90749514>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90641946>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90620804>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9049271>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9079448>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90722376>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90665424>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9060074>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90613455>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90551096>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90510416>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9060968>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9081846>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9080871>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9068084>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90761405>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9059403>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9073834>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9049944>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90850973>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90627456>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90663236>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90694886>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90705556>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90597737>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9070245>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9083942>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9070481>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90768194>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9067048>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9086538>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90700203>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9069432>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90821785>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.907425>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.906249>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90733266>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90835345>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9055458>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90674096>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90469116>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90578383>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9065688>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9074266>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9082615>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90706587>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90703255>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9060676>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90696245>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90656185>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90680164>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9068684>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90764534>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9063046>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9063704>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90690124>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9065749>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9061461>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90684664>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9054334>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90534216>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90697426>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9065213>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9063292>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9079031>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90638673>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9084936>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9064861>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9073006>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9076862>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9065621>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9075152>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9070984>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.904806>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9063103>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90642726>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90735674>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9069698>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9086217>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.907154>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90693414>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9063777>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9083587>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9075623>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9071857>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90594196>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9062219>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9047365>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9068606>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9081482>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9074199>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9065591>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9074274>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9067606>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9070897>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90598136>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9060483>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90571404>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9065057>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90773875>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90546775>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 300\n",
    "trunc_normal_crps.fit(X_train, y_train, variances_train, epochs, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 104 values in the PIT histogram\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = 15\n",
    "bins = 10\n",
    "make_cpit_hist_emos(trunc_normal_crps, X_test, y_test, variances_test, t=t, bins = bins)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
