{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.training import train_model, train_and_save, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_distribution = 'distr_mixture_linear'\n",
    "distribution_1 = 'distr_trunc_normal'\n",
    "distribution_2 = 'distr_log_normal'\n",
    "\n",
    "loss = 'loss_twCRPS_sample' # options: loss_CRPS_sample, loss_twCRPS_sample, loss_log_likelihood\n",
    "\n",
    "chain_function = 'chain_function_normal_cdf_plus_constant' # options: chain_function_normal_cdf, chain_function_indicator, chain_function_normal_cdf_plus_constant\n",
    "chain_function_mean = 13\n",
    "chain_function_std = 2\n",
    "chain_function_threshold = 15 # 12 / 15\n",
    "chain_function_constant = 0.09\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.03\n",
    "folds = [1,2]\n",
    "neighbourhood_size = 11\n",
    "ignore = ['229', '285', '323']\n",
    "epochs = 600\n",
    "\n",
    "samples = 100\n",
    "printing = True\n",
    "pretrained = True\n",
    "random_init = False\n",
    "\n",
    "all_features = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "\n",
    "location_features = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "\n",
    "scale_features = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Log Normal distribution\n",
      "Final loss:  0.13321903\n",
      "Final loss:  0.13803501\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using given parameters for Log Normal distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 0.1333722621202469\n",
      "Step: 1, Loss: 0.13386060297489166\n",
      "Step: 2, Loss: 0.13307422399520874\n",
      "Step: 3, Loss: 0.13383789360523224\n",
      "Step: 4, Loss: 0.13279610872268677\n",
      "Step: 5, Loss: 0.13245519995689392\n",
      "Step: 6, Loss: 0.13243617117404938\n",
      "Step: 7, Loss: 0.13255766034126282\n",
      "Step: 8, Loss: 0.1322520673274994\n",
      "Step: 9, Loss: 0.1322324424982071\n",
      "Step: 10, Loss: 0.13262274861335754\n",
      "Step: 11, Loss: 0.13203784823417664\n",
      "Step: 12, Loss: 0.13180547952651978\n",
      "Step: 13, Loss: 0.131642147898674\n",
      "Step: 14, Loss: 0.1317080855369568\n",
      "Step: 15, Loss: 0.13184580206871033\n",
      "Step: 16, Loss: 0.13143745064735413\n",
      "Step: 17, Loss: 0.13131004571914673\n",
      "Step: 18, Loss: 0.13122469186782837\n",
      "Step: 19, Loss: 0.13142532110214233\n",
      "Step: 20, Loss: 0.13196294009685516\n",
      "Step: 21, Loss: 0.13123194873332977\n",
      "Step: 22, Loss: 0.1314646452665329\n",
      "Step: 23, Loss: 0.1310061812400818\n",
      "Step: 24, Loss: 0.13128021359443665\n",
      "Step: 25, Loss: 0.13184425234794617\n",
      "Step: 26, Loss: 0.1310722678899765\n",
      "Step: 27, Loss: 0.13076110184192657\n",
      "Step: 28, Loss: 0.13103434443473816\n",
      "Step: 29, Loss: 0.13172483444213867\n",
      "Step: 30, Loss: 0.13060565292835236\n",
      "Step: 31, Loss: 0.13089267909526825\n",
      "Step: 32, Loss: 0.13074608147144318\n",
      "Step: 33, Loss: 0.13084851205348969\n",
      "Step: 34, Loss: 0.1313551962375641\n",
      "Step: 35, Loss: 0.13164158165454865\n",
      "Step: 36, Loss: 0.1308422088623047\n",
      "Step: 37, Loss: 0.13135981559753418\n",
      "Step: 38, Loss: 0.13102754950523376\n",
      "Step: 39, Loss: 0.13122200965881348\n",
      "Step: 40, Loss: 0.13074630498886108\n",
      "Step: 41, Loss: 0.1305627077817917\n",
      "Step: 42, Loss: 0.1313876509666443\n",
      "Step: 43, Loss: 0.13116072118282318\n",
      "Step: 44, Loss: 0.13093732297420502\n",
      "Step: 45, Loss: 0.13071082532405853\n",
      "Step: 46, Loss: 0.13128207623958588\n",
      "Step: 47, Loss: 0.1311388909816742\n",
      "Step: 48, Loss: 0.13102051615715027\n",
      "Step: 49, Loss: 0.13075345754623413\n",
      "Step: 50, Loss: 0.13140496611595154\n",
      "Step: 51, Loss: 0.13088224828243256\n",
      "Step: 52, Loss: 0.131007120013237\n",
      "Step: 53, Loss: 0.13106708228588104\n",
      "Step: 54, Loss: 0.13122639060020447\n",
      "Step: 55, Loss: 0.1312319040298462\n",
      "Step: 56, Loss: 0.13116557896137238\n",
      "Step: 57, Loss: 0.13043653964996338\n",
      "Step: 58, Loss: 0.1311125010251999\n",
      "Step: 59, Loss: 0.1308688223361969\n",
      "Step: 60, Loss: 0.13062068819999695\n",
      "Step: 61, Loss: 0.13043731451034546\n",
      "Step: 62, Loss: 0.13057945668697357\n",
      "Step: 63, Loss: 0.1310236006975174\n",
      "Step: 64, Loss: 0.13091538846492767\n",
      "Step: 65, Loss: 0.1305881142616272\n",
      "Step: 66, Loss: 0.13099305331707\n",
      "Step: 67, Loss: 0.13095641136169434\n",
      "Step: 68, Loss: 0.13098087906837463\n",
      "Step: 69, Loss: 0.13095514476299286\n",
      "Step: 70, Loss: 0.1306792050600052\n",
      "Step: 71, Loss: 0.1307215392589569\n",
      "Step: 72, Loss: 0.13107924163341522\n",
      "Step: 73, Loss: 0.13065172731876373\n",
      "Step: 74, Loss: 0.1306651085615158\n",
      "Step: 75, Loss: 0.13063271343708038\n",
      "Step: 76, Loss: 0.1308424025774002\n",
      "Step: 77, Loss: 0.13083024322986603\n",
      "Step: 78, Loss: 0.13145709037780762\n",
      "Step: 79, Loss: 0.13065552711486816\n",
      "Step: 80, Loss: 0.1317882388830185\n",
      "Step: 81, Loss: 0.13110332190990448\n",
      "Step: 82, Loss: 0.1311742514371872\n",
      "Step: 83, Loss: 0.13087762892246246\n",
      "Step: 84, Loss: 0.13053244352340698\n",
      "Step: 85, Loss: 0.1307687610387802\n",
      "Step: 86, Loss: 0.13064174354076385\n",
      "Step: 87, Loss: 0.13124524056911469\n",
      "Step: 88, Loss: 0.13081546127796173\n",
      "Step: 89, Loss: 0.13106997311115265\n",
      "Step: 90, Loss: 0.1305953562259674\n",
      "Step: 91, Loss: 0.13046501576900482\n",
      "Step: 92, Loss: 0.1309845745563507\n",
      "Step: 93, Loss: 0.13107359409332275\n",
      "Step: 94, Loss: 0.1312844604253769\n",
      "Step: 95, Loss: 0.13104842603206635\n",
      "Step: 96, Loss: 0.13089580833911896\n",
      "Step: 97, Loss: 0.13131509721279144\n",
      "Step: 98, Loss: 0.1308794468641281\n",
      "Step: 99, Loss: 0.13080063462257385\n",
      "Step: 100, Loss: 0.13136206567287445\n",
      "Step: 101, Loss: 0.1310894638299942\n",
      "Step: 102, Loss: 0.13156013190746307\n",
      "Step: 103, Loss: 0.1316814124584198\n",
      "Step: 104, Loss: 0.13078556954860687\n",
      "Step: 105, Loss: 0.13097435235977173\n",
      "Step: 106, Loss: 0.13102443516254425\n",
      "Step: 107, Loss: 0.13101422786712646\n",
      "Step: 108, Loss: 0.13060413300991058\n",
      "Step: 109, Loss: 0.1308220773935318\n",
      "Step: 110, Loss: 0.13148069381713867\n",
      "Step: 111, Loss: 0.13165223598480225\n",
      "Step: 112, Loss: 0.13187558948993683\n",
      "Step: 113, Loss: 0.13171391189098358\n",
      "Step: 114, Loss: 0.13188885152339935\n",
      "Step: 115, Loss: 0.13147230446338654\n",
      "Step: 116, Loss: 0.13240674138069153\n",
      "Step: 117, Loss: 0.1321801245212555\n",
      "Step: 118, Loss: 0.1323525309562683\n",
      "Step: 119, Loss: 0.13215741515159607\n",
      "Step: 120, Loss: 0.13209977746009827\n",
      "Step: 121, Loss: 0.1322687417268753\n",
      "Step: 122, Loss: 0.13251857459545135\n",
      "Step: 123, Loss: 0.13245989382266998\n",
      "Step: 124, Loss: 0.13239358365535736\n",
      "Step: 125, Loss: 0.13277003169059753\n",
      "Step: 126, Loss: 0.13231782615184784\n",
      "Step: 127, Loss: 0.1324959546327591\n",
      "Step: 128, Loss: 0.13220953941345215\n",
      "Step: 129, Loss: 0.1323331892490387\n",
      "Step: 130, Loss: 0.13245180249214172\n",
      "Step: 131, Loss: 0.13202805817127228\n",
      "Step: 132, Loss: 0.13252130150794983\n",
      "Step: 133, Loss: 0.13163959980010986\n",
      "Step: 134, Loss: 0.1324755847454071\n",
      "Step: 135, Loss: 0.13196246325969696\n",
      "Step: 136, Loss: 0.1320059895515442\n",
      "Step: 137, Loss: 0.13197708129882812\n",
      "Step: 138, Loss: 0.13224728405475616\n",
      "Step: 139, Loss: 0.13172617554664612\n",
      "Step: 140, Loss: 0.1319427639245987\n",
      "Step: 141, Loss: 0.13190241158008575\n",
      "Step: 142, Loss: 0.1316581666469574\n",
      "Step: 143, Loss: 0.1317165493965149\n",
      "Step: 144, Loss: 0.13154934346675873\n",
      "Step: 145, Loss: 0.13131479918956757\n",
      "Step: 146, Loss: 0.13182687759399414\n",
      "Step: 147, Loss: 0.13186468183994293\n",
      "Step: 148, Loss: 0.13165859878063202\n",
      "Step: 149, Loss: 0.13178716599941254\n",
      "Step: 150, Loss: 0.1316438615322113\n",
      "Step: 151, Loss: 0.13167250156402588\n",
      "Step: 152, Loss: 0.13131652772426605\n",
      "Step: 153, Loss: 0.13121527433395386\n",
      "Step: 154, Loss: 0.13135482370853424\n",
      "Step: 155, Loss: 0.13152773678302765\n",
      "Step: 156, Loss: 0.13150373101234436\n",
      "Step: 157, Loss: 0.13151085376739502\n",
      "Step: 158, Loss: 0.13163341581821442\n",
      "Step: 159, Loss: 0.13146840035915375\n",
      "Step: 160, Loss: 0.13159039616584778\n",
      "Step: 161, Loss: 0.13128487765789032\n",
      "Step: 162, Loss: 0.13210976123809814\n",
      "Step: 163, Loss: 0.13166619837284088\n",
      "Step: 164, Loss: 0.13173632323741913\n",
      "Step: 165, Loss: 0.1318272352218628\n",
      "Step: 166, Loss: 0.13156135380268097\n",
      "Step: 167, Loss: 0.13117653131484985\n",
      "Step: 168, Loss: 0.13132868707180023\n",
      "Step: 169, Loss: 0.13156040012836456\n",
      "Step: 170, Loss: 0.13088710606098175\n",
      "Step: 171, Loss: 0.13206343352794647\n",
      "Step: 172, Loss: 0.13155484199523926\n",
      "Step: 173, Loss: 0.1310640424489975\n",
      "Step: 174, Loss: 0.1309637725353241\n",
      "Step: 175, Loss: 0.13094261288642883\n",
      "Step: 176, Loss: 0.1306784600019455\n",
      "Step: 177, Loss: 0.13117285072803497\n",
      "Step: 178, Loss: 0.13128267228603363\n",
      "Step: 179, Loss: 0.13156072795391083\n",
      "Step: 180, Loss: 0.13107037544250488\n",
      "Step: 181, Loss: 0.13157044351100922\n",
      "Step: 182, Loss: 0.1318468153476715\n",
      "Step: 183, Loss: 0.13179466128349304\n",
      "Step: 184, Loss: 0.13078932464122772\n",
      "Step: 185, Loss: 0.1311992108821869\n",
      "Step: 186, Loss: 0.1311427652835846\n",
      "Step: 187, Loss: 0.1312735676765442\n",
      "Step: 188, Loss: 0.13144204020500183\n",
      "Step: 189, Loss: 0.13139741122722626\n",
      "Step: 190, Loss: 0.13142208755016327\n",
      "Step: 191, Loss: 0.13167952001094818\n",
      "Step: 192, Loss: 0.1308673918247223\n",
      "Step: 193, Loss: 0.13134200870990753\n",
      "Step: 194, Loss: 0.131223663687706\n",
      "Step: 195, Loss: 0.13136686384677887\n",
      "Step: 196, Loss: 0.13150529563426971\n",
      "Step: 197, Loss: 0.1314796656370163\n",
      "Step: 198, Loss: 0.13132531940937042\n",
      "Step: 199, Loss: 0.13116951286792755\n",
      "Step: 200, Loss: 0.13116450607776642\n",
      "Step: 201, Loss: 0.13113795220851898\n",
      "Step: 202, Loss: 0.131726935505867\n",
      "Step: 203, Loss: 0.131353497505188\n",
      "Step: 204, Loss: 0.1311800181865692\n",
      "Step: 205, Loss: 0.13136762380599976\n",
      "Step: 206, Loss: 0.13130035996437073\n",
      "Step: 207, Loss: 0.13097453117370605\n",
      "Step: 208, Loss: 0.13152951002120972\n",
      "Step: 209, Loss: 0.13137350976467133\n",
      "Step: 210, Loss: 0.13104437291622162\n",
      "Step: 211, Loss: 0.13132774829864502\n",
      "Step: 212, Loss: 0.1319785714149475\n",
      "Step: 213, Loss: 0.1313055157661438\n",
      "Step: 214, Loss: 0.13135980069637299\n",
      "Step: 215, Loss: 0.1311684548854828\n",
      "Step: 216, Loss: 0.13110825419425964\n",
      "Step: 217, Loss: 0.1314876526594162\n",
      "Step: 218, Loss: 0.13113804161548615\n",
      "Step: 219, Loss: 0.1313711404800415\n",
      "Step: 220, Loss: 0.13108345866203308\n",
      "Step: 221, Loss: 0.13127727806568146\n",
      "Step: 222, Loss: 0.1312907487154007\n",
      "Step: 223, Loss: 0.13116800785064697\n",
      "Step: 224, Loss: 0.13127313554286957\n",
      "Step: 225, Loss: 0.13166752457618713\n",
      "Step: 226, Loss: 0.13094623386859894\n",
      "Step: 227, Loss: 0.1317320466041565\n",
      "Step: 228, Loss: 0.13122551143169403\n",
      "Step: 229, Loss: 0.1311751753091812\n",
      "Step: 230, Loss: 0.13099248707294464\n",
      "Step: 231, Loss: 0.13139137625694275\n",
      "Step: 232, Loss: 0.1315211057662964\n",
      "Step: 233, Loss: 0.13163863122463226\n",
      "Step: 234, Loss: 0.13122235238552094\n",
      "Step: 235, Loss: 0.1312716156244278\n",
      "Step: 236, Loss: 0.1308409422636032\n",
      "Step: 237, Loss: 0.13178327679634094\n",
      "Step: 238, Loss: 0.1309879720211029\n",
      "Step: 239, Loss: 0.13142479956150055\n",
      "Step: 240, Loss: 0.13129109144210815\n",
      "Step: 241, Loss: 0.13179178535938263\n",
      "Step: 242, Loss: 0.1314353048801422\n",
      "Step: 243, Loss: 0.1306135505437851\n",
      "Step: 244, Loss: 0.13141711056232452\n",
      "Step: 245, Loss: 0.13121724128723145\n",
      "Step: 246, Loss: 0.1315290629863739\n",
      "Step: 247, Loss: 0.13102950155735016\n",
      "Step: 248, Loss: 0.1312924176454544\n",
      "Step: 249, Loss: 0.13118603825569153\n",
      "Step: 250, Loss: 0.13080449402332306\n",
      "Step: 251, Loss: 0.1310107707977295\n",
      "Step: 252, Loss: 0.1314115673303604\n",
      "Step: 253, Loss: 0.13089118897914886\n",
      "Step: 254, Loss: 0.13091790676116943\n",
      "Step: 255, Loss: 0.13061374425888062\n",
      "Step: 256, Loss: 0.13144341111183167\n",
      "Step: 257, Loss: 0.13090132176876068\n",
      "Step: 258, Loss: 0.1311926692724228\n",
      "Step: 259, Loss: 0.13139352202415466\n",
      "Step: 260, Loss: 0.13085491955280304\n",
      "Step: 261, Loss: 0.1313416212797165\n",
      "Step: 262, Loss: 0.13154861330986023\n",
      "Step: 263, Loss: 0.130738765001297\n",
      "Step: 264, Loss: 0.13056513667106628\n",
      "Step: 265, Loss: 0.1304691880941391\n",
      "Step: 266, Loss: 0.13109153509140015\n",
      "Step: 267, Loss: 0.13146854937076569\n",
      "Step: 268, Loss: 0.13098330795764923\n",
      "Step: 269, Loss: 0.1305493265390396\n",
      "Step: 270, Loss: 0.1314311921596527\n",
      "Step: 271, Loss: 0.13124924898147583\n",
      "Step: 272, Loss: 0.13070718944072723\n",
      "Step: 273, Loss: 0.13116095960140228\n",
      "Step: 274, Loss: 0.13116854429244995\n",
      "Step: 275, Loss: 0.1308760941028595\n",
      "Step: 276, Loss: 0.13114012777805328\n",
      "Step: 277, Loss: 0.1312839537858963\n",
      "Step: 278, Loss: 0.13119949400424957\n",
      "Step: 279, Loss: 0.13081248104572296\n",
      "Step: 280, Loss: 0.1307317167520523\n",
      "Step: 281, Loss: 0.13126802444458008\n",
      "Step: 282, Loss: 0.13107852637767792\n",
      "Step: 283, Loss: 0.13072477281093597\n",
      "Step: 284, Loss: 0.13095276057720184\n",
      "Step: 285, Loss: 0.13127058744430542\n",
      "Step: 286, Loss: 0.1309117078781128\n",
      "Step: 287, Loss: 0.13057132065296173\n",
      "Step: 288, Loss: 0.13070443272590637\n",
      "Step: 289, Loss: 0.1306420862674713\n",
      "Step: 290, Loss: 0.13078300654888153\n",
      "Step: 291, Loss: 0.13078247010707855\n",
      "Step: 292, Loss: 0.13043376803398132\n",
      "Step: 293, Loss: 0.13140547275543213\n",
      "Step: 294, Loss: 0.13105066120624542\n",
      "Step: 295, Loss: 0.1311870962381363\n",
      "Step: 296, Loss: 0.13089558482170105\n",
      "Step: 297, Loss: 0.13034820556640625\n",
      "Step: 298, Loss: 0.1309555619955063\n",
      "Step: 299, Loss: 0.1310311108827591\n",
      "Step: 300, Loss: 0.13128601014614105\n",
      "Step: 301, Loss: 0.13082244992256165\n",
      "Step: 302, Loss: 0.13103087246418\n",
      "Step: 303, Loss: 0.1310570389032364\n",
      "Step: 304, Loss: 0.13082127273082733\n",
      "Step: 305, Loss: 0.1314724236726761\n",
      "Step: 306, Loss: 0.13098976016044617\n",
      "Step: 307, Loss: 0.13063091039657593\n",
      "Step: 308, Loss: 0.13079845905303955\n",
      "Step: 309, Loss: 0.1310023069381714\n",
      "Step: 310, Loss: 0.13159547746181488\n",
      "Step: 311, Loss: 0.130841925740242\n",
      "Step: 312, Loss: 0.13082048296928406\n",
      "Step: 313, Loss: 0.13082319498062134\n",
      "Step: 314, Loss: 0.13064055144786835\n",
      "Step: 315, Loss: 0.1312609761953354\n",
      "Step: 316, Loss: 0.13066865503787994\n",
      "Step: 317, Loss: 0.13037633895874023\n",
      "Step: 318, Loss: 0.13163894414901733\n",
      "Step: 319, Loss: 0.1312086284160614\n",
      "Step: 320, Loss: 0.1305784285068512\n",
      "Step: 321, Loss: 0.13131742179393768\n",
      "Step: 322, Loss: 0.13043594360351562\n",
      "Step: 323, Loss: 0.13067585229873657\n",
      "Step: 324, Loss: 0.130845308303833\n",
      "Step: 325, Loss: 0.13097167015075684\n",
      "Step: 326, Loss: 0.13123533129692078\n",
      "Step: 327, Loss: 0.13043035566806793\n",
      "Step: 328, Loss: 0.13104426860809326\n",
      "Step: 329, Loss: 0.13044513761997223\n",
      "Step: 330, Loss: 0.1307620257139206\n",
      "Step: 331, Loss: 0.13100965321063995\n",
      "Step: 332, Loss: 0.13066688179969788\n",
      "Step: 333, Loss: 0.13041098415851593\n",
      "Step: 334, Loss: 0.13075704872608185\n",
      "Step: 335, Loss: 0.1307254433631897\n",
      "Step: 336, Loss: 0.13106738030910492\n",
      "Step: 337, Loss: 0.13079817593097687\n",
      "Step: 338, Loss: 0.1309976726770401\n",
      "Step: 339, Loss: 0.1307687759399414\n",
      "Step: 340, Loss: 0.13041169941425323\n",
      "Step: 341, Loss: 0.1302233785390854\n",
      "Step: 342, Loss: 0.13092133402824402\n",
      "Step: 343, Loss: 0.13086655735969543\n",
      "Step: 344, Loss: 0.1313043236732483\n",
      "Step: 345, Loss: 0.1303774118423462\n",
      "Step: 346, Loss: 0.13066495954990387\n",
      "Step: 347, Loss: 0.13105793297290802\n",
      "Step: 348, Loss: 0.13107597827911377\n",
      "Step: 349, Loss: 0.13099932670593262\n",
      "Step: 350, Loss: 0.130764439702034\n",
      "Step: 351, Loss: 0.13014453649520874\n",
      "Step: 352, Loss: 0.13063064217567444\n",
      "Step: 353, Loss: 0.13093356788158417\n",
      "Step: 354, Loss: 0.13036221265792847\n",
      "Step: 355, Loss: 0.13091954588890076\n",
      "Step: 356, Loss: 0.13112704455852509\n",
      "Step: 357, Loss: 0.13131242990493774\n",
      "Step: 358, Loss: 0.1306615173816681\n",
      "Step: 359, Loss: 0.13107329607009888\n",
      "Step: 360, Loss: 0.1304333508014679\n",
      "Step: 361, Loss: 0.13111260533332825\n",
      "Step: 362, Loss: 0.13056983053684235\n",
      "Step: 363, Loss: 0.1309184581041336\n",
      "Step: 364, Loss: 0.13113348186016083\n",
      "Step: 365, Loss: 0.13068623840808868\n",
      "Step: 366, Loss: 0.1309751719236374\n",
      "Step: 367, Loss: 0.1308731734752655\n",
      "Step: 368, Loss: 0.1302025318145752\n",
      "Step: 369, Loss: 0.13066792488098145\n",
      "Step: 370, Loss: 0.13025440275669098\n",
      "Step: 371, Loss: 0.13055391609668732\n",
      "Step: 372, Loss: 0.13028022646903992\n",
      "Step: 373, Loss: 0.13097521662712097\n",
      "Step: 374, Loss: 0.13054707646369934\n",
      "Step: 375, Loss: 0.13076840341091156\n",
      "Step: 376, Loss: 0.1310829520225525\n",
      "Step: 377, Loss: 0.13084925711154938\n",
      "Step: 378, Loss: 0.13023266196250916\n",
      "Step: 379, Loss: 0.13024868071079254\n",
      "Step: 380, Loss: 0.13074907660484314\n",
      "Step: 381, Loss: 0.13098742067813873\n",
      "Step: 382, Loss: 0.13047872483730316\n",
      "Step: 383, Loss: 0.13042454421520233\n",
      "Step: 384, Loss: 0.13072489202022552\n",
      "Step: 385, Loss: 0.13066478073596954\n",
      "Step: 386, Loss: 0.13111822307109833\n",
      "Step: 387, Loss: 0.13062822818756104\n",
      "Step: 388, Loss: 0.1305866241455078\n",
      "Step: 389, Loss: 0.1310822069644928\n",
      "Step: 390, Loss: 0.13055232167243958\n",
      "Step: 391, Loss: 0.13073034584522247\n",
      "Step: 392, Loss: 0.1309792548418045\n",
      "Step: 393, Loss: 0.13038486242294312\n",
      "Step: 394, Loss: 0.1309576779603958\n",
      "Step: 395, Loss: 0.13127745687961578\n",
      "Step: 396, Loss: 0.13097046315670013\n",
      "Step: 397, Loss: 0.1305149495601654\n",
      "Step: 398, Loss: 0.13106653094291687\n",
      "Step: 399, Loss: 0.13052138686180115\n",
      "Step: 400, Loss: 0.13096405565738678\n",
      "Step: 401, Loss: 0.1305234283208847\n",
      "Step: 402, Loss: 0.1307706981897354\n",
      "Step: 403, Loss: 0.13093823194503784\n",
      "Step: 404, Loss: 0.13102227449417114\n",
      "Step: 405, Loss: 0.13069838285446167\n",
      "Step: 406, Loss: 0.13075090944766998\n",
      "Step: 407, Loss: 0.13062424957752228\n",
      "Step: 408, Loss: 0.13041873276233673\n",
      "Step: 409, Loss: 0.13056764006614685\n",
      "Step: 410, Loss: 0.1307523250579834\n",
      "Step: 411, Loss: 0.12996304035186768\n",
      "Step: 412, Loss: 0.13068656623363495\n",
      "Step: 413, Loss: 0.13063085079193115\n",
      "Step: 414, Loss: 0.13045378029346466\n",
      "Step: 415, Loss: 0.13046172261238098\n",
      "Step: 416, Loss: 0.1305668205022812\n",
      "Step: 417, Loss: 0.13040763139724731\n",
      "Step: 418, Loss: 0.13032133877277374\n",
      "Step: 419, Loss: 0.13047203421592712\n",
      "Step: 420, Loss: 0.13084383308887482\n",
      "Step: 421, Loss: 0.130568727850914\n",
      "Step: 422, Loss: 0.13093461096286774\n",
      "Step: 423, Loss: 0.13043467700481415\n",
      "Step: 424, Loss: 0.13034115731716156\n",
      "Step: 425, Loss: 0.13048285245895386\n",
      "Step: 426, Loss: 0.13053004443645477\n",
      "Step: 427, Loss: 0.13078631460666656\n",
      "Step: 428, Loss: 0.1310475766658783\n",
      "Step: 429, Loss: 0.1310124695301056\n",
      "Step: 430, Loss: 0.1314133107662201\n",
      "Step: 431, Loss: 0.13117238879203796\n",
      "Step: 432, Loss: 0.1314316838979721\n",
      "Step: 433, Loss: 0.13127899169921875\n",
      "Step: 434, Loss: 0.131801575422287\n",
      "Step: 435, Loss: 0.13191457092761993\n",
      "Step: 436, Loss: 0.13171574473381042\n",
      "Step: 437, Loss: 0.1321203112602234\n",
      "Step: 438, Loss: 0.13211682438850403\n",
      "Step: 439, Loss: 0.13198508322238922\n",
      "Step: 440, Loss: 0.13272544741630554\n",
      "Step: 441, Loss: 0.132191002368927\n",
      "Step: 442, Loss: 0.13268424570560455\n",
      "Step: 443, Loss: 0.13210956752300262\n",
      "Step: 444, Loss: 0.13179026544094086\n",
      "Step: 445, Loss: 0.1323607712984085\n",
      "Step: 446, Loss: 0.13198605179786682\n",
      "Step: 447, Loss: 0.13193126022815704\n",
      "Step: 448, Loss: 0.13154318928718567\n",
      "Step: 449, Loss: 0.13115620613098145\n",
      "Step: 450, Loss: 0.13210061192512512\n",
      "Step: 451, Loss: 0.13191410899162292\n",
      "Step: 452, Loss: 0.13200412690639496\n",
      "Step: 453, Loss: 0.13149966299533844\n",
      "Step: 454, Loss: 0.13137273490428925\n",
      "Step: 455, Loss: 0.1317027062177658\n",
      "Step: 456, Loss: 0.13191643357276917\n",
      "Step: 457, Loss: 0.13202624022960663\n",
      "Step: 458, Loss: 0.13167163729667664\n",
      "Step: 459, Loss: 0.13122256100177765\n",
      "Step: 460, Loss: 0.1318155974149704\n",
      "Step: 461, Loss: 0.13173522055149078\n",
      "Step: 462, Loss: 0.13177797198295593\n",
      "Step: 463, Loss: 0.1314103752374649\n",
      "Step: 464, Loss: 0.131708025932312\n",
      "Step: 465, Loss: 0.13154731690883636\n",
      "Step: 466, Loss: 0.13165371119976044\n",
      "Step: 467, Loss: 0.13126623630523682\n",
      "Step: 468, Loss: 0.13158273696899414\n",
      "Step: 469, Loss: 0.13168765604496002\n",
      "Step: 470, Loss: 0.13163498044013977\n",
      "Step: 471, Loss: 0.1310552954673767\n",
      "Step: 472, Loss: 0.13120946288108826\n",
      "Step: 473, Loss: 0.13158202171325684\n",
      "Step: 474, Loss: 0.1318957507610321\n",
      "Step: 475, Loss: 0.13138186931610107\n",
      "Step: 476, Loss: 0.13147184252738953\n",
      "Step: 477, Loss: 0.13163822889328003\n",
      "Step: 478, Loss: 0.13091325759887695\n",
      "Step: 479, Loss: 0.13124892115592957\n",
      "Step: 480, Loss: 0.13126587867736816\n",
      "Step: 481, Loss: 0.13078385591506958\n",
      "Step: 482, Loss: 0.1314520686864853\n",
      "Step: 483, Loss: 0.13136930763721466\n",
      "Step: 484, Loss: 0.13085389137268066\n",
      "Step: 485, Loss: 0.13052423298358917\n",
      "Step: 486, Loss: 0.13134750723838806\n",
      "Step: 487, Loss: 0.13079307973384857\n",
      "Step: 488, Loss: 0.131203755736351\n",
      "Step: 489, Loss: 0.1310451328754425\n",
      "Step: 490, Loss: 0.1309061199426651\n",
      "Step: 491, Loss: 0.13102903962135315\n",
      "Step: 492, Loss: 0.13109852373600006\n",
      "Step: 493, Loss: 0.13048602640628815\n",
      "Step: 494, Loss: 0.1308501958847046\n",
      "Step: 495, Loss: 0.13106216490268707\n",
      "Step: 496, Loss: 0.13111385703086853\n",
      "Step: 497, Loss: 0.13122417032718658\n",
      "Step: 498, Loss: 0.13058650493621826\n",
      "Step: 499, Loss: 0.13137376308441162\n",
      "Step: 500, Loss: 0.13047924637794495\n",
      "Step: 501, Loss: 0.13118532299995422\n",
      "Step: 502, Loss: 0.13055682182312012\n",
      "Step: 503, Loss: 0.13081611692905426\n",
      "Step: 504, Loss: 0.1304778903722763\n",
      "Step: 505, Loss: 0.13026386499404907\n",
      "Step: 506, Loss: 0.13045719265937805\n",
      "Step: 507, Loss: 0.13062350451946259\n",
      "Step: 508, Loss: 0.1307462751865387\n",
      "Step: 509, Loss: 0.1296241283416748\n",
      "Step: 510, Loss: 0.13011252880096436\n",
      "Step: 511, Loss: 0.13060463964939117\n",
      "Step: 512, Loss: 0.13068504631519318\n",
      "Step: 513, Loss: 0.13037987053394318\n",
      "Step: 514, Loss: 0.13058094680309296\n",
      "Step: 515, Loss: 0.1303114891052246\n",
      "Step: 516, Loss: 0.1304292231798172\n",
      "Step: 517, Loss: 0.1306573450565338\n",
      "Step: 518, Loss: 0.1307530403137207\n",
      "Step: 519, Loss: 0.1312420666217804\n",
      "Step: 520, Loss: 0.130939319729805\n",
      "Step: 521, Loss: 0.13077028095722198\n",
      "Step: 522, Loss: 0.13093656301498413\n",
      "Step: 523, Loss: 0.13081660866737366\n",
      "Step: 524, Loss: 0.13049551844596863\n",
      "Step: 525, Loss: 0.13052642345428467\n",
      "Step: 526, Loss: 0.13095998764038086\n",
      "Step: 527, Loss: 0.13073788583278656\n",
      "Step: 528, Loss: 0.1305769681930542\n",
      "Step: 529, Loss: 0.13012023270130157\n",
      "Step: 530, Loss: 0.13098599016666412\n",
      "Step: 531, Loss: 0.13024435937404633\n",
      "Step: 532, Loss: 0.13030481338500977\n",
      "Step: 533, Loss: 0.1302424818277359\n",
      "Step: 534, Loss: 0.13034000992774963\n",
      "Step: 535, Loss: 0.13067364692687988\n",
      "Step: 536, Loss: 0.13039739429950714\n",
      "Step: 537, Loss: 0.13040131330490112\n",
      "Step: 538, Loss: 0.13049644231796265\n",
      "Step: 539, Loss: 0.1304539442062378\n",
      "Step: 540, Loss: 0.13063330948352814\n",
      "Step: 541, Loss: 0.13026215136051178\n",
      "Step: 542, Loss: 0.13044396042823792\n",
      "Step: 543, Loss: 0.13073937594890594\n",
      "Step: 544, Loss: 0.1300552636384964\n",
      "Step: 545, Loss: 0.13039854168891907\n",
      "Step: 546, Loss: 0.13048280775547028\n",
      "Step: 547, Loss: 0.13002274930477142\n",
      "Step: 548, Loss: 0.13004834949970245\n",
      "Step: 549, Loss: 0.13036106526851654\n",
      "Step: 550, Loss: 0.13024768233299255\n",
      "Step: 551, Loss: 0.13030080497264862\n",
      "Step: 552, Loss: 0.13022544980049133\n",
      "Step: 553, Loss: 0.13107775151729584\n",
      "Step: 554, Loss: 0.1311720311641693\n",
      "Step: 555, Loss: 0.1306007355451584\n",
      "Step: 556, Loss: 0.1304759830236435\n",
      "Step: 557, Loss: 0.13005416095256805\n",
      "Step: 558, Loss: 0.13034629821777344\n",
      "Step: 559, Loss: 0.13062459230422974\n",
      "Step: 560, Loss: 0.13035397231578827\n",
      "Step: 561, Loss: 0.13034486770629883\n",
      "Step: 562, Loss: 0.13055694103240967\n",
      "Step: 563, Loss: 0.13072888553142548\n",
      "Step: 564, Loss: 0.1306024044752121\n",
      "Step: 565, Loss: 0.13054028153419495\n",
      "Step: 566, Loss: 0.13018593192100525\n",
      "Step: 567, Loss: 0.13025827705860138\n",
      "Step: 568, Loss: 0.1303383708000183\n",
      "Step: 569, Loss: 0.1303313821554184\n",
      "Step: 570, Loss: 0.13067658245563507\n",
      "Step: 571, Loss: 0.1305229514837265\n",
      "Step: 572, Loss: 0.13017822802066803\n",
      "Step: 573, Loss: 0.13067395985126495\n",
      "Step: 574, Loss: 0.13019239902496338\n",
      "Step: 575, Loss: 0.13025949895381927\n",
      "Step: 576, Loss: 0.13026323914527893\n",
      "Step: 577, Loss: 0.13081248104572296\n",
      "Step: 578, Loss: 0.12998762726783752\n",
      "Step: 579, Loss: 0.13041628897190094\n",
      "Step: 580, Loss: 0.13056571781635284\n",
      "Step: 581, Loss: 0.130498468875885\n",
      "Step: 582, Loss: 0.13063611090183258\n",
      "Step: 583, Loss: 0.13036717474460602\n",
      "Step: 584, Loss: 0.13000407814979553\n",
      "Step: 585, Loss: 0.13056154549121857\n",
      "Step: 586, Loss: 0.1303614377975464\n",
      "Step: 587, Loss: 0.1304352730512619\n",
      "Step: 588, Loss: 0.1306580752134323\n",
      "Step: 589, Loss: 0.13042546808719635\n",
      "Step: 590, Loss: 0.13072846829891205\n",
      "Step: 591, Loss: 0.13040634989738464\n",
      "Step: 592, Loss: 0.13000905513763428\n",
      "Step: 593, Loss: 0.13017617166042328\n",
      "Step: 594, Loss: 0.13033084571361542\n",
      "Step: 595, Loss: 0.13050517439842224\n",
      "Step: 596, Loss: 0.13024941086769104\n",
      "Step: 597, Loss: 0.13033179938793182\n",
      "Step: 598, Loss: 0.13028864562511444\n",
      "Step: 599, Loss: 0.13054662942886353\n",
      "Final loss:  0.13054663\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/ml_tn_ln_twcrps_mean13.0_std2.0_constant0.09000000357627869_epochs600_folds_1_2_mean0_1_2_3_4_std0_1_2_3_4_.pkl\n"
     ]
    }
   ],
   "source": [
    "model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    all_features,\n",
    "    location_features,\n",
    "    scale_features,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_constant = chain_function_constant,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained,\n",
    "    random_init = random_init\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Final loss:  0.10335002\n",
      "Final loss:  0.101744905\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using given parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 0.10101192444562912\n",
      "Step: 1, Loss: 0.1045975387096405\n",
      "Step: 2, Loss: 0.10153059661388397\n",
      "Step: 3, Loss: 0.10202322900295258\n",
      "Step: 4, Loss: 0.1033298522233963\n",
      "Step: 5, Loss: 0.10229266434907913\n",
      "Step: 6, Loss: 0.1007765606045723\n",
      "Step: 7, Loss: 0.10148461908102036\n",
      "Step: 8, Loss: 0.10205074399709702\n",
      "Step: 9, Loss: 0.10181190073490143\n",
      "Step: 10, Loss: 0.10107484459877014\n",
      "Step: 11, Loss: 0.10078710317611694\n",
      "Step: 12, Loss: 0.1010366752743721\n",
      "Step: 13, Loss: 0.10091517120599747\n",
      "Step: 14, Loss: 0.1011858657002449\n",
      "Step: 15, Loss: 0.10120128095149994\n",
      "Step: 16, Loss: 0.10065849870443344\n",
      "Step: 17, Loss: 0.10093311220407486\n",
      "Step: 18, Loss: 0.10144681483507156\n",
      "Step: 19, Loss: 0.10081422328948975\n",
      "Step: 20, Loss: 0.100248321890831\n",
      "Step: 21, Loss: 0.10068947076797485\n",
      "Step: 22, Loss: 0.10042747110128403\n",
      "Step: 23, Loss: 0.10088712722063065\n",
      "Step: 24, Loss: 0.1005655974149704\n",
      "Step: 25, Loss: 0.10123177617788315\n",
      "Step: 26, Loss: 0.10017429292201996\n",
      "Step: 27, Loss: 0.10028838366270065\n",
      "Step: 28, Loss: 0.10079685598611832\n",
      "Step: 29, Loss: 0.1003103256225586\n",
      "Step: 30, Loss: 0.10113375633955002\n",
      "Step: 31, Loss: 0.10008753091096878\n",
      "Step: 32, Loss: 0.10060267895460129\n",
      "Step: 33, Loss: 0.09993112087249756\n",
      "Step: 34, Loss: 0.10006285458803177\n",
      "Step: 35, Loss: 0.10051048547029495\n",
      "Step: 36, Loss: 0.10016465932130814\n",
      "Step: 37, Loss: 0.1008554995059967\n",
      "Step: 38, Loss: 0.10040871053934097\n",
      "Step: 39, Loss: 0.10027137398719788\n",
      "Step: 40, Loss: 0.10040444135665894\n",
      "Step: 41, Loss: 0.10068489611148834\n",
      "Step: 42, Loss: 0.10042519867420197\n",
      "Step: 43, Loss: 0.1005309671163559\n",
      "Step: 44, Loss: 0.100456103682518\n",
      "Step: 45, Loss: 0.10045760124921799\n",
      "Step: 46, Loss: 0.1009252518415451\n",
      "Step: 47, Loss: 0.10039106011390686\n",
      "Step: 48, Loss: 0.10039254277944565\n",
      "Step: 49, Loss: 0.10032002627849579\n",
      "Step: 50, Loss: 0.10016867518424988\n",
      "Step: 51, Loss: 0.10017027705907822\n",
      "Step: 52, Loss: 0.10000988841056824\n",
      "Step: 53, Loss: 0.10024469345808029\n",
      "Step: 54, Loss: 0.09989137947559357\n",
      "Step: 55, Loss: 0.10024677962064743\n",
      "Step: 56, Loss: 0.0998087152838707\n",
      "Step: 57, Loss: 0.10010010749101639\n",
      "Step: 58, Loss: 0.10027453303337097\n",
      "Step: 59, Loss: 0.10038748383522034\n",
      "Step: 60, Loss: 0.09995193034410477\n",
      "Step: 61, Loss: 0.10020797699689865\n",
      "Step: 62, Loss: 0.10054070502519608\n",
      "Step: 63, Loss: 0.09990429133176804\n",
      "Step: 64, Loss: 0.10013564676046371\n",
      "Step: 65, Loss: 0.10048423707485199\n",
      "Step: 66, Loss: 0.10039737820625305\n",
      "Step: 67, Loss: 0.10024860501289368\n",
      "Step: 68, Loss: 0.10020972788333893\n",
      "Step: 69, Loss: 0.1000327616930008\n",
      "Step: 70, Loss: 0.10040455311536789\n",
      "Step: 71, Loss: 0.10010264068841934\n",
      "Step: 72, Loss: 0.100120410323143\n",
      "Step: 73, Loss: 0.10007312148809433\n",
      "Step: 74, Loss: 0.09965398907661438\n",
      "Step: 75, Loss: 0.10011614859104156\n",
      "Step: 76, Loss: 0.1004413589835167\n",
      "Step: 77, Loss: 0.10010948032140732\n",
      "Step: 78, Loss: 0.10064411908388138\n",
      "Step: 79, Loss: 0.10035866498947144\n",
      "Step: 80, Loss: 0.10028905421495438\n",
      "Step: 81, Loss: 0.10032825917005539\n",
      "Step: 82, Loss: 0.09956841170787811\n",
      "Step: 83, Loss: 0.1001385971903801\n",
      "Step: 84, Loss: 0.10013581067323685\n",
      "Step: 85, Loss: 0.10000420361757278\n",
      "Step: 86, Loss: 0.09996264427900314\n",
      "Step: 87, Loss: 0.10030550509691238\n",
      "Step: 88, Loss: 0.09974067658185959\n",
      "Step: 89, Loss: 0.10065500438213348\n",
      "Step: 90, Loss: 0.10021546483039856\n",
      "Step: 91, Loss: 0.10012366622686386\n",
      "Step: 92, Loss: 0.1002422347664833\n",
      "Step: 93, Loss: 0.09985672682523727\n",
      "Step: 94, Loss: 0.10030867159366608\n",
      "Step: 95, Loss: 0.09985712915658951\n",
      "Step: 96, Loss: 0.10033926367759705\n",
      "Step: 97, Loss: 0.10037868469953537\n",
      "Step: 98, Loss: 0.099961057305336\n",
      "Step: 99, Loss: 0.10020636022090912\n",
      "Step: 100, Loss: 0.09966293722391129\n",
      "Step: 101, Loss: 0.10012321919202805\n",
      "Step: 102, Loss: 0.10028219223022461\n",
      "Step: 103, Loss: 0.10012897104024887\n",
      "Step: 104, Loss: 0.10037192702293396\n",
      "Step: 105, Loss: 0.10018285363912582\n",
      "Step: 106, Loss: 0.10033261775970459\n",
      "Step: 107, Loss: 0.10020512342453003\n",
      "Step: 108, Loss: 0.09996239095926285\n",
      "Step: 109, Loss: 0.09985598176717758\n",
      "Step: 110, Loss: 0.10007429122924805\n",
      "Step: 111, Loss: 0.10003470629453659\n",
      "Step: 112, Loss: 0.10045289248228073\n",
      "Step: 113, Loss: 0.1001242995262146\n",
      "Step: 114, Loss: 0.1000967025756836\n",
      "Step: 115, Loss: 0.10052905231714249\n",
      "Step: 116, Loss: 0.10022064298391342\n",
      "Step: 117, Loss: 0.10005631297826767\n",
      "Step: 118, Loss: 0.1000601276755333\n",
      "Step: 119, Loss: 0.09992896765470505\n",
      "Step: 120, Loss: 0.10031336545944214\n",
      "Step: 121, Loss: 0.10004186630249023\n",
      "Step: 122, Loss: 0.09985711425542831\n",
      "Step: 123, Loss: 0.09995575249195099\n",
      "Step: 124, Loss: 0.0998339056968689\n",
      "Step: 125, Loss: 0.10030561685562134\n",
      "Step: 126, Loss: 0.10000582784414291\n",
      "Step: 127, Loss: 0.1001092940568924\n",
      "Step: 128, Loss: 0.09972014278173447\n",
      "Step: 129, Loss: 0.09994105994701385\n",
      "Step: 130, Loss: 0.1002492755651474\n",
      "Step: 131, Loss: 0.0996551588177681\n",
      "Step: 132, Loss: 0.10011706501245499\n",
      "Step: 133, Loss: 0.10062992572784424\n",
      "Step: 134, Loss: 0.10021651536226273\n",
      "Step: 135, Loss: 0.10009622573852539\n",
      "Step: 136, Loss: 0.09982169419527054\n",
      "Step: 137, Loss: 0.1003158912062645\n",
      "Step: 138, Loss: 0.10016825795173645\n",
      "Step: 139, Loss: 0.10030484199523926\n",
      "Step: 140, Loss: 0.09976093471050262\n",
      "Step: 141, Loss: 0.10006352514028549\n",
      "Step: 142, Loss: 0.1001281887292862\n",
      "Step: 143, Loss: 0.0998583659529686\n",
      "Step: 144, Loss: 0.09987346082925797\n",
      "Step: 145, Loss: 0.09980994462966919\n",
      "Step: 146, Loss: 0.0999317392706871\n",
      "Step: 147, Loss: 0.09982917457818985\n",
      "Step: 148, Loss: 0.09982779622077942\n",
      "Step: 149, Loss: 0.10061391443014145\n",
      "Step: 150, Loss: 0.09979795664548874\n",
      "Step: 151, Loss: 0.09970931708812714\n",
      "Step: 152, Loss: 0.09963594377040863\n",
      "Step: 153, Loss: 0.09963522106409073\n",
      "Step: 154, Loss: 0.10000116378068924\n",
      "Step: 155, Loss: 0.09996750950813293\n",
      "Step: 156, Loss: 0.09971670806407928\n",
      "Step: 157, Loss: 0.10017547011375427\n",
      "Step: 158, Loss: 0.10012545436620712\n",
      "Step: 159, Loss: 0.1003195270895958\n",
      "Step: 160, Loss: 0.09978775680065155\n",
      "Step: 161, Loss: 0.0996885895729065\n",
      "Step: 162, Loss: 0.09987661987543106\n",
      "Step: 163, Loss: 0.09971199184656143\n",
      "Step: 164, Loss: 0.09981833398342133\n",
      "Step: 165, Loss: 0.09961768984794617\n",
      "Step: 166, Loss: 0.099788136780262\n",
      "Step: 167, Loss: 0.09958240389823914\n",
      "Step: 168, Loss: 0.0994737520813942\n",
      "Step: 169, Loss: 0.09954477846622467\n",
      "Step: 170, Loss: 0.09974025934934616\n",
      "Step: 171, Loss: 0.09960589557886124\n",
      "Step: 172, Loss: 0.09965706616640091\n",
      "Step: 173, Loss: 0.10000422596931458\n",
      "Step: 174, Loss: 0.09968196600675583\n",
      "Step: 175, Loss: 0.09961492568254471\n",
      "Step: 176, Loss: 0.09962505102157593\n",
      "Step: 177, Loss: 0.09971467405557632\n",
      "Step: 178, Loss: 0.09962061792612076\n",
      "Step: 179, Loss: 0.10008540749549866\n",
      "Step: 180, Loss: 0.09984629601240158\n",
      "Step: 181, Loss: 0.09999776631593704\n",
      "Step: 182, Loss: 0.09971991181373596\n",
      "Step: 183, Loss: 0.09985431283712387\n",
      "Step: 184, Loss: 0.09943965077400208\n",
      "Step: 185, Loss: 0.09947750717401505\n",
      "Step: 186, Loss: 0.09945524483919144\n",
      "Step: 187, Loss: 0.09984668344259262\n",
      "Step: 188, Loss: 0.09993193298578262\n",
      "Step: 189, Loss: 0.10006702691316605\n",
      "Step: 190, Loss: 0.09995385259389877\n",
      "Step: 191, Loss: 0.09992042928934097\n",
      "Step: 192, Loss: 0.09999304264783859\n",
      "Step: 193, Loss: 0.09988008439540863\n",
      "Step: 194, Loss: 0.10006505995988846\n",
      "Step: 195, Loss: 0.0996040552854538\n",
      "Step: 196, Loss: 0.09975820779800415\n",
      "Step: 197, Loss: 0.0998237207531929\n",
      "Step: 198, Loss: 0.10023599117994308\n",
      "Step: 199, Loss: 0.09960661828517914\n",
      "Step: 200, Loss: 0.10003604739904404\n",
      "Step: 201, Loss: 0.09946393966674805\n",
      "Step: 202, Loss: 0.100131094455719\n",
      "Step: 203, Loss: 0.09991443157196045\n",
      "Step: 204, Loss: 0.09985866397619247\n",
      "Step: 205, Loss: 0.09977446496486664\n",
      "Step: 206, Loss: 0.09983466565608978\n",
      "Step: 207, Loss: 0.09991797804832458\n",
      "Step: 208, Loss: 0.10013588517904282\n",
      "Step: 209, Loss: 0.09971050173044205\n",
      "Step: 210, Loss: 0.09985184669494629\n",
      "Step: 211, Loss: 0.10012660920619965\n",
      "Step: 212, Loss: 0.10011982917785645\n",
      "Step: 213, Loss: 0.10008106380701065\n",
      "Step: 214, Loss: 0.0996464341878891\n",
      "Step: 215, Loss: 0.09952999651432037\n",
      "Step: 216, Loss: 0.09975336492061615\n",
      "Step: 217, Loss: 0.10034859925508499\n",
      "Step: 218, Loss: 0.10001468658447266\n",
      "Step: 219, Loss: 0.09998709708452225\n",
      "Step: 220, Loss: 0.09959638863801956\n",
      "Step: 221, Loss: 0.10007799416780472\n",
      "Step: 222, Loss: 0.09981153905391693\n",
      "Step: 223, Loss: 0.09976130723953247\n",
      "Step: 224, Loss: 0.09986881166696548\n",
      "Step: 225, Loss: 0.1004132404923439\n",
      "Step: 226, Loss: 0.10018006712198257\n",
      "Step: 227, Loss: 0.09992454946041107\n",
      "Step: 228, Loss: 0.09985758364200592\n",
      "Step: 229, Loss: 0.10003688186407089\n",
      "Step: 230, Loss: 0.09983201324939728\n",
      "Step: 231, Loss: 0.09984076023101807\n",
      "Step: 232, Loss: 0.09968853741884232\n",
      "Step: 233, Loss: 0.10021143406629562\n",
      "Step: 234, Loss: 0.10005252063274384\n",
      "Step: 235, Loss: 0.09953856468200684\n",
      "Step: 236, Loss: 0.09988480806350708\n",
      "Step: 237, Loss: 0.09968262910842896\n",
      "Step: 238, Loss: 0.09969595074653625\n",
      "Step: 239, Loss: 0.09975699335336685\n",
      "Step: 240, Loss: 0.09970810264348984\n",
      "Step: 241, Loss: 0.09993459284305573\n",
      "Step: 242, Loss: 0.10000336170196533\n",
      "Step: 243, Loss: 0.10025124996900558\n",
      "Step: 244, Loss: 0.09998960793018341\n",
      "Step: 245, Loss: 0.09986984729766846\n",
      "Step: 246, Loss: 0.09957825392484665\n",
      "Step: 247, Loss: 0.09991302341222763\n",
      "Step: 248, Loss: 0.09960884600877762\n",
      "Step: 249, Loss: 0.10005608946084976\n",
      "Step: 250, Loss: 0.10001975297927856\n",
      "Step: 251, Loss: 0.10007541626691818\n",
      "Step: 252, Loss: 0.09981565922498703\n",
      "Step: 253, Loss: 0.0997411459684372\n",
      "Step: 254, Loss: 0.09985959529876709\n",
      "Step: 255, Loss: 0.09977702796459198\n",
      "Step: 256, Loss: 0.09994734078645706\n",
      "Step: 257, Loss: 0.09973761439323425\n",
      "Step: 258, Loss: 0.09974502772092819\n",
      "Step: 259, Loss: 0.09975292533636093\n",
      "Step: 260, Loss: 0.0995725467801094\n",
      "Step: 261, Loss: 0.09971857070922852\n",
      "Step: 262, Loss: 0.09960460662841797\n",
      "Step: 263, Loss: 0.09983714669942856\n",
      "Step: 264, Loss: 0.0994969978928566\n",
      "Step: 265, Loss: 0.09993026405572891\n",
      "Step: 266, Loss: 0.10000336170196533\n",
      "Step: 267, Loss: 0.0997394397854805\n",
      "Step: 268, Loss: 0.0997757539153099\n",
      "Step: 269, Loss: 0.10010257363319397\n",
      "Step: 270, Loss: 0.10022033751010895\n",
      "Step: 271, Loss: 0.09982910007238388\n",
      "Step: 272, Loss: 0.1000540629029274\n",
      "Step: 273, Loss: 0.09944810718297958\n",
      "Step: 274, Loss: 0.09984477609395981\n",
      "Step: 275, Loss: 0.0992569848895073\n",
      "Step: 276, Loss: 0.09955143183469772\n",
      "Step: 277, Loss: 0.09981421381235123\n",
      "Step: 278, Loss: 0.09991719573736191\n",
      "Step: 279, Loss: 0.0994381308555603\n",
      "Step: 280, Loss: 0.09965036064386368\n",
      "Step: 281, Loss: 0.09970209002494812\n",
      "Step: 282, Loss: 0.10018698871135712\n",
      "Step: 283, Loss: 0.09987757354974747\n",
      "Step: 284, Loss: 0.0996500626206398\n",
      "Step: 285, Loss: 0.100091852247715\n",
      "Step: 286, Loss: 0.09989342093467712\n",
      "Step: 287, Loss: 0.10021798312664032\n",
      "Step: 288, Loss: 0.0999566838145256\n",
      "Step: 289, Loss: 0.0995660126209259\n",
      "Step: 290, Loss: 0.09934667497873306\n",
      "Step: 291, Loss: 0.0996696874499321\n",
      "Step: 292, Loss: 0.09985460340976715\n",
      "Step: 293, Loss: 0.09978452324867249\n",
      "Step: 294, Loss: 0.09970322996377945\n",
      "Step: 295, Loss: 0.09984125941991806\n",
      "Step: 296, Loss: 0.09951096773147583\n",
      "Step: 297, Loss: 0.09961052238941193\n",
      "Step: 298, Loss: 0.09987451136112213\n",
      "Step: 299, Loss: 0.09990452975034714\n",
      "Step: 300, Loss: 0.10009047389030457\n",
      "Step: 301, Loss: 0.09997782856225967\n",
      "Step: 302, Loss: 0.09924758970737457\n",
      "Step: 303, Loss: 0.09946586191654205\n",
      "Step: 304, Loss: 0.0998670756816864\n",
      "Step: 305, Loss: 0.10005007684230804\n",
      "Step: 306, Loss: 0.10001236945390701\n",
      "Step: 307, Loss: 0.09990290552377701\n",
      "Step: 308, Loss: 0.09959187358617783\n",
      "Step: 309, Loss: 0.1002177894115448\n",
      "Step: 310, Loss: 0.09970739483833313\n",
      "Step: 311, Loss: 0.09943041950464249\n",
      "Step: 312, Loss: 0.0995108038187027\n",
      "Step: 313, Loss: 0.09992336481809616\n",
      "Step: 314, Loss: 0.09930036962032318\n",
      "Step: 315, Loss: 0.09970066696405411\n",
      "Step: 316, Loss: 0.09972327202558517\n",
      "Step: 317, Loss: 0.09958726167678833\n",
      "Step: 318, Loss: 0.09994520246982574\n",
      "Step: 319, Loss: 0.09952209144830704\n",
      "Step: 320, Loss: 0.09948418289422989\n",
      "Step: 321, Loss: 0.09951978921890259\n",
      "Step: 322, Loss: 0.09943463653326035\n",
      "Step: 323, Loss: 0.09984159469604492\n",
      "Step: 324, Loss: 0.09962151199579239\n",
      "Step: 325, Loss: 0.0996653139591217\n",
      "Step: 326, Loss: 0.09927761554718018\n",
      "Step: 327, Loss: 0.09962236881256104\n",
      "Step: 328, Loss: 0.0997888371348381\n",
      "Step: 329, Loss: 0.09921903908252716\n",
      "Step: 330, Loss: 0.10006149858236313\n",
      "Step: 331, Loss: 0.0994364321231842\n",
      "Step: 332, Loss: 0.10026399791240692\n",
      "Step: 333, Loss: 0.09971509873867035\n",
      "Step: 334, Loss: 0.10039117187261581\n",
      "Step: 335, Loss: 0.09976818412542343\n",
      "Step: 336, Loss: 0.09957128018140793\n",
      "Step: 337, Loss: 0.10015113651752472\n",
      "Step: 338, Loss: 0.10032059252262115\n",
      "Step: 339, Loss: 0.09994091838598251\n",
      "Step: 340, Loss: 0.10010998696088791\n",
      "Step: 341, Loss: 0.09978180378675461\n",
      "Step: 342, Loss: 0.09978150576353073\n",
      "Step: 343, Loss: 0.10012223571538925\n",
      "Step: 344, Loss: 0.09989391267299652\n",
      "Step: 345, Loss: 0.09989049285650253\n",
      "Step: 346, Loss: 0.10021966695785522\n",
      "Step: 347, Loss: 0.10012882947921753\n",
      "Step: 348, Loss: 0.09990854561328888\n",
      "Step: 349, Loss: 0.09975525736808777\n",
      "Step: 350, Loss: 0.10012611001729965\n",
      "Step: 351, Loss: 0.09984101355075836\n",
      "Step: 352, Loss: 0.09986074268817902\n",
      "Step: 353, Loss: 0.09975353628396988\n",
      "Step: 354, Loss: 0.09982657432556152\n",
      "Step: 355, Loss: 0.09984082728624344\n",
      "Step: 356, Loss: 0.10024143010377884\n",
      "Step: 357, Loss: 0.10007154941558838\n",
      "Step: 358, Loss: 0.10003611445426941\n",
      "Step: 359, Loss: 0.10013946890830994\n",
      "Step: 360, Loss: 0.10002493113279343\n",
      "Step: 361, Loss: 0.10018520057201385\n",
      "Step: 362, Loss: 0.09982316195964813\n",
      "Step: 363, Loss: 0.1001284196972847\n",
      "Step: 364, Loss: 0.10002555698156357\n",
      "Step: 365, Loss: 0.09970288723707199\n",
      "Step: 366, Loss: 0.09989278018474579\n",
      "Step: 367, Loss: 0.1003192737698555\n",
      "Step: 368, Loss: 0.10013553500175476\n",
      "Step: 369, Loss: 0.09986501932144165\n",
      "Step: 370, Loss: 0.10005918145179749\n",
      "Step: 371, Loss: 0.10015542060136795\n",
      "Step: 372, Loss: 0.09939827024936676\n",
      "Step: 373, Loss: 0.10003519803285599\n",
      "Step: 374, Loss: 0.0998372957110405\n",
      "Step: 375, Loss: 0.10035349428653717\n",
      "Step: 376, Loss: 0.09981882572174072\n",
      "Step: 377, Loss: 0.09971339255571365\n",
      "Step: 378, Loss: 0.09950613230466843\n",
      "Step: 379, Loss: 0.0999230444431305\n",
      "Step: 380, Loss: 0.10002359747886658\n",
      "Step: 381, Loss: 0.10012052953243256\n",
      "Step: 382, Loss: 0.10015081614255905\n",
      "Step: 383, Loss: 0.09993571788072586\n",
      "Step: 384, Loss: 0.10028791427612305\n",
      "Step: 385, Loss: 0.09959328919649124\n",
      "Step: 386, Loss: 0.09993324428796768\n",
      "Step: 387, Loss: 0.09984131157398224\n",
      "Step: 388, Loss: 0.09991424530744553\n",
      "Step: 389, Loss: 0.10016492754220963\n",
      "Step: 390, Loss: 0.09998331218957901\n",
      "Step: 391, Loss: 0.09986256062984467\n",
      "Step: 392, Loss: 0.09996448457241058\n",
      "Step: 393, Loss: 0.09943445771932602\n",
      "Step: 394, Loss: 0.09974827617406845\n",
      "Step: 395, Loss: 0.10029754787683487\n",
      "Step: 396, Loss: 0.10010799765586853\n",
      "Step: 397, Loss: 0.09959585964679718\n",
      "Step: 398, Loss: 0.09976475685834885\n",
      "Step: 399, Loss: 0.09986741095781326\n",
      "Step: 400, Loss: 0.09970524162054062\n",
      "Step: 401, Loss: 0.09987582266330719\n",
      "Step: 402, Loss: 0.09949187934398651\n",
      "Step: 403, Loss: 0.09993467479944229\n",
      "Step: 404, Loss: 0.10013909637928009\n",
      "Step: 405, Loss: 0.09980069845914841\n",
      "Step: 406, Loss: 0.10038300603628159\n",
      "Step: 407, Loss: 0.09998966008424759\n",
      "Step: 408, Loss: 0.09987952560186386\n",
      "Step: 409, Loss: 0.0997140109539032\n",
      "Step: 410, Loss: 0.09966640919446945\n",
      "Step: 411, Loss: 0.09958469867706299\n",
      "Step: 412, Loss: 0.10002169013023376\n",
      "Step: 413, Loss: 0.09982061386108398\n",
      "Step: 414, Loss: 0.09994416683912277\n",
      "Step: 415, Loss: 0.09970349818468094\n",
      "Step: 416, Loss: 0.10004322975873947\n",
      "Step: 417, Loss: 0.0996941328048706\n",
      "Step: 418, Loss: 0.09992799162864685\n",
      "Step: 419, Loss: 0.10003471374511719\n",
      "Step: 420, Loss: 0.09996713697910309\n",
      "Step: 421, Loss: 0.09973790496587753\n",
      "Step: 422, Loss: 0.09977637976408005\n",
      "Step: 423, Loss: 0.10000914335250854\n",
      "Step: 424, Loss: 0.10004518181085587\n",
      "Step: 425, Loss: 0.10023046284914017\n",
      "Step: 426, Loss: 0.09995432198047638\n",
      "Step: 427, Loss: 0.09925954788923264\n",
      "Step: 428, Loss: 0.09991728514432907\n",
      "Step: 429, Loss: 0.09946966916322708\n",
      "Step: 430, Loss: 0.09996330738067627\n",
      "Step: 431, Loss: 0.09970986098051071\n",
      "Step: 432, Loss: 0.09970537573099136\n",
      "Step: 433, Loss: 0.10017500817775726\n",
      "Step: 434, Loss: 0.0995701476931572\n",
      "Step: 435, Loss: 0.09996020793914795\n",
      "Step: 436, Loss: 0.09986601769924164\n",
      "Step: 437, Loss: 0.09947854280471802\n",
      "Step: 438, Loss: 0.09946966916322708\n",
      "Step: 439, Loss: 0.09941795468330383\n",
      "Step: 440, Loss: 0.0998176634311676\n",
      "Step: 441, Loss: 0.0996798649430275\n",
      "Step: 442, Loss: 0.0996241644024849\n",
      "Step: 443, Loss: 0.09985533356666565\n",
      "Step: 444, Loss: 0.09977034479379654\n",
      "Step: 445, Loss: 0.09954078495502472\n",
      "Step: 446, Loss: 0.09973353892564774\n",
      "Step: 447, Loss: 0.09921430796384811\n",
      "Step: 448, Loss: 0.09962459653615952\n",
      "Step: 449, Loss: 0.09963539242744446\n",
      "Step: 450, Loss: 0.09927268326282501\n",
      "Step: 451, Loss: 0.09973824769258499\n",
      "Step: 452, Loss: 0.10009041428565979\n",
      "Step: 453, Loss: 0.09966740012168884\n",
      "Step: 454, Loss: 0.10032093524932861\n",
      "Step: 455, Loss: 0.10001173615455627\n",
      "Step: 456, Loss: 0.09977711737155914\n",
      "Step: 457, Loss: 0.09990888833999634\n",
      "Step: 458, Loss: 0.10095333307981491\n",
      "Step: 459, Loss: 0.10029976814985275\n",
      "Step: 460, Loss: 0.10055486112833023\n",
      "Step: 461, Loss: 0.100510373711586\n",
      "Step: 462, Loss: 0.1001894623041153\n",
      "Step: 463, Loss: 0.1000116840004921\n",
      "Step: 464, Loss: 0.10017205774784088\n",
      "Step: 465, Loss: 0.09987734258174896\n",
      "Step: 466, Loss: 0.09993784874677658\n",
      "Step: 467, Loss: 0.10050167143344879\n",
      "Step: 468, Loss: 0.10080045461654663\n",
      "Step: 469, Loss: 0.10109151899814606\n",
      "Step: 470, Loss: 0.10154446959495544\n",
      "Step: 471, Loss: 0.101442851126194\n",
      "Step: 472, Loss: 0.10067947208881378\n",
      "Step: 473, Loss: 0.10112825036048889\n",
      "Step: 474, Loss: 0.10031715780496597\n",
      "Step: 475, Loss: 0.10064329952001572\n",
      "Step: 476, Loss: 0.1008189469575882\n",
      "Step: 477, Loss: 0.1007021814584732\n",
      "Step: 478, Loss: 0.10026750713586807\n",
      "Step: 479, Loss: 0.10025402158498764\n",
      "Step: 480, Loss: 0.10001331567764282\n",
      "Step: 481, Loss: 0.10018754005432129\n",
      "Step: 482, Loss: 0.10039862245321274\n",
      "Step: 483, Loss: 0.10031452029943466\n",
      "Step: 484, Loss: 0.10024953633546829\n",
      "Step: 485, Loss: 0.10004188120365143\n",
      "Step: 486, Loss: 0.09993920475244522\n",
      "Step: 487, Loss: 0.10009193420410156\n",
      "Step: 488, Loss: 0.09955208003520966\n",
      "Step: 489, Loss: 0.09983590990304947\n",
      "Step: 490, Loss: 0.10010526329278946\n",
      "Step: 491, Loss: 0.09993577003479004\n",
      "Step: 492, Loss: 0.10001025348901749\n",
      "Step: 493, Loss: 0.09999268501996994\n",
      "Step: 494, Loss: 0.1001076027750969\n",
      "Step: 495, Loss: 0.09974785894155502\n",
      "Step: 496, Loss: 0.10024365037679672\n",
      "Step: 497, Loss: 0.09986631572246552\n",
      "Step: 498, Loss: 0.09976784884929657\n",
      "Step: 499, Loss: 0.10009763389825821\n",
      "Step: 500, Loss: 0.0998651534318924\n",
      "Step: 501, Loss: 0.0996825098991394\n",
      "Step: 502, Loss: 0.0994672030210495\n",
      "Step: 503, Loss: 0.09935992956161499\n",
      "Step: 504, Loss: 0.09979315102100372\n",
      "Step: 505, Loss: 0.09958283603191376\n",
      "Step: 506, Loss: 0.09996163845062256\n",
      "Step: 507, Loss: 0.09993566572666168\n",
      "Step: 508, Loss: 0.1003762036561966\n",
      "Step: 509, Loss: 0.10008284449577332\n",
      "Step: 510, Loss: 0.09933522343635559\n",
      "Step: 511, Loss: 0.0995444506406784\n",
      "Step: 512, Loss: 0.10031702369451523\n",
      "Step: 513, Loss: 0.09976620972156525\n",
      "Step: 514, Loss: 0.09976449608802795\n",
      "Step: 515, Loss: 0.0998372808098793\n",
      "Step: 516, Loss: 0.0995604619383812\n",
      "Step: 517, Loss: 0.09973584115505219\n",
      "Step: 518, Loss: 0.09963072091341019\n",
      "Step: 519, Loss: 0.09962606430053711\n",
      "Step: 520, Loss: 0.09978165477514267\n",
      "Step: 521, Loss: 0.0993596613407135\n",
      "Step: 522, Loss: 0.09998148679733276\n",
      "Step: 523, Loss: 0.1000947579741478\n",
      "Step: 524, Loss: 0.09979057312011719\n",
      "Step: 525, Loss: 0.1000363752245903\n",
      "Step: 526, Loss: 0.099674291908741\n",
      "Step: 527, Loss: 0.09972982108592987\n",
      "Step: 528, Loss: 0.09998524188995361\n",
      "Step: 529, Loss: 0.09960104525089264\n",
      "Step: 530, Loss: 0.09983497858047485\n",
      "Step: 531, Loss: 0.09992524981498718\n",
      "Step: 532, Loss: 0.09966707229614258\n",
      "Step: 533, Loss: 0.09958945959806442\n",
      "Step: 534, Loss: 0.09980438649654388\n",
      "Step: 535, Loss: 0.0994650349020958\n",
      "Step: 536, Loss: 0.09983722120523453\n",
      "Step: 537, Loss: 0.09963468462228775\n",
      "Step: 538, Loss: 0.09971854090690613\n",
      "Step: 539, Loss: 0.09986277669668198\n",
      "Step: 540, Loss: 0.09945572912693024\n",
      "Step: 541, Loss: 0.10004832595586777\n",
      "Step: 542, Loss: 0.09977001696825027\n",
      "Step: 543, Loss: 0.09960256516933441\n",
      "Step: 544, Loss: 0.09927938133478165\n",
      "Step: 545, Loss: 0.09926994144916534\n",
      "Step: 546, Loss: 0.09978073835372925\n",
      "Step: 547, Loss: 0.09961432218551636\n",
      "Step: 548, Loss: 0.09936332702636719\n",
      "Step: 549, Loss: 0.09980623424053192\n",
      "Step: 550, Loss: 0.09983431547880173\n",
      "Step: 551, Loss: 0.09938172250986099\n",
      "Step: 552, Loss: 0.09943331778049469\n",
      "Step: 553, Loss: 0.09966979175806046\n",
      "Step: 554, Loss: 0.09966657310724258\n",
      "Step: 555, Loss: 0.09943845868110657\n",
      "Step: 556, Loss: 0.09954114258289337\n",
      "Step: 557, Loss: 0.09956977516412735\n",
      "Step: 558, Loss: 0.09957275539636612\n",
      "Step: 559, Loss: 0.09975379705429077\n",
      "Step: 560, Loss: 0.09955392777919769\n",
      "Step: 561, Loss: 0.09936036169528961\n",
      "Step: 562, Loss: 0.09943887591362\n",
      "Step: 563, Loss: 0.09995297342538834\n",
      "Step: 564, Loss: 0.0995626151561737\n",
      "Step: 565, Loss: 0.09926672279834747\n",
      "Step: 566, Loss: 0.09947508573532104\n",
      "Step: 567, Loss: 0.09905603528022766\n",
      "Step: 568, Loss: 0.09952205419540405\n",
      "Step: 569, Loss: 0.09928470104932785\n",
      "Step: 570, Loss: 0.09992332756519318\n",
      "Step: 571, Loss: 0.09943296760320663\n",
      "Step: 572, Loss: 0.09956399351358414\n",
      "Step: 573, Loss: 0.09931079298257828\n",
      "Step: 574, Loss: 0.09973365068435669\n",
      "Step: 575, Loss: 0.0994490459561348\n",
      "Step: 576, Loss: 0.09972744435071945\n",
      "Step: 577, Loss: 0.09966512769460678\n",
      "Step: 578, Loss: 0.09981945157051086\n",
      "Step: 579, Loss: 0.0996147096157074\n",
      "Step: 580, Loss: 0.09937168657779694\n",
      "Step: 581, Loss: 0.09961981326341629\n",
      "Step: 582, Loss: 0.09938687086105347\n",
      "Step: 583, Loss: 0.09917938709259033\n",
      "Step: 584, Loss: 0.09949074685573578\n",
      "Step: 585, Loss: 0.09972810000181198\n",
      "Step: 586, Loss: 0.09952537715435028\n",
      "Step: 587, Loss: 0.09971737116575241\n",
      "Step: 588, Loss: 0.09962929040193558\n",
      "Step: 589, Loss: 0.09988339990377426\n",
      "Step: 590, Loss: 0.09978655725717545\n",
      "Step: 591, Loss: 0.09918398410081863\n",
      "Step: 592, Loss: 0.09979411959648132\n",
      "Step: 593, Loss: 0.09905647486448288\n",
      "Step: 594, Loss: 0.09952230006456375\n",
      "Step: 595, Loss: 0.09919173270463943\n",
      "Step: 596, Loss: 0.09927141666412354\n",
      "Step: 597, Loss: 0.09943622350692749\n",
      "Step: 598, Loss: 0.09921099990606308\n",
      "Step: 599, Loss: 0.09971991926431656\n",
      "Final loss:  0.09971992\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/mixturelinear_tn_gev_twcrps_mean13.0_std4.0_epochs600_folds_1_3.pkl\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
