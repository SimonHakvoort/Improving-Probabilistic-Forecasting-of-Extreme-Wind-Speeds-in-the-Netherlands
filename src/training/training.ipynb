{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 13:34:35.354978: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-05 13:34:35.381984: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-05 13:34:35.382004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-05 13:34:35.382785: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-05 13:34:35.387081: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-05 13:34:35.387509: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-05 13:34:41.642964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.training.training import train_model, train_and_save, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_distribution = 'distr_mixture_linear'\n",
    "distribution_1 = 'distr_trunc_normal'\n",
    "distribution_2 = 'distr_trunc_normal'\n",
    "\n",
    "loss = 'loss_twCRPS_sample' # options: loss_CRPS_sample, loss_twCRPS_sample, loss_log_likelihood\n",
    "\n",
    "chain_function = 'chain_function_normal_cdf_plus_constant' # options: chain_function_normal_cdf, chain_function_indicator, chain_function_normal_cdf_plus_constant\n",
    "chain_function_mean = 13\n",
    "chain_function_std = 2\n",
    "chain_function_threshold = 15 # 12 / 15\n",
    "chain_function_constant = 0.05\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.03\n",
    "folds = [1,2]\n",
    "neighbourhood_size = 11\n",
    "ignore = ['229', '285', '323']\n",
    "epochs = 600\n",
    "\n",
    "samples = 100\n",
    "printing = True\n",
    "pretrained = True\n",
    "random_init = False\n",
    "\n",
    "all_features = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot', 'spatial_variance']\n",
    "\n",
    "location_features = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "\n",
    "scale_features = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot', 'spatial_variance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for truncated normal distribution\n",
      "Final loss:  0.09627709\n",
      "Final loss:  0.09665087\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 0.09668280184268951\n",
      "Step: 1, Loss: 0.09665318578481674\n",
      "Step: 2, Loss: 0.09613275527954102\n",
      "Step: 3, Loss: 0.09617417305707932\n",
      "Step: 4, Loss: 0.0958273857831955\n",
      "Step: 5, Loss: 0.0965953841805458\n",
      "Step: 6, Loss: 0.09613750129938126\n",
      "Step: 7, Loss: 0.09613967686891556\n",
      "Step: 8, Loss: 0.09677041321992874\n",
      "Step: 9, Loss: 0.09609454870223999\n",
      "Step: 10, Loss: 0.09640740603208542\n",
      "Step: 11, Loss: 0.09632386267185211\n",
      "Step: 12, Loss: 0.09643039107322693\n",
      "Step: 13, Loss: 0.0962715744972229\n",
      "Step: 14, Loss: 0.09656800329685211\n",
      "Step: 15, Loss: 0.09598542749881744\n",
      "Step: 16, Loss: 0.09661862999200821\n",
      "Step: 17, Loss: 0.0961601734161377\n",
      "Step: 18, Loss: 0.09609281271696091\n",
      "Step: 19, Loss: 0.09580698609352112\n",
      "Step: 20, Loss: 0.09633659571409225\n",
      "Step: 21, Loss: 0.09565901011228561\n",
      "Step: 22, Loss: 0.09649988263845444\n",
      "Step: 23, Loss: 0.0958397313952446\n",
      "Step: 24, Loss: 0.09634453803300858\n",
      "Step: 25, Loss: 0.09591279923915863\n",
      "Step: 26, Loss: 0.09584066271781921\n",
      "Step: 27, Loss: 0.09589686989784241\n",
      "Step: 28, Loss: 0.09653574228286743\n",
      "Step: 29, Loss: 0.09652862697839737\n",
      "Step: 30, Loss: 0.09594560414552689\n",
      "Step: 31, Loss: 0.09620147198438644\n",
      "Step: 32, Loss: 0.09608540683984756\n",
      "Step: 33, Loss: 0.09597084671258926\n",
      "Step: 34, Loss: 0.09574434161186218\n",
      "Step: 35, Loss: 0.0955568253993988\n",
      "Step: 36, Loss: 0.09602417796850204\n",
      "Step: 37, Loss: 0.09568832069635391\n",
      "Step: 38, Loss: 0.09603343904018402\n",
      "Step: 39, Loss: 0.09597598761320114\n",
      "Step: 40, Loss: 0.09566111862659454\n",
      "Step: 41, Loss: 0.09540154039859772\n",
      "Step: 42, Loss: 0.09581398963928223\n",
      "Step: 43, Loss: 0.09594615548849106\n",
      "Step: 44, Loss: 0.09557095915079117\n",
      "Step: 45, Loss: 0.09576722234487534\n",
      "Step: 46, Loss: 0.09553773701190948\n",
      "Step: 47, Loss: 0.09576517343521118\n",
      "Step: 48, Loss: 0.09571924060583115\n",
      "Step: 49, Loss: 0.09555867314338684\n",
      "Step: 50, Loss: 0.09513944387435913\n",
      "Step: 51, Loss: 0.09533024579286575\n",
      "Step: 52, Loss: 0.0959632471203804\n",
      "Step: 53, Loss: 0.09537912160158157\n",
      "Step: 54, Loss: 0.09547970443964005\n",
      "Step: 55, Loss: 0.09567634761333466\n",
      "Step: 56, Loss: 0.0955461636185646\n",
      "Step: 57, Loss: 0.09544985741376877\n",
      "Step: 58, Loss: 0.09545893222093582\n",
      "Step: 59, Loss: 0.09532792121171951\n",
      "Step: 60, Loss: 0.09578226506710052\n",
      "Step: 61, Loss: 0.09523101896047592\n",
      "Step: 62, Loss: 0.09520383179187775\n",
      "Step: 63, Loss: 0.09511277079582214\n",
      "Step: 64, Loss: 0.09520352631807327\n",
      "Step: 65, Loss: 0.09497802704572678\n",
      "Step: 66, Loss: 0.09534627944231033\n",
      "Step: 67, Loss: 0.09537436068058014\n",
      "Step: 68, Loss: 0.09500622749328613\n",
      "Step: 69, Loss: 0.09534946829080582\n",
      "Step: 70, Loss: 0.09506594389677048\n",
      "Step: 71, Loss: 0.09441389888525009\n",
      "Step: 72, Loss: 0.09525739401578903\n",
      "Step: 73, Loss: 0.09490266442298889\n",
      "Step: 74, Loss: 0.09479960054159164\n",
      "Step: 75, Loss: 0.09517627209424973\n",
      "Step: 76, Loss: 0.09503108263015747\n",
      "Step: 77, Loss: 0.09485115110874176\n",
      "Step: 78, Loss: 0.09495386481285095\n",
      "Step: 79, Loss: 0.0951014906167984\n",
      "Step: 80, Loss: 0.09527844935655594\n",
      "Step: 81, Loss: 0.09486138820648193\n",
      "Step: 82, Loss: 0.0947456881403923\n",
      "Step: 83, Loss: 0.09498938173055649\n",
      "Step: 84, Loss: 0.09506842494010925\n",
      "Step: 85, Loss: 0.09530004858970642\n",
      "Step: 86, Loss: 0.09523684531450272\n",
      "Step: 87, Loss: 0.09470067173242569\n",
      "Step: 88, Loss: 0.09472000598907471\n",
      "Step: 89, Loss: 0.09508350491523743\n",
      "Step: 90, Loss: 0.09476057440042496\n",
      "Step: 91, Loss: 0.09497278928756714\n",
      "Step: 92, Loss: 0.09498947858810425\n",
      "Step: 93, Loss: 0.09474746137857437\n",
      "Step: 94, Loss: 0.09484859555959702\n",
      "Step: 95, Loss: 0.09462882578372955\n",
      "Step: 96, Loss: 0.09483367949724197\n",
      "Step: 97, Loss: 0.09497065842151642\n",
      "Step: 98, Loss: 0.09491720050573349\n",
      "Step: 99, Loss: 0.09491739422082901\n",
      "Step: 100, Loss: 0.09513115137815475\n",
      "Step: 101, Loss: 0.09487742930650711\n",
      "Step: 102, Loss: 0.09464414417743683\n",
      "Step: 103, Loss: 0.09532763063907623\n",
      "Step: 104, Loss: 0.0950782522559166\n",
      "Step: 105, Loss: 0.09441236406564713\n",
      "Step: 106, Loss: 0.09480535238981247\n",
      "Step: 107, Loss: 0.09512008726596832\n",
      "Step: 108, Loss: 0.09493099898099899\n",
      "Step: 109, Loss: 0.09436265379190445\n",
      "Step: 110, Loss: 0.09468115866184235\n",
      "Step: 111, Loss: 0.09524401277303696\n",
      "Step: 112, Loss: 0.09469854831695557\n",
      "Step: 113, Loss: 0.09480150789022446\n",
      "Step: 114, Loss: 0.09478162229061127\n",
      "Step: 115, Loss: 0.09503531455993652\n",
      "Step: 116, Loss: 0.09462789446115494\n",
      "Step: 117, Loss: 0.09490533173084259\n",
      "Step: 118, Loss: 0.09519194811582565\n",
      "Step: 119, Loss: 0.09488427639007568\n",
      "Step: 120, Loss: 0.09488040208816528\n",
      "Step: 121, Loss: 0.09467258304357529\n",
      "Step: 122, Loss: 0.09485378116369247\n",
      "Step: 123, Loss: 0.09500615298748016\n",
      "Step: 124, Loss: 0.0949973613023758\n",
      "Step: 125, Loss: 0.09466233849525452\n",
      "Step: 126, Loss: 0.09502429515123367\n",
      "Step: 127, Loss: 0.09492604434490204\n",
      "Step: 128, Loss: 0.09480006247758865\n",
      "Step: 129, Loss: 0.09501907229423523\n",
      "Step: 130, Loss: 0.0948672667145729\n",
      "Step: 131, Loss: 0.0946417823433876\n",
      "Step: 132, Loss: 0.09484802186489105\n",
      "Step: 133, Loss: 0.0946802943944931\n",
      "Step: 134, Loss: 0.09509450942277908\n",
      "Step: 135, Loss: 0.09517541527748108\n",
      "Step: 136, Loss: 0.09469622373580933\n",
      "Step: 137, Loss: 0.09514900296926498\n",
      "Step: 138, Loss: 0.09503133594989777\n",
      "Step: 139, Loss: 0.09485601633787155\n",
      "Step: 140, Loss: 0.09468945115804672\n",
      "Step: 141, Loss: 0.09511890262365341\n",
      "Step: 142, Loss: 0.09489987790584564\n",
      "Step: 143, Loss: 0.09430459886789322\n",
      "Step: 144, Loss: 0.09456426650285721\n",
      "Step: 145, Loss: 0.09485205262899399\n",
      "Step: 146, Loss: 0.09516911208629608\n",
      "Step: 147, Loss: 0.09498237073421478\n",
      "Step: 148, Loss: 0.09499707072973251\n",
      "Step: 149, Loss: 0.0949651449918747\n",
      "Step: 150, Loss: 0.09492409229278564\n",
      "Step: 151, Loss: 0.09512370079755783\n",
      "Step: 152, Loss: 0.09467722475528717\n",
      "Step: 153, Loss: 0.0953325554728508\n",
      "Step: 154, Loss: 0.09446722269058228\n",
      "Step: 155, Loss: 0.09504582732915878\n",
      "Step: 156, Loss: 0.09485741704702377\n",
      "Step: 157, Loss: 0.09493347257375717\n",
      "Step: 158, Loss: 0.09455931186676025\n",
      "Step: 159, Loss: 0.09495075792074203\n",
      "Step: 160, Loss: 0.0946754589676857\n",
      "Step: 161, Loss: 0.09451009333133698\n",
      "Step: 162, Loss: 0.09468095749616623\n",
      "Step: 163, Loss: 0.09492926299571991\n",
      "Step: 164, Loss: 0.09474422037601471\n",
      "Step: 165, Loss: 0.09516215324401855\n",
      "Step: 166, Loss: 0.09485702216625214\n",
      "Step: 167, Loss: 0.09429238736629486\n",
      "Step: 168, Loss: 0.09516307711601257\n",
      "Step: 169, Loss: 0.0951070562005043\n",
      "Step: 170, Loss: 0.09476637840270996\n",
      "Step: 171, Loss: 0.09483278542757034\n",
      "Step: 172, Loss: 0.09480514377355576\n",
      "Step: 173, Loss: 0.0947028174996376\n",
      "Step: 174, Loss: 0.09514592587947845\n",
      "Step: 175, Loss: 0.09484988451004028\n",
      "Step: 176, Loss: 0.09496770799160004\n",
      "Step: 177, Loss: 0.09471361339092255\n",
      "Step: 178, Loss: 0.09503825753927231\n",
      "Step: 179, Loss: 0.09491393715143204\n",
      "Step: 180, Loss: 0.09462940692901611\n",
      "Step: 181, Loss: 0.0950152650475502\n",
      "Step: 182, Loss: 0.09504400938749313\n",
      "Step: 183, Loss: 0.09520240873098373\n",
      "Step: 184, Loss: 0.09476453810930252\n",
      "Step: 185, Loss: 0.09493134915828705\n",
      "Step: 186, Loss: 0.09454097598791122\n",
      "Step: 187, Loss: 0.0947950929403305\n",
      "Step: 188, Loss: 0.09430362284183502\n",
      "Step: 189, Loss: 0.09495347738265991\n",
      "Step: 190, Loss: 0.09452738612890244\n",
      "Step: 191, Loss: 0.09465409815311432\n",
      "Step: 192, Loss: 0.09490709751844406\n",
      "Step: 193, Loss: 0.09485096484422684\n",
      "Step: 194, Loss: 0.09478062391281128\n",
      "Step: 195, Loss: 0.0947699323296547\n",
      "Step: 196, Loss: 0.09483867883682251\n",
      "Step: 197, Loss: 0.09443556517362595\n",
      "Step: 198, Loss: 0.09468663483858109\n",
      "Step: 199, Loss: 0.0951467826962471\n",
      "Step: 200, Loss: 0.09481827914714813\n",
      "Step: 201, Loss: 0.09490008652210236\n",
      "Step: 202, Loss: 0.09477932006120682\n",
      "Step: 203, Loss: 0.09511703252792358\n",
      "Step: 204, Loss: 0.09481710940599442\n",
      "Step: 205, Loss: 0.09552163630723953\n",
      "Step: 206, Loss: 0.09508807957172394\n",
      "Step: 207, Loss: 0.09532937407493591\n",
      "Step: 208, Loss: 0.09524926543235779\n",
      "Step: 209, Loss: 0.09503337740898132\n",
      "Step: 210, Loss: 0.09519873559474945\n",
      "Step: 211, Loss: 0.09499505162239075\n",
      "Step: 212, Loss: 0.09502045810222626\n",
      "Step: 213, Loss: 0.09491429477930069\n",
      "Step: 214, Loss: 0.09497864544391632\n",
      "Step: 215, Loss: 0.09487443417310715\n",
      "Step: 216, Loss: 0.09494488686323166\n",
      "Step: 217, Loss: 0.09531312435865402\n",
      "Step: 218, Loss: 0.09482423216104507\n",
      "Step: 219, Loss: 0.09503569453954697\n",
      "Step: 220, Loss: 0.09506658464670181\n",
      "Step: 221, Loss: 0.09498415142297745\n",
      "Step: 222, Loss: 0.0949341282248497\n",
      "Step: 223, Loss: 0.09479975700378418\n",
      "Step: 224, Loss: 0.09478723257780075\n",
      "Step: 225, Loss: 0.09471465647220612\n",
      "Step: 226, Loss: 0.0948912501335144\n",
      "Step: 227, Loss: 0.0951661691069603\n",
      "Step: 228, Loss: 0.09468399733304977\n",
      "Step: 229, Loss: 0.09504960477352142\n",
      "Step: 230, Loss: 0.09461037814617157\n",
      "Step: 231, Loss: 0.09479428082704544\n",
      "Step: 232, Loss: 0.09484769403934479\n",
      "Step: 233, Loss: 0.09498198330402374\n",
      "Step: 234, Loss: 0.09502403438091278\n",
      "Step: 235, Loss: 0.09475734084844589\n",
      "Step: 236, Loss: 0.09477764368057251\n",
      "Step: 237, Loss: 0.0951259657740593\n",
      "Step: 238, Loss: 0.09465965628623962\n",
      "Step: 239, Loss: 0.09509429335594177\n",
      "Step: 240, Loss: 0.09474824368953705\n",
      "Step: 241, Loss: 0.09490852802991867\n",
      "Step: 242, Loss: 0.09491366893053055\n",
      "Step: 243, Loss: 0.0946551039814949\n",
      "Step: 244, Loss: 0.09444759041070938\n",
      "Step: 245, Loss: 0.09522158652544022\n",
      "Step: 246, Loss: 0.09468270093202591\n",
      "Step: 247, Loss: 0.09482508152723312\n",
      "Step: 248, Loss: 0.09494639188051224\n",
      "Step: 249, Loss: 0.09502703696489334\n",
      "Step: 250, Loss: 0.09505649656057358\n",
      "Step: 251, Loss: 0.09510887414216995\n",
      "Step: 252, Loss: 0.09513360261917114\n",
      "Step: 253, Loss: 0.09472805261611938\n",
      "Step: 254, Loss: 0.09452259540557861\n",
      "Step: 255, Loss: 0.09493944048881531\n",
      "Step: 256, Loss: 0.09474137425422668\n",
      "Step: 257, Loss: 0.09510328620672226\n",
      "Step: 258, Loss: 0.09515241533517838\n",
      "Step: 259, Loss: 0.09486737102270126\n",
      "Step: 260, Loss: 0.09475904703140259\n",
      "Step: 261, Loss: 0.09471190720796585\n",
      "Step: 262, Loss: 0.0947154313325882\n",
      "Step: 263, Loss: 0.09494615346193314\n",
      "Step: 264, Loss: 0.09470711648464203\n",
      "Step: 265, Loss: 0.09490538388490677\n",
      "Step: 266, Loss: 0.09491512924432755\n",
      "Step: 267, Loss: 0.09508492052555084\n",
      "Step: 268, Loss: 0.09487111121416092\n",
      "Step: 269, Loss: 0.09497746080160141\n",
      "Step: 270, Loss: 0.09470199048519135\n",
      "Step: 271, Loss: 0.09463981539011002\n",
      "Step: 272, Loss: 0.0945238396525383\n",
      "Step: 273, Loss: 0.09488793462514877\n",
      "Step: 274, Loss: 0.09452133625745773\n",
      "Step: 275, Loss: 0.09527503699064255\n",
      "Step: 276, Loss: 0.09521762281656265\n",
      "Step: 277, Loss: 0.09476204961538315\n",
      "Step: 278, Loss: 0.09474126994609833\n",
      "Step: 279, Loss: 0.0946723148226738\n",
      "Step: 280, Loss: 0.09505423903465271\n",
      "Step: 281, Loss: 0.09502745419740677\n",
      "Step: 282, Loss: 0.09461134672164917\n",
      "Step: 283, Loss: 0.09495337307453156\n",
      "Step: 284, Loss: 0.09480657428503036\n",
      "Step: 285, Loss: 0.09535220265388489\n",
      "Step: 286, Loss: 0.0949174240231514\n",
      "Step: 287, Loss: 0.0949525311589241\n",
      "Step: 288, Loss: 0.09491312503814697\n",
      "Step: 289, Loss: 0.09510515630245209\n",
      "Step: 290, Loss: 0.09471350908279419\n",
      "Step: 291, Loss: 0.09504841268062592\n",
      "Step: 292, Loss: 0.09521991014480591\n",
      "Step: 293, Loss: 0.09457598626613617\n",
      "Step: 294, Loss: 0.09492334723472595\n",
      "Step: 295, Loss: 0.09494433552026749\n",
      "Step: 296, Loss: 0.09501246362924576\n",
      "Step: 297, Loss: 0.09482983499765396\n",
      "Step: 298, Loss: 0.09478576481342316\n",
      "Step: 299, Loss: 0.09493851661682129\n",
      "Step: 300, Loss: 0.09476850926876068\n",
      "Step: 301, Loss: 0.09480851143598557\n",
      "Step: 302, Loss: 0.09502003341913223\n",
      "Step: 303, Loss: 0.09458955377340317\n",
      "Step: 304, Loss: 0.09476915746927261\n",
      "Step: 305, Loss: 0.09480729699134827\n",
      "Step: 306, Loss: 0.09481928497552872\n",
      "Step: 307, Loss: 0.09466899186372757\n",
      "Step: 308, Loss: 0.09538419544696808\n",
      "Step: 309, Loss: 0.09484299272298813\n",
      "Step: 310, Loss: 0.09523144364356995\n",
      "Step: 311, Loss: 0.09504980593919754\n",
      "Step: 312, Loss: 0.0950765460729599\n",
      "Step: 313, Loss: 0.0946914553642273\n",
      "Step: 314, Loss: 0.0944749191403389\n",
      "Step: 315, Loss: 0.09490091353654861\n",
      "Step: 316, Loss: 0.09468439966440201\n",
      "Step: 317, Loss: 0.09493798762559891\n",
      "Step: 318, Loss: 0.09490391612052917\n",
      "Step: 319, Loss: 0.095054492354393\n",
      "Step: 320, Loss: 0.09475582838058472\n",
      "Step: 321, Loss: 0.09460753947496414\n",
      "Step: 322, Loss: 0.09480754286050797\n",
      "Step: 323, Loss: 0.09449662268161774\n",
      "Step: 324, Loss: 0.09479405730962753\n",
      "Step: 325, Loss: 0.09448964148759842\n",
      "Step: 326, Loss: 0.09532278031110764\n",
      "Step: 327, Loss: 0.09504720568656921\n",
      "Step: 328, Loss: 0.09467442333698273\n",
      "Step: 329, Loss: 0.09483754634857178\n",
      "Step: 330, Loss: 0.09482575207948685\n",
      "Step: 331, Loss: 0.09478681534528732\n",
      "Step: 332, Loss: 0.09490420669317245\n",
      "Step: 333, Loss: 0.09459742903709412\n",
      "Step: 334, Loss: 0.09490954130887985\n",
      "Step: 335, Loss: 0.09472235292196274\n",
      "Step: 336, Loss: 0.09469190239906311\n",
      "Step: 337, Loss: 0.09485701471567154\n",
      "Step: 338, Loss: 0.09456231445074081\n",
      "Step: 339, Loss: 0.09508746862411499\n",
      "Step: 340, Loss: 0.09443977475166321\n",
      "Step: 341, Loss: 0.09502588957548141\n",
      "Step: 342, Loss: 0.09489814937114716\n",
      "Step: 343, Loss: 0.09490425884723663\n",
      "Step: 344, Loss: 0.09465273469686508\n",
      "Step: 345, Loss: 0.09472600370645523\n",
      "Step: 346, Loss: 0.09474990516901016\n",
      "Step: 347, Loss: 0.09493941813707352\n",
      "Step: 348, Loss: 0.09482257068157196\n",
      "Step: 349, Loss: 0.09468540549278259\n",
      "Step: 350, Loss: 0.0946703851222992\n",
      "Step: 351, Loss: 0.09488564729690552\n",
      "Step: 352, Loss: 0.09481827169656754\n",
      "Step: 353, Loss: 0.0946638435125351\n",
      "Step: 354, Loss: 0.09480174630880356\n",
      "Step: 355, Loss: 0.09509926289319992\n",
      "Step: 356, Loss: 0.09436120092868805\n",
      "Step: 357, Loss: 0.09456480294466019\n",
      "Step: 358, Loss: 0.09492891281843185\n",
      "Step: 359, Loss: 0.09510941803455353\n",
      "Step: 360, Loss: 0.09479424357414246\n",
      "Step: 361, Loss: 0.09461361914873123\n",
      "Step: 362, Loss: 0.09462665021419525\n",
      "Step: 363, Loss: 0.0947919711470604\n",
      "Step: 364, Loss: 0.09466324001550674\n",
      "Step: 365, Loss: 0.09500730037689209\n",
      "Step: 366, Loss: 0.09468834847211838\n",
      "Step: 367, Loss: 0.09480997920036316\n",
      "Step: 368, Loss: 0.09499961882829666\n",
      "Step: 369, Loss: 0.09465280175209045\n",
      "Step: 370, Loss: 0.09488866478204727\n",
      "Step: 371, Loss: 0.09482615441083908\n",
      "Step: 372, Loss: 0.09499421715736389\n",
      "Step: 373, Loss: 0.0948820635676384\n",
      "Step: 374, Loss: 0.09472735971212387\n",
      "Step: 375, Loss: 0.09491482377052307\n",
      "Step: 376, Loss: 0.09515144675970078\n",
      "Step: 377, Loss: 0.09484288096427917\n",
      "Step: 378, Loss: 0.09495251625776291\n",
      "Step: 379, Loss: 0.09507514536380768\n",
      "Step: 380, Loss: 0.09481535106897354\n",
      "Step: 381, Loss: 0.09459096193313599\n",
      "Step: 382, Loss: 0.09495975077152252\n",
      "Step: 383, Loss: 0.0949351117014885\n",
      "Step: 384, Loss: 0.09485258162021637\n",
      "Step: 385, Loss: 0.09503727406263351\n",
      "Step: 386, Loss: 0.09497521072626114\n",
      "Step: 387, Loss: 0.09472174942493439\n",
      "Step: 388, Loss: 0.09444175660610199\n",
      "Step: 389, Loss: 0.095175601541996\n",
      "Step: 390, Loss: 0.09489590674638748\n",
      "Step: 391, Loss: 0.09491130709648132\n",
      "Step: 392, Loss: 0.09503770619630814\n",
      "Step: 393, Loss: 0.09465484321117401\n",
      "Step: 394, Loss: 0.09455186873674393\n",
      "Step: 395, Loss: 0.0948072075843811\n",
      "Step: 396, Loss: 0.09484072774648666\n",
      "Step: 397, Loss: 0.09475866705179214\n",
      "Step: 398, Loss: 0.09467311203479767\n",
      "Step: 399, Loss: 0.09487482160329819\n",
      "Step: 400, Loss: 0.09543292969465256\n",
      "Step: 401, Loss: 0.09456711262464523\n",
      "Step: 402, Loss: 0.09508932381868362\n",
      "Step: 403, Loss: 0.09470703452825546\n",
      "Step: 404, Loss: 0.09500294178724289\n",
      "Step: 405, Loss: 0.0946538969874382\n",
      "Step: 406, Loss: 0.09478852897882462\n",
      "Step: 407, Loss: 0.09472154825925827\n",
      "Step: 408, Loss: 0.09454699605703354\n",
      "Step: 409, Loss: 0.09518889337778091\n",
      "Step: 410, Loss: 0.09462758898735046\n",
      "Step: 411, Loss: 0.09468132257461548\n",
      "Step: 412, Loss: 0.09492256492376328\n",
      "Step: 413, Loss: 0.09494657814502716\n",
      "Step: 414, Loss: 0.09473124891519547\n",
      "Step: 415, Loss: 0.09486525505781174\n",
      "Step: 416, Loss: 0.09492731839418411\n",
      "Step: 417, Loss: 0.094855897128582\n",
      "Step: 418, Loss: 0.09457827359437943\n",
      "Step: 419, Loss: 0.09478134661912918\n",
      "Step: 420, Loss: 0.09487248957157135\n",
      "Step: 421, Loss: 0.0948362797498703\n",
      "Step: 422, Loss: 0.09468018263578415\n",
      "Step: 423, Loss: 0.09436851739883423\n",
      "Step: 424, Loss: 0.09467592090368271\n",
      "Step: 425, Loss: 0.09454648941755295\n",
      "Step: 426, Loss: 0.09490714222192764\n",
      "Step: 427, Loss: 0.09489377588033676\n",
      "Step: 428, Loss: 0.09483902156352997\n",
      "Step: 429, Loss: 0.0949130728840828\n",
      "Step: 430, Loss: 0.09513140469789505\n",
      "Step: 431, Loss: 0.09509597718715668\n",
      "Step: 432, Loss: 0.0943477675318718\n",
      "Step: 433, Loss: 0.09481009840965271\n",
      "Step: 434, Loss: 0.09474223107099533\n",
      "Step: 435, Loss: 0.09506203234195709\n",
      "Step: 436, Loss: 0.0949152261018753\n",
      "Step: 437, Loss: 0.09481502324342728\n",
      "Step: 438, Loss: 0.09475433081388474\n",
      "Step: 439, Loss: 0.09441518038511276\n",
      "Step: 440, Loss: 0.09476635605096817\n",
      "Step: 441, Loss: 0.09436734765768051\n",
      "Step: 442, Loss: 0.09431619942188263\n",
      "Step: 443, Loss: 0.09459775686264038\n",
      "Step: 444, Loss: 0.0947457104921341\n",
      "Step: 445, Loss: 0.09443474560976028\n",
      "Step: 446, Loss: 0.09488968551158905\n",
      "Step: 447, Loss: 0.09429299831390381\n",
      "Step: 448, Loss: 0.09468407183885574\n",
      "Step: 449, Loss: 0.09458159655332565\n",
      "Step: 450, Loss: 0.09438937902450562\n",
      "Step: 451, Loss: 0.09473083913326263\n",
      "Step: 452, Loss: 0.09441029280424118\n",
      "Step: 453, Loss: 0.09482547640800476\n",
      "Step: 454, Loss: 0.09486085176467896\n",
      "Step: 455, Loss: 0.09453972429037094\n",
      "Step: 456, Loss: 0.09422893822193146\n",
      "Step: 457, Loss: 0.09460677951574326\n",
      "Step: 458, Loss: 0.09457951039075851\n",
      "Step: 459, Loss: 0.09477721154689789\n",
      "Step: 460, Loss: 0.09458494931459427\n",
      "Step: 461, Loss: 0.09423597157001495\n",
      "Step: 462, Loss: 0.09516377002000809\n",
      "Step: 463, Loss: 0.09486105293035507\n",
      "Step: 464, Loss: 0.09450212866067886\n",
      "Step: 465, Loss: 0.09428399801254272\n",
      "Step: 466, Loss: 0.09440763294696808\n",
      "Step: 467, Loss: 0.09441405534744263\n",
      "Step: 468, Loss: 0.09436824172735214\n",
      "Step: 469, Loss: 0.09479749202728271\n",
      "Step: 470, Loss: 0.09449354559183121\n",
      "Step: 471, Loss: 0.09458249062299728\n",
      "Step: 472, Loss: 0.09492824971675873\n",
      "Step: 473, Loss: 0.09454255551099777\n",
      "Step: 474, Loss: 0.09480360895395279\n",
      "Step: 475, Loss: 0.09485546499490738\n",
      "Step: 476, Loss: 0.09467288106679916\n",
      "Step: 477, Loss: 0.0950903445482254\n",
      "Step: 478, Loss: 0.09452252835035324\n",
      "Step: 479, Loss: 0.09496515989303589\n",
      "Step: 480, Loss: 0.09515202045440674\n",
      "Step: 481, Loss: 0.09468233585357666\n",
      "Step: 482, Loss: 0.0948048084974289\n",
      "Step: 483, Loss: 0.09511683136224747\n",
      "Step: 484, Loss: 0.09487418085336685\n",
      "Step: 485, Loss: 0.09501238912343979\n",
      "Step: 486, Loss: 0.09507723152637482\n",
      "Step: 487, Loss: 0.09538944065570831\n",
      "Step: 488, Loss: 0.09599099308252335\n",
      "Step: 489, Loss: 0.09600081294775009\n",
      "Step: 490, Loss: 0.0955032929778099\n",
      "Step: 491, Loss: 0.09620770066976547\n",
      "Step: 492, Loss: 0.09623126685619354\n",
      "Step: 493, Loss: 0.0963004007935524\n",
      "Step: 494, Loss: 0.09587936848402023\n",
      "Step: 495, Loss: 0.09636253863573074\n",
      "Step: 496, Loss: 0.0965772494673729\n",
      "Step: 497, Loss: 0.09623628109693527\n",
      "Step: 498, Loss: 0.09620187431573868\n",
      "Step: 499, Loss: 0.09662467241287231\n",
      "Step: 500, Loss: 0.09618374705314636\n",
      "Step: 501, Loss: 0.09609997272491455\n",
      "Step: 502, Loss: 0.09686818718910217\n",
      "Step: 503, Loss: 0.09588273614645004\n",
      "Step: 504, Loss: 0.09625813364982605\n",
      "Step: 505, Loss: 0.09635133296251297\n",
      "Step: 506, Loss: 0.0961725264787674\n",
      "Step: 507, Loss: 0.09640872478485107\n",
      "Step: 508, Loss: 0.0962180495262146\n",
      "Step: 509, Loss: 0.09612058848142624\n",
      "Step: 510, Loss: 0.09633144736289978\n",
      "Step: 511, Loss: 0.09591421484947205\n",
      "Step: 512, Loss: 0.09642114490270615\n",
      "Step: 513, Loss: 0.0965234711766243\n",
      "Step: 514, Loss: 0.09667255729436874\n",
      "Step: 515, Loss: 0.09644422680139542\n",
      "Step: 516, Loss: 0.09630389511585236\n",
      "Step: 517, Loss: 0.09618997573852539\n",
      "Step: 518, Loss: 0.09650391340255737\n",
      "Step: 519, Loss: 0.09657212346792221\n",
      "Step: 520, Loss: 0.09632332623004913\n",
      "Step: 521, Loss: 0.09625913947820663\n",
      "Step: 522, Loss: 0.09620688110589981\n",
      "Step: 523, Loss: 0.09635896980762482\n",
      "Step: 524, Loss: 0.09634864330291748\n",
      "Step: 525, Loss: 0.09651229530572891\n",
      "Step: 526, Loss: 0.09609948098659515\n",
      "Step: 527, Loss: 0.09614985436201096\n",
      "Step: 528, Loss: 0.09650269150733948\n",
      "Step: 529, Loss: 0.09633266925811768\n",
      "Step: 530, Loss: 0.09597524255514145\n",
      "Step: 531, Loss: 0.09603230655193329\n",
      "Step: 532, Loss: 0.09663011133670807\n",
      "Step: 533, Loss: 0.09640214592218399\n",
      "Step: 534, Loss: 0.09636536985635757\n",
      "Step: 535, Loss: 0.0964302122592926\n",
      "Step: 536, Loss: 0.09630957990884781\n",
      "Step: 537, Loss: 0.095991350710392\n",
      "Step: 538, Loss: 0.09623105823993683\n",
      "Step: 539, Loss: 0.09593416005373001\n",
      "Step: 540, Loss: 0.09621013700962067\n",
      "Step: 541, Loss: 0.09579209238290787\n",
      "Step: 542, Loss: 0.09620118886232376\n",
      "Step: 543, Loss: 0.09616557508707047\n",
      "Step: 544, Loss: 0.09559541940689087\n",
      "Step: 545, Loss: 0.0958644226193428\n",
      "Step: 546, Loss: 0.09602588415145874\n",
      "Step: 547, Loss: 0.09621191024780273\n",
      "Step: 548, Loss: 0.09637132287025452\n",
      "Step: 549, Loss: 0.09631984680891037\n",
      "Step: 550, Loss: 0.09610238671302795\n",
      "Step: 551, Loss: 0.09624011814594269\n",
      "Step: 552, Loss: 0.09613174945116043\n",
      "Step: 553, Loss: 0.09583671391010284\n",
      "Step: 554, Loss: 0.09637486189603806\n",
      "Step: 555, Loss: 0.0957302674651146\n",
      "Step: 556, Loss: 0.09605114161968231\n",
      "Step: 557, Loss: 0.09611696004867554\n",
      "Step: 558, Loss: 0.09595579653978348\n",
      "Step: 559, Loss: 0.09604397416114807\n",
      "Step: 560, Loss: 0.095741868019104\n",
      "Step: 561, Loss: 0.09581782668828964\n",
      "Step: 562, Loss: 0.09584352374076843\n",
      "Step: 563, Loss: 0.09574389457702637\n",
      "Step: 564, Loss: 0.09556180983781815\n",
      "Step: 565, Loss: 0.09612347185611725\n",
      "Step: 566, Loss: 0.0954737663269043\n",
      "Step: 567, Loss: 0.09572804719209671\n",
      "Step: 568, Loss: 0.09535472840070724\n",
      "Step: 569, Loss: 0.09533949941396713\n",
      "Step: 570, Loss: 0.09551618248224258\n",
      "Step: 571, Loss: 0.0954708531498909\n",
      "Step: 572, Loss: 0.09505010396242142\n",
      "Step: 573, Loss: 0.09563460946083069\n",
      "Step: 574, Loss: 0.09511880576610565\n",
      "Step: 575, Loss: 0.09508984535932541\n",
      "Step: 576, Loss: 0.09552499651908875\n",
      "Step: 577, Loss: 0.09531144797801971\n",
      "Step: 578, Loss: 0.09481053054332733\n",
      "Step: 579, Loss: 0.09489767253398895\n",
      "Step: 580, Loss: 0.09486143290996552\n",
      "Step: 581, Loss: 0.09544782340526581\n",
      "Step: 582, Loss: 0.09514530748128891\n",
      "Step: 583, Loss: 0.09514150023460388\n",
      "Step: 584, Loss: 0.09463422000408173\n",
      "Step: 585, Loss: 0.09520310163497925\n",
      "Step: 586, Loss: 0.09491885453462601\n",
      "Step: 587, Loss: 0.09550818055868149\n",
      "Step: 588, Loss: 0.0947786271572113\n",
      "Step: 589, Loss: 0.09468673169612885\n",
      "Step: 590, Loss: 0.09574437141418457\n",
      "Step: 591, Loss: 0.09523213654756546\n",
      "Step: 592, Loss: 0.09473400563001633\n",
      "Step: 593, Loss: 0.09507128596305847\n",
      "Step: 594, Loss: 0.0954957827925682\n",
      "Step: 595, Loss: 0.09483809769153595\n",
      "Step: 596, Loss: 0.0953514501452446\n",
      "Step: 597, Loss: 0.09488142281770706\n",
      "Step: 598, Loss: 0.09457393735647202\n",
      "Step: 599, Loss: 0.09478125721216202\n",
      "Final loss:  0.09478126\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/ml_tn_tn_twcrps_mean13.0_std2.0_constant0.05000000074505806_epochs600_folds_1_2_mean0_1_2_3_4_std0_1_2_3_4_5_.pkl\n"
     ]
    }
   ],
   "source": [
    "model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    all_features,\n",
    "    location_features,\n",
    "    scale_features,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_constant = chain_function_constant,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained,\n",
    "    random_init = random_init\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Final loss:  0.10335002\n",
      "Final loss:  0.101744905\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using given parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 0.10101192444562912\n",
      "Step: 1, Loss: 0.1045975387096405\n",
      "Step: 2, Loss: 0.10153059661388397\n",
      "Step: 3, Loss: 0.10202322900295258\n",
      "Step: 4, Loss: 0.1033298522233963\n",
      "Step: 5, Loss: 0.10229266434907913\n",
      "Step: 6, Loss: 0.1007765606045723\n",
      "Step: 7, Loss: 0.10148461908102036\n",
      "Step: 8, Loss: 0.10205074399709702\n",
      "Step: 9, Loss: 0.10181190073490143\n",
      "Step: 10, Loss: 0.10107484459877014\n",
      "Step: 11, Loss: 0.10078710317611694\n",
      "Step: 12, Loss: 0.1010366752743721\n",
      "Step: 13, Loss: 0.10091517120599747\n",
      "Step: 14, Loss: 0.1011858657002449\n",
      "Step: 15, Loss: 0.10120128095149994\n",
      "Step: 16, Loss: 0.10065849870443344\n",
      "Step: 17, Loss: 0.10093311220407486\n",
      "Step: 18, Loss: 0.10144681483507156\n",
      "Step: 19, Loss: 0.10081422328948975\n",
      "Step: 20, Loss: 0.100248321890831\n",
      "Step: 21, Loss: 0.10068947076797485\n",
      "Step: 22, Loss: 0.10042747110128403\n",
      "Step: 23, Loss: 0.10088712722063065\n",
      "Step: 24, Loss: 0.1005655974149704\n",
      "Step: 25, Loss: 0.10123177617788315\n",
      "Step: 26, Loss: 0.10017429292201996\n",
      "Step: 27, Loss: 0.10028838366270065\n",
      "Step: 28, Loss: 0.10079685598611832\n",
      "Step: 29, Loss: 0.1003103256225586\n",
      "Step: 30, Loss: 0.10113375633955002\n",
      "Step: 31, Loss: 0.10008753091096878\n",
      "Step: 32, Loss: 0.10060267895460129\n",
      "Step: 33, Loss: 0.09993112087249756\n",
      "Step: 34, Loss: 0.10006285458803177\n",
      "Step: 35, Loss: 0.10051048547029495\n",
      "Step: 36, Loss: 0.10016465932130814\n",
      "Step: 37, Loss: 0.1008554995059967\n",
      "Step: 38, Loss: 0.10040871053934097\n",
      "Step: 39, Loss: 0.10027137398719788\n",
      "Step: 40, Loss: 0.10040444135665894\n",
      "Step: 41, Loss: 0.10068489611148834\n",
      "Step: 42, Loss: 0.10042519867420197\n",
      "Step: 43, Loss: 0.1005309671163559\n",
      "Step: 44, Loss: 0.100456103682518\n",
      "Step: 45, Loss: 0.10045760124921799\n",
      "Step: 46, Loss: 0.1009252518415451\n",
      "Step: 47, Loss: 0.10039106011390686\n",
      "Step: 48, Loss: 0.10039254277944565\n",
      "Step: 49, Loss: 0.10032002627849579\n",
      "Step: 50, Loss: 0.10016867518424988\n",
      "Step: 51, Loss: 0.10017027705907822\n",
      "Step: 52, Loss: 0.10000988841056824\n",
      "Step: 53, Loss: 0.10024469345808029\n",
      "Step: 54, Loss: 0.09989137947559357\n",
      "Step: 55, Loss: 0.10024677962064743\n",
      "Step: 56, Loss: 0.0998087152838707\n",
      "Step: 57, Loss: 0.10010010749101639\n",
      "Step: 58, Loss: 0.10027453303337097\n",
      "Step: 59, Loss: 0.10038748383522034\n",
      "Step: 60, Loss: 0.09995193034410477\n",
      "Step: 61, Loss: 0.10020797699689865\n",
      "Step: 62, Loss: 0.10054070502519608\n",
      "Step: 63, Loss: 0.09990429133176804\n",
      "Step: 64, Loss: 0.10013564676046371\n",
      "Step: 65, Loss: 0.10048423707485199\n",
      "Step: 66, Loss: 0.10039737820625305\n",
      "Step: 67, Loss: 0.10024860501289368\n",
      "Step: 68, Loss: 0.10020972788333893\n",
      "Step: 69, Loss: 0.1000327616930008\n",
      "Step: 70, Loss: 0.10040455311536789\n",
      "Step: 71, Loss: 0.10010264068841934\n",
      "Step: 72, Loss: 0.100120410323143\n",
      "Step: 73, Loss: 0.10007312148809433\n",
      "Step: 74, Loss: 0.09965398907661438\n",
      "Step: 75, Loss: 0.10011614859104156\n",
      "Step: 76, Loss: 0.1004413589835167\n",
      "Step: 77, Loss: 0.10010948032140732\n",
      "Step: 78, Loss: 0.10064411908388138\n",
      "Step: 79, Loss: 0.10035866498947144\n",
      "Step: 80, Loss: 0.10028905421495438\n",
      "Step: 81, Loss: 0.10032825917005539\n",
      "Step: 82, Loss: 0.09956841170787811\n",
      "Step: 83, Loss: 0.1001385971903801\n",
      "Step: 84, Loss: 0.10013581067323685\n",
      "Step: 85, Loss: 0.10000420361757278\n",
      "Step: 86, Loss: 0.09996264427900314\n",
      "Step: 87, Loss: 0.10030550509691238\n",
      "Step: 88, Loss: 0.09974067658185959\n",
      "Step: 89, Loss: 0.10065500438213348\n",
      "Step: 90, Loss: 0.10021546483039856\n",
      "Step: 91, Loss: 0.10012366622686386\n",
      "Step: 92, Loss: 0.1002422347664833\n",
      "Step: 93, Loss: 0.09985672682523727\n",
      "Step: 94, Loss: 0.10030867159366608\n",
      "Step: 95, Loss: 0.09985712915658951\n",
      "Step: 96, Loss: 0.10033926367759705\n",
      "Step: 97, Loss: 0.10037868469953537\n",
      "Step: 98, Loss: 0.099961057305336\n",
      "Step: 99, Loss: 0.10020636022090912\n",
      "Step: 100, Loss: 0.09966293722391129\n",
      "Step: 101, Loss: 0.10012321919202805\n",
      "Step: 102, Loss: 0.10028219223022461\n",
      "Step: 103, Loss: 0.10012897104024887\n",
      "Step: 104, Loss: 0.10037192702293396\n",
      "Step: 105, Loss: 0.10018285363912582\n",
      "Step: 106, Loss: 0.10033261775970459\n",
      "Step: 107, Loss: 0.10020512342453003\n",
      "Step: 108, Loss: 0.09996239095926285\n",
      "Step: 109, Loss: 0.09985598176717758\n",
      "Step: 110, Loss: 0.10007429122924805\n",
      "Step: 111, Loss: 0.10003470629453659\n",
      "Step: 112, Loss: 0.10045289248228073\n",
      "Step: 113, Loss: 0.1001242995262146\n",
      "Step: 114, Loss: 0.1000967025756836\n",
      "Step: 115, Loss: 0.10052905231714249\n",
      "Step: 116, Loss: 0.10022064298391342\n",
      "Step: 117, Loss: 0.10005631297826767\n",
      "Step: 118, Loss: 0.1000601276755333\n",
      "Step: 119, Loss: 0.09992896765470505\n",
      "Step: 120, Loss: 0.10031336545944214\n",
      "Step: 121, Loss: 0.10004186630249023\n",
      "Step: 122, Loss: 0.09985711425542831\n",
      "Step: 123, Loss: 0.09995575249195099\n",
      "Step: 124, Loss: 0.0998339056968689\n",
      "Step: 125, Loss: 0.10030561685562134\n",
      "Step: 126, Loss: 0.10000582784414291\n",
      "Step: 127, Loss: 0.1001092940568924\n",
      "Step: 128, Loss: 0.09972014278173447\n",
      "Step: 129, Loss: 0.09994105994701385\n",
      "Step: 130, Loss: 0.1002492755651474\n",
      "Step: 131, Loss: 0.0996551588177681\n",
      "Step: 132, Loss: 0.10011706501245499\n",
      "Step: 133, Loss: 0.10062992572784424\n",
      "Step: 134, Loss: 0.10021651536226273\n",
      "Step: 135, Loss: 0.10009622573852539\n",
      "Step: 136, Loss: 0.09982169419527054\n",
      "Step: 137, Loss: 0.1003158912062645\n",
      "Step: 138, Loss: 0.10016825795173645\n",
      "Step: 139, Loss: 0.10030484199523926\n",
      "Step: 140, Loss: 0.09976093471050262\n",
      "Step: 141, Loss: 0.10006352514028549\n",
      "Step: 142, Loss: 0.1001281887292862\n",
      "Step: 143, Loss: 0.0998583659529686\n",
      "Step: 144, Loss: 0.09987346082925797\n",
      "Step: 145, Loss: 0.09980994462966919\n",
      "Step: 146, Loss: 0.0999317392706871\n",
      "Step: 147, Loss: 0.09982917457818985\n",
      "Step: 148, Loss: 0.09982779622077942\n",
      "Step: 149, Loss: 0.10061391443014145\n",
      "Step: 150, Loss: 0.09979795664548874\n",
      "Step: 151, Loss: 0.09970931708812714\n",
      "Step: 152, Loss: 0.09963594377040863\n",
      "Step: 153, Loss: 0.09963522106409073\n",
      "Step: 154, Loss: 0.10000116378068924\n",
      "Step: 155, Loss: 0.09996750950813293\n",
      "Step: 156, Loss: 0.09971670806407928\n",
      "Step: 157, Loss: 0.10017547011375427\n",
      "Step: 158, Loss: 0.10012545436620712\n",
      "Step: 159, Loss: 0.1003195270895958\n",
      "Step: 160, Loss: 0.09978775680065155\n",
      "Step: 161, Loss: 0.0996885895729065\n",
      "Step: 162, Loss: 0.09987661987543106\n",
      "Step: 163, Loss: 0.09971199184656143\n",
      "Step: 164, Loss: 0.09981833398342133\n",
      "Step: 165, Loss: 0.09961768984794617\n",
      "Step: 166, Loss: 0.099788136780262\n",
      "Step: 167, Loss: 0.09958240389823914\n",
      "Step: 168, Loss: 0.0994737520813942\n",
      "Step: 169, Loss: 0.09954477846622467\n",
      "Step: 170, Loss: 0.09974025934934616\n",
      "Step: 171, Loss: 0.09960589557886124\n",
      "Step: 172, Loss: 0.09965706616640091\n",
      "Step: 173, Loss: 0.10000422596931458\n",
      "Step: 174, Loss: 0.09968196600675583\n",
      "Step: 175, Loss: 0.09961492568254471\n",
      "Step: 176, Loss: 0.09962505102157593\n",
      "Step: 177, Loss: 0.09971467405557632\n",
      "Step: 178, Loss: 0.09962061792612076\n",
      "Step: 179, Loss: 0.10008540749549866\n",
      "Step: 180, Loss: 0.09984629601240158\n",
      "Step: 181, Loss: 0.09999776631593704\n",
      "Step: 182, Loss: 0.09971991181373596\n",
      "Step: 183, Loss: 0.09985431283712387\n",
      "Step: 184, Loss: 0.09943965077400208\n",
      "Step: 185, Loss: 0.09947750717401505\n",
      "Step: 186, Loss: 0.09945524483919144\n",
      "Step: 187, Loss: 0.09984668344259262\n",
      "Step: 188, Loss: 0.09993193298578262\n",
      "Step: 189, Loss: 0.10006702691316605\n",
      "Step: 190, Loss: 0.09995385259389877\n",
      "Step: 191, Loss: 0.09992042928934097\n",
      "Step: 192, Loss: 0.09999304264783859\n",
      "Step: 193, Loss: 0.09988008439540863\n",
      "Step: 194, Loss: 0.10006505995988846\n",
      "Step: 195, Loss: 0.0996040552854538\n",
      "Step: 196, Loss: 0.09975820779800415\n",
      "Step: 197, Loss: 0.0998237207531929\n",
      "Step: 198, Loss: 0.10023599117994308\n",
      "Step: 199, Loss: 0.09960661828517914\n",
      "Step: 200, Loss: 0.10003604739904404\n",
      "Step: 201, Loss: 0.09946393966674805\n",
      "Step: 202, Loss: 0.100131094455719\n",
      "Step: 203, Loss: 0.09991443157196045\n",
      "Step: 204, Loss: 0.09985866397619247\n",
      "Step: 205, Loss: 0.09977446496486664\n",
      "Step: 206, Loss: 0.09983466565608978\n",
      "Step: 207, Loss: 0.09991797804832458\n",
      "Step: 208, Loss: 0.10013588517904282\n",
      "Step: 209, Loss: 0.09971050173044205\n",
      "Step: 210, Loss: 0.09985184669494629\n",
      "Step: 211, Loss: 0.10012660920619965\n",
      "Step: 212, Loss: 0.10011982917785645\n",
      "Step: 213, Loss: 0.10008106380701065\n",
      "Step: 214, Loss: 0.0996464341878891\n",
      "Step: 215, Loss: 0.09952999651432037\n",
      "Step: 216, Loss: 0.09975336492061615\n",
      "Step: 217, Loss: 0.10034859925508499\n",
      "Step: 218, Loss: 0.10001468658447266\n",
      "Step: 219, Loss: 0.09998709708452225\n",
      "Step: 220, Loss: 0.09959638863801956\n",
      "Step: 221, Loss: 0.10007799416780472\n",
      "Step: 222, Loss: 0.09981153905391693\n",
      "Step: 223, Loss: 0.09976130723953247\n",
      "Step: 224, Loss: 0.09986881166696548\n",
      "Step: 225, Loss: 0.1004132404923439\n",
      "Step: 226, Loss: 0.10018006712198257\n",
      "Step: 227, Loss: 0.09992454946041107\n",
      "Step: 228, Loss: 0.09985758364200592\n",
      "Step: 229, Loss: 0.10003688186407089\n",
      "Step: 230, Loss: 0.09983201324939728\n",
      "Step: 231, Loss: 0.09984076023101807\n",
      "Step: 232, Loss: 0.09968853741884232\n",
      "Step: 233, Loss: 0.10021143406629562\n",
      "Step: 234, Loss: 0.10005252063274384\n",
      "Step: 235, Loss: 0.09953856468200684\n",
      "Step: 236, Loss: 0.09988480806350708\n",
      "Step: 237, Loss: 0.09968262910842896\n",
      "Step: 238, Loss: 0.09969595074653625\n",
      "Step: 239, Loss: 0.09975699335336685\n",
      "Step: 240, Loss: 0.09970810264348984\n",
      "Step: 241, Loss: 0.09993459284305573\n",
      "Step: 242, Loss: 0.10000336170196533\n",
      "Step: 243, Loss: 0.10025124996900558\n",
      "Step: 244, Loss: 0.09998960793018341\n",
      "Step: 245, Loss: 0.09986984729766846\n",
      "Step: 246, Loss: 0.09957825392484665\n",
      "Step: 247, Loss: 0.09991302341222763\n",
      "Step: 248, Loss: 0.09960884600877762\n",
      "Step: 249, Loss: 0.10005608946084976\n",
      "Step: 250, Loss: 0.10001975297927856\n",
      "Step: 251, Loss: 0.10007541626691818\n",
      "Step: 252, Loss: 0.09981565922498703\n",
      "Step: 253, Loss: 0.0997411459684372\n",
      "Step: 254, Loss: 0.09985959529876709\n",
      "Step: 255, Loss: 0.09977702796459198\n",
      "Step: 256, Loss: 0.09994734078645706\n",
      "Step: 257, Loss: 0.09973761439323425\n",
      "Step: 258, Loss: 0.09974502772092819\n",
      "Step: 259, Loss: 0.09975292533636093\n",
      "Step: 260, Loss: 0.0995725467801094\n",
      "Step: 261, Loss: 0.09971857070922852\n",
      "Step: 262, Loss: 0.09960460662841797\n",
      "Step: 263, Loss: 0.09983714669942856\n",
      "Step: 264, Loss: 0.0994969978928566\n",
      "Step: 265, Loss: 0.09993026405572891\n",
      "Step: 266, Loss: 0.10000336170196533\n",
      "Step: 267, Loss: 0.0997394397854805\n",
      "Step: 268, Loss: 0.0997757539153099\n",
      "Step: 269, Loss: 0.10010257363319397\n",
      "Step: 270, Loss: 0.10022033751010895\n",
      "Step: 271, Loss: 0.09982910007238388\n",
      "Step: 272, Loss: 0.1000540629029274\n",
      "Step: 273, Loss: 0.09944810718297958\n",
      "Step: 274, Loss: 0.09984477609395981\n",
      "Step: 275, Loss: 0.0992569848895073\n",
      "Step: 276, Loss: 0.09955143183469772\n",
      "Step: 277, Loss: 0.09981421381235123\n",
      "Step: 278, Loss: 0.09991719573736191\n",
      "Step: 279, Loss: 0.0994381308555603\n",
      "Step: 280, Loss: 0.09965036064386368\n",
      "Step: 281, Loss: 0.09970209002494812\n",
      "Step: 282, Loss: 0.10018698871135712\n",
      "Step: 283, Loss: 0.09987757354974747\n",
      "Step: 284, Loss: 0.0996500626206398\n",
      "Step: 285, Loss: 0.100091852247715\n",
      "Step: 286, Loss: 0.09989342093467712\n",
      "Step: 287, Loss: 0.10021798312664032\n",
      "Step: 288, Loss: 0.0999566838145256\n",
      "Step: 289, Loss: 0.0995660126209259\n",
      "Step: 290, Loss: 0.09934667497873306\n",
      "Step: 291, Loss: 0.0996696874499321\n",
      "Step: 292, Loss: 0.09985460340976715\n",
      "Step: 293, Loss: 0.09978452324867249\n",
      "Step: 294, Loss: 0.09970322996377945\n",
      "Step: 295, Loss: 0.09984125941991806\n",
      "Step: 296, Loss: 0.09951096773147583\n",
      "Step: 297, Loss: 0.09961052238941193\n",
      "Step: 298, Loss: 0.09987451136112213\n",
      "Step: 299, Loss: 0.09990452975034714\n",
      "Step: 300, Loss: 0.10009047389030457\n",
      "Step: 301, Loss: 0.09997782856225967\n",
      "Step: 302, Loss: 0.09924758970737457\n",
      "Step: 303, Loss: 0.09946586191654205\n",
      "Step: 304, Loss: 0.0998670756816864\n",
      "Step: 305, Loss: 0.10005007684230804\n",
      "Step: 306, Loss: 0.10001236945390701\n",
      "Step: 307, Loss: 0.09990290552377701\n",
      "Step: 308, Loss: 0.09959187358617783\n",
      "Step: 309, Loss: 0.1002177894115448\n",
      "Step: 310, Loss: 0.09970739483833313\n",
      "Step: 311, Loss: 0.09943041950464249\n",
      "Step: 312, Loss: 0.0995108038187027\n",
      "Step: 313, Loss: 0.09992336481809616\n",
      "Step: 314, Loss: 0.09930036962032318\n",
      "Step: 315, Loss: 0.09970066696405411\n",
      "Step: 316, Loss: 0.09972327202558517\n",
      "Step: 317, Loss: 0.09958726167678833\n",
      "Step: 318, Loss: 0.09994520246982574\n",
      "Step: 319, Loss: 0.09952209144830704\n",
      "Step: 320, Loss: 0.09948418289422989\n",
      "Step: 321, Loss: 0.09951978921890259\n",
      "Step: 322, Loss: 0.09943463653326035\n",
      "Step: 323, Loss: 0.09984159469604492\n",
      "Step: 324, Loss: 0.09962151199579239\n",
      "Step: 325, Loss: 0.0996653139591217\n",
      "Step: 326, Loss: 0.09927761554718018\n",
      "Step: 327, Loss: 0.09962236881256104\n",
      "Step: 328, Loss: 0.0997888371348381\n",
      "Step: 329, Loss: 0.09921903908252716\n",
      "Step: 330, Loss: 0.10006149858236313\n",
      "Step: 331, Loss: 0.0994364321231842\n",
      "Step: 332, Loss: 0.10026399791240692\n",
      "Step: 333, Loss: 0.09971509873867035\n",
      "Step: 334, Loss: 0.10039117187261581\n",
      "Step: 335, Loss: 0.09976818412542343\n",
      "Step: 336, Loss: 0.09957128018140793\n",
      "Step: 337, Loss: 0.10015113651752472\n",
      "Step: 338, Loss: 0.10032059252262115\n",
      "Step: 339, Loss: 0.09994091838598251\n",
      "Step: 340, Loss: 0.10010998696088791\n",
      "Step: 341, Loss: 0.09978180378675461\n",
      "Step: 342, Loss: 0.09978150576353073\n",
      "Step: 343, Loss: 0.10012223571538925\n",
      "Step: 344, Loss: 0.09989391267299652\n",
      "Step: 345, Loss: 0.09989049285650253\n",
      "Step: 346, Loss: 0.10021966695785522\n",
      "Step: 347, Loss: 0.10012882947921753\n",
      "Step: 348, Loss: 0.09990854561328888\n",
      "Step: 349, Loss: 0.09975525736808777\n",
      "Step: 350, Loss: 0.10012611001729965\n",
      "Step: 351, Loss: 0.09984101355075836\n",
      "Step: 352, Loss: 0.09986074268817902\n",
      "Step: 353, Loss: 0.09975353628396988\n",
      "Step: 354, Loss: 0.09982657432556152\n",
      "Step: 355, Loss: 0.09984082728624344\n",
      "Step: 356, Loss: 0.10024143010377884\n",
      "Step: 357, Loss: 0.10007154941558838\n",
      "Step: 358, Loss: 0.10003611445426941\n",
      "Step: 359, Loss: 0.10013946890830994\n",
      "Step: 360, Loss: 0.10002493113279343\n",
      "Step: 361, Loss: 0.10018520057201385\n",
      "Step: 362, Loss: 0.09982316195964813\n",
      "Step: 363, Loss: 0.1001284196972847\n",
      "Step: 364, Loss: 0.10002555698156357\n",
      "Step: 365, Loss: 0.09970288723707199\n",
      "Step: 366, Loss: 0.09989278018474579\n",
      "Step: 367, Loss: 0.1003192737698555\n",
      "Step: 368, Loss: 0.10013553500175476\n",
      "Step: 369, Loss: 0.09986501932144165\n",
      "Step: 370, Loss: 0.10005918145179749\n",
      "Step: 371, Loss: 0.10015542060136795\n",
      "Step: 372, Loss: 0.09939827024936676\n",
      "Step: 373, Loss: 0.10003519803285599\n",
      "Step: 374, Loss: 0.0998372957110405\n",
      "Step: 375, Loss: 0.10035349428653717\n",
      "Step: 376, Loss: 0.09981882572174072\n",
      "Step: 377, Loss: 0.09971339255571365\n",
      "Step: 378, Loss: 0.09950613230466843\n",
      "Step: 379, Loss: 0.0999230444431305\n",
      "Step: 380, Loss: 0.10002359747886658\n",
      "Step: 381, Loss: 0.10012052953243256\n",
      "Step: 382, Loss: 0.10015081614255905\n",
      "Step: 383, Loss: 0.09993571788072586\n",
      "Step: 384, Loss: 0.10028791427612305\n",
      "Step: 385, Loss: 0.09959328919649124\n",
      "Step: 386, Loss: 0.09993324428796768\n",
      "Step: 387, Loss: 0.09984131157398224\n",
      "Step: 388, Loss: 0.09991424530744553\n",
      "Step: 389, Loss: 0.10016492754220963\n",
      "Step: 390, Loss: 0.09998331218957901\n",
      "Step: 391, Loss: 0.09986256062984467\n",
      "Step: 392, Loss: 0.09996448457241058\n",
      "Step: 393, Loss: 0.09943445771932602\n",
      "Step: 394, Loss: 0.09974827617406845\n",
      "Step: 395, Loss: 0.10029754787683487\n",
      "Step: 396, Loss: 0.10010799765586853\n",
      "Step: 397, Loss: 0.09959585964679718\n",
      "Step: 398, Loss: 0.09976475685834885\n",
      "Step: 399, Loss: 0.09986741095781326\n",
      "Step: 400, Loss: 0.09970524162054062\n",
      "Step: 401, Loss: 0.09987582266330719\n",
      "Step: 402, Loss: 0.09949187934398651\n",
      "Step: 403, Loss: 0.09993467479944229\n",
      "Step: 404, Loss: 0.10013909637928009\n",
      "Step: 405, Loss: 0.09980069845914841\n",
      "Step: 406, Loss: 0.10038300603628159\n",
      "Step: 407, Loss: 0.09998966008424759\n",
      "Step: 408, Loss: 0.09987952560186386\n",
      "Step: 409, Loss: 0.0997140109539032\n",
      "Step: 410, Loss: 0.09966640919446945\n",
      "Step: 411, Loss: 0.09958469867706299\n",
      "Step: 412, Loss: 0.10002169013023376\n",
      "Step: 413, Loss: 0.09982061386108398\n",
      "Step: 414, Loss: 0.09994416683912277\n",
      "Step: 415, Loss: 0.09970349818468094\n",
      "Step: 416, Loss: 0.10004322975873947\n",
      "Step: 417, Loss: 0.0996941328048706\n",
      "Step: 418, Loss: 0.09992799162864685\n",
      "Step: 419, Loss: 0.10003471374511719\n",
      "Step: 420, Loss: 0.09996713697910309\n",
      "Step: 421, Loss: 0.09973790496587753\n",
      "Step: 422, Loss: 0.09977637976408005\n",
      "Step: 423, Loss: 0.10000914335250854\n",
      "Step: 424, Loss: 0.10004518181085587\n",
      "Step: 425, Loss: 0.10023046284914017\n",
      "Step: 426, Loss: 0.09995432198047638\n",
      "Step: 427, Loss: 0.09925954788923264\n",
      "Step: 428, Loss: 0.09991728514432907\n",
      "Step: 429, Loss: 0.09946966916322708\n",
      "Step: 430, Loss: 0.09996330738067627\n",
      "Step: 431, Loss: 0.09970986098051071\n",
      "Step: 432, Loss: 0.09970537573099136\n",
      "Step: 433, Loss: 0.10017500817775726\n",
      "Step: 434, Loss: 0.0995701476931572\n",
      "Step: 435, Loss: 0.09996020793914795\n",
      "Step: 436, Loss: 0.09986601769924164\n",
      "Step: 437, Loss: 0.09947854280471802\n",
      "Step: 438, Loss: 0.09946966916322708\n",
      "Step: 439, Loss: 0.09941795468330383\n",
      "Step: 440, Loss: 0.0998176634311676\n",
      "Step: 441, Loss: 0.0996798649430275\n",
      "Step: 442, Loss: 0.0996241644024849\n",
      "Step: 443, Loss: 0.09985533356666565\n",
      "Step: 444, Loss: 0.09977034479379654\n",
      "Step: 445, Loss: 0.09954078495502472\n",
      "Step: 446, Loss: 0.09973353892564774\n",
      "Step: 447, Loss: 0.09921430796384811\n",
      "Step: 448, Loss: 0.09962459653615952\n",
      "Step: 449, Loss: 0.09963539242744446\n",
      "Step: 450, Loss: 0.09927268326282501\n",
      "Step: 451, Loss: 0.09973824769258499\n",
      "Step: 452, Loss: 0.10009041428565979\n",
      "Step: 453, Loss: 0.09966740012168884\n",
      "Step: 454, Loss: 0.10032093524932861\n",
      "Step: 455, Loss: 0.10001173615455627\n",
      "Step: 456, Loss: 0.09977711737155914\n",
      "Step: 457, Loss: 0.09990888833999634\n",
      "Step: 458, Loss: 0.10095333307981491\n",
      "Step: 459, Loss: 0.10029976814985275\n",
      "Step: 460, Loss: 0.10055486112833023\n",
      "Step: 461, Loss: 0.100510373711586\n",
      "Step: 462, Loss: 0.1001894623041153\n",
      "Step: 463, Loss: 0.1000116840004921\n",
      "Step: 464, Loss: 0.10017205774784088\n",
      "Step: 465, Loss: 0.09987734258174896\n",
      "Step: 466, Loss: 0.09993784874677658\n",
      "Step: 467, Loss: 0.10050167143344879\n",
      "Step: 468, Loss: 0.10080045461654663\n",
      "Step: 469, Loss: 0.10109151899814606\n",
      "Step: 470, Loss: 0.10154446959495544\n",
      "Step: 471, Loss: 0.101442851126194\n",
      "Step: 472, Loss: 0.10067947208881378\n",
      "Step: 473, Loss: 0.10112825036048889\n",
      "Step: 474, Loss: 0.10031715780496597\n",
      "Step: 475, Loss: 0.10064329952001572\n",
      "Step: 476, Loss: 0.1008189469575882\n",
      "Step: 477, Loss: 0.1007021814584732\n",
      "Step: 478, Loss: 0.10026750713586807\n",
      "Step: 479, Loss: 0.10025402158498764\n",
      "Step: 480, Loss: 0.10001331567764282\n",
      "Step: 481, Loss: 0.10018754005432129\n",
      "Step: 482, Loss: 0.10039862245321274\n",
      "Step: 483, Loss: 0.10031452029943466\n",
      "Step: 484, Loss: 0.10024953633546829\n",
      "Step: 485, Loss: 0.10004188120365143\n",
      "Step: 486, Loss: 0.09993920475244522\n",
      "Step: 487, Loss: 0.10009193420410156\n",
      "Step: 488, Loss: 0.09955208003520966\n",
      "Step: 489, Loss: 0.09983590990304947\n",
      "Step: 490, Loss: 0.10010526329278946\n",
      "Step: 491, Loss: 0.09993577003479004\n",
      "Step: 492, Loss: 0.10001025348901749\n",
      "Step: 493, Loss: 0.09999268501996994\n",
      "Step: 494, Loss: 0.1001076027750969\n",
      "Step: 495, Loss: 0.09974785894155502\n",
      "Step: 496, Loss: 0.10024365037679672\n",
      "Step: 497, Loss: 0.09986631572246552\n",
      "Step: 498, Loss: 0.09976784884929657\n",
      "Step: 499, Loss: 0.10009763389825821\n",
      "Step: 500, Loss: 0.0998651534318924\n",
      "Step: 501, Loss: 0.0996825098991394\n",
      "Step: 502, Loss: 0.0994672030210495\n",
      "Step: 503, Loss: 0.09935992956161499\n",
      "Step: 504, Loss: 0.09979315102100372\n",
      "Step: 505, Loss: 0.09958283603191376\n",
      "Step: 506, Loss: 0.09996163845062256\n",
      "Step: 507, Loss: 0.09993566572666168\n",
      "Step: 508, Loss: 0.1003762036561966\n",
      "Step: 509, Loss: 0.10008284449577332\n",
      "Step: 510, Loss: 0.09933522343635559\n",
      "Step: 511, Loss: 0.0995444506406784\n",
      "Step: 512, Loss: 0.10031702369451523\n",
      "Step: 513, Loss: 0.09976620972156525\n",
      "Step: 514, Loss: 0.09976449608802795\n",
      "Step: 515, Loss: 0.0998372808098793\n",
      "Step: 516, Loss: 0.0995604619383812\n",
      "Step: 517, Loss: 0.09973584115505219\n",
      "Step: 518, Loss: 0.09963072091341019\n",
      "Step: 519, Loss: 0.09962606430053711\n",
      "Step: 520, Loss: 0.09978165477514267\n",
      "Step: 521, Loss: 0.0993596613407135\n",
      "Step: 522, Loss: 0.09998148679733276\n",
      "Step: 523, Loss: 0.1000947579741478\n",
      "Step: 524, Loss: 0.09979057312011719\n",
      "Step: 525, Loss: 0.1000363752245903\n",
      "Step: 526, Loss: 0.099674291908741\n",
      "Step: 527, Loss: 0.09972982108592987\n",
      "Step: 528, Loss: 0.09998524188995361\n",
      "Step: 529, Loss: 0.09960104525089264\n",
      "Step: 530, Loss: 0.09983497858047485\n",
      "Step: 531, Loss: 0.09992524981498718\n",
      "Step: 532, Loss: 0.09966707229614258\n",
      "Step: 533, Loss: 0.09958945959806442\n",
      "Step: 534, Loss: 0.09980438649654388\n",
      "Step: 535, Loss: 0.0994650349020958\n",
      "Step: 536, Loss: 0.09983722120523453\n",
      "Step: 537, Loss: 0.09963468462228775\n",
      "Step: 538, Loss: 0.09971854090690613\n",
      "Step: 539, Loss: 0.09986277669668198\n",
      "Step: 540, Loss: 0.09945572912693024\n",
      "Step: 541, Loss: 0.10004832595586777\n",
      "Step: 542, Loss: 0.09977001696825027\n",
      "Step: 543, Loss: 0.09960256516933441\n",
      "Step: 544, Loss: 0.09927938133478165\n",
      "Step: 545, Loss: 0.09926994144916534\n",
      "Step: 546, Loss: 0.09978073835372925\n",
      "Step: 547, Loss: 0.09961432218551636\n",
      "Step: 548, Loss: 0.09936332702636719\n",
      "Step: 549, Loss: 0.09980623424053192\n",
      "Step: 550, Loss: 0.09983431547880173\n",
      "Step: 551, Loss: 0.09938172250986099\n",
      "Step: 552, Loss: 0.09943331778049469\n",
      "Step: 553, Loss: 0.09966979175806046\n",
      "Step: 554, Loss: 0.09966657310724258\n",
      "Step: 555, Loss: 0.09943845868110657\n",
      "Step: 556, Loss: 0.09954114258289337\n",
      "Step: 557, Loss: 0.09956977516412735\n",
      "Step: 558, Loss: 0.09957275539636612\n",
      "Step: 559, Loss: 0.09975379705429077\n",
      "Step: 560, Loss: 0.09955392777919769\n",
      "Step: 561, Loss: 0.09936036169528961\n",
      "Step: 562, Loss: 0.09943887591362\n",
      "Step: 563, Loss: 0.09995297342538834\n",
      "Step: 564, Loss: 0.0995626151561737\n",
      "Step: 565, Loss: 0.09926672279834747\n",
      "Step: 566, Loss: 0.09947508573532104\n",
      "Step: 567, Loss: 0.09905603528022766\n",
      "Step: 568, Loss: 0.09952205419540405\n",
      "Step: 569, Loss: 0.09928470104932785\n",
      "Step: 570, Loss: 0.09992332756519318\n",
      "Step: 571, Loss: 0.09943296760320663\n",
      "Step: 572, Loss: 0.09956399351358414\n",
      "Step: 573, Loss: 0.09931079298257828\n",
      "Step: 574, Loss: 0.09973365068435669\n",
      "Step: 575, Loss: 0.0994490459561348\n",
      "Step: 576, Loss: 0.09972744435071945\n",
      "Step: 577, Loss: 0.09966512769460678\n",
      "Step: 578, Loss: 0.09981945157051086\n",
      "Step: 579, Loss: 0.0996147096157074\n",
      "Step: 580, Loss: 0.09937168657779694\n",
      "Step: 581, Loss: 0.09961981326341629\n",
      "Step: 582, Loss: 0.09938687086105347\n",
      "Step: 583, Loss: 0.09917938709259033\n",
      "Step: 584, Loss: 0.09949074685573578\n",
      "Step: 585, Loss: 0.09972810000181198\n",
      "Step: 586, Loss: 0.09952537715435028\n",
      "Step: 587, Loss: 0.09971737116575241\n",
      "Step: 588, Loss: 0.09962929040193558\n",
      "Step: 589, Loss: 0.09988339990377426\n",
      "Step: 590, Loss: 0.09978655725717545\n",
      "Step: 591, Loss: 0.09918398410081863\n",
      "Step: 592, Loss: 0.09979411959648132\n",
      "Step: 593, Loss: 0.09905647486448288\n",
      "Step: 594, Loss: 0.09952230006456375\n",
      "Step: 595, Loss: 0.09919173270463943\n",
      "Step: 596, Loss: 0.09927141666412354\n",
      "Step: 597, Loss: 0.09943622350692749\n",
      "Step: 598, Loss: 0.09921099990606308\n",
      "Step: 599, Loss: 0.09971991926431656\n",
      "Final loss:  0.09971992\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/mixturelinear_tn_gev_twcrps_mean13.0_std4.0_epochs600_folds_1_3.pkl\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
