{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 10:56:49.788151: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-14 10:56:49.813981: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-14 10:56:49.814003: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-14 10:56:49.814668: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-14 10:56:49.818631: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-14 10:56:49.819061: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 10:56:55.602627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.training.training import train_model, train_and_save, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NORMAL models\n",
    "## default epochs = 400\n",
    "\n",
    "\n",
    "forecast_distribution = 'distr_frechet'\n",
    "distribution_1 = 'distr_trunc_normal'\n",
    "distribution_2 = 'distr_gev'\n",
    "\n",
    "loss = 'loss_twCRPS_sample' # options: loss_CRPS_sample, loss_twCRPS_sample, loss_log_likelihood\n",
    "\n",
    "chain_function = 'chain_function_normal_cdf' # options: chain_function_normal_cdf, chain_function_indicator\n",
    "chain_function_mean = 16\n",
    "chain_function_std = 5\n",
    "chain_function_threshold = 15 # 12 / 15\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.01\n",
    "folds = [1,2]\n",
    "parameter_names = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "neighbourhood_size = 11\n",
    "ignore = ['229', '285', '323']\n",
    "epochs = 400\n",
    "\n",
    "samples = 200\n",
    "printing = True\n",
    "pretrained = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for Frechet distribution\n",
      "Step: 0, Loss: 0.10259361565113068\n",
      "Step: 1, Loss: 0.09997643530368805\n",
      "Step: 2, Loss: 0.0985613465309143\n",
      "Step: 3, Loss: 0.09597565233707428\n",
      "Step: 4, Loss: 0.09468824416399002\n",
      "Step: 5, Loss: 0.09359659254550934\n",
      "Step: 6, Loss: 0.08961667120456696\n",
      "Step: 7, Loss: 0.08771990239620209\n",
      "Step: 8, Loss: 0.08746044337749481\n",
      "Step: 9, Loss: 0.08675369620323181\n",
      "Step: 10, Loss: 0.08628425002098083\n",
      "Step: 11, Loss: 0.08183974027633667\n",
      "Step: 12, Loss: 0.0819113552570343\n",
      "Step: 13, Loss: 0.0792398750782013\n",
      "Step: 14, Loss: 0.08431020379066467\n",
      "Step: 15, Loss: 0.08203381299972534\n",
      "Step: 16, Loss: 0.08091863989830017\n",
      "Step: 17, Loss: 0.08201482892036438\n",
      "Step: 18, Loss: 0.08251658082008362\n",
      "Step: 19, Loss: 0.07598456740379333\n",
      "Step: 20, Loss: 0.08143541216850281\n",
      "Step: 21, Loss: 0.07771918177604675\n",
      "Step: 22, Loss: 0.0808006227016449\n",
      "Step: 23, Loss: 0.08142778277397156\n",
      "Step: 24, Loss: 0.07413390278816223\n",
      "Step: 25, Loss: 0.0770033597946167\n",
      "Step: 26, Loss: 0.07573327422142029\n",
      "Step: 27, Loss: 0.07114946842193604\n",
      "Step: 28, Loss: 0.07376706600189209\n",
      "Step: 29, Loss: 0.07399971783161163\n",
      "Step: 30, Loss: 0.07136474549770355\n",
      "Step: 31, Loss: 0.07146942615509033\n",
      "Step: 32, Loss: 0.07074345648288727\n",
      "Step: 33, Loss: 0.0697014182806015\n",
      "Step: 34, Loss: 0.07011537253856659\n",
      "Step: 35, Loss: 0.06854240596294403\n",
      "Step: 36, Loss: 0.066961869597435\n",
      "Step: 37, Loss: 0.06665405631065369\n",
      "Step: 38, Loss: 0.06608156859874725\n",
      "Step: 39, Loss: 0.06522372364997864\n",
      "Step: 40, Loss: 0.06348879635334015\n",
      "Step: 41, Loss: 0.06341221928596497\n",
      "Step: 42, Loss: 0.06217913329601288\n",
      "Step: 43, Loss: 0.06142003834247589\n",
      "Step: 44, Loss: 0.06145235896110535\n",
      "Step: 45, Loss: 0.06012566387653351\n",
      "Step: 46, Loss: 0.060283541679382324\n",
      "Step: 47, Loss: 0.05962657928466797\n",
      "Step: 48, Loss: 0.05896264314651489\n",
      "Step: 49, Loss: 0.05843684822320938\n",
      "Step: 50, Loss: 0.057991720736026764\n",
      "Step: 51, Loss: 0.057975463569164276\n",
      "Step: 52, Loss: 0.05661601573228836\n",
      "Step: 53, Loss: 0.056467168033123016\n",
      "Step: 54, Loss: 0.056331098079681396\n",
      "Step: 55, Loss: 0.05604373663663864\n",
      "Step: 56, Loss: 0.05569300055503845\n",
      "Step: 57, Loss: 0.055201150476932526\n",
      "Step: 58, Loss: 0.055182985961437225\n",
      "Step: 59, Loss: 0.05458482354879379\n",
      "Step: 60, Loss: 0.05426209419965744\n",
      "Step: 61, Loss: 0.054338425397872925\n",
      "Step: 62, Loss: 0.05415771156549454\n",
      "Step: 63, Loss: 0.05352136492729187\n",
      "Step: 64, Loss: 0.053572095930576324\n",
      "Step: 65, Loss: 0.05371149629354477\n",
      "Step: 66, Loss: 0.053027525544166565\n",
      "Step: 67, Loss: 0.05312731862068176\n",
      "Step: 68, Loss: 0.05301973968744278\n",
      "Step: 69, Loss: 0.05259787291288376\n",
      "Step: 70, Loss: 0.05270339548587799\n",
      "Step: 71, Loss: 0.052177391946315765\n",
      "Step: 72, Loss: 0.05224662274122238\n",
      "Step: 73, Loss: 0.0523034930229187\n",
      "Step: 74, Loss: 0.052114807069301605\n",
      "Step: 75, Loss: 0.05198650807142258\n",
      "Step: 76, Loss: 0.05182627588510513\n",
      "Step: 77, Loss: 0.051635779440402985\n",
      "Step: 78, Loss: 0.05161566287279129\n",
      "Step: 79, Loss: 0.051467351615428925\n",
      "Step: 80, Loss: 0.05157143622636795\n",
      "Step: 81, Loss: 0.05116177350282669\n",
      "Step: 82, Loss: 0.051128193736076355\n",
      "Step: 83, Loss: 0.050951555371284485\n",
      "Step: 84, Loss: 0.050927311182022095\n",
      "Step: 85, Loss: 0.051154255867004395\n",
      "Step: 86, Loss: 0.05118127912282944\n",
      "Step: 87, Loss: 0.0508243627846241\n",
      "Step: 88, Loss: 0.05110447481274605\n",
      "Step: 89, Loss: 0.05070876702666283\n",
      "Step: 90, Loss: 0.05058281123638153\n",
      "Step: 91, Loss: 0.05055907741189003\n",
      "Step: 92, Loss: 0.050530966371297836\n",
      "Step: 93, Loss: 0.050609178841114044\n",
      "Step: 94, Loss: 0.050626058131456375\n",
      "Step: 95, Loss: 0.050605274736881256\n",
      "Step: 96, Loss: 0.050620585680007935\n",
      "Step: 97, Loss: 0.0503990463912487\n",
      "Step: 98, Loss: 0.05047294870018959\n",
      "Step: 99, Loss: 0.05023926869034767\n",
      "Step: 100, Loss: 0.05035078898072243\n",
      "Step: 101, Loss: 0.05050859972834587\n",
      "Step: 102, Loss: 0.05029338598251343\n",
      "Step: 103, Loss: 0.050487201660871506\n",
      "Step: 104, Loss: 0.050365932285785675\n",
      "Step: 105, Loss: 0.050220903009176254\n",
      "Step: 106, Loss: 0.05042196437716484\n",
      "Step: 107, Loss: 0.05005573481321335\n",
      "Step: 108, Loss: 0.0499209463596344\n",
      "Step: 109, Loss: 0.05015312880277634\n",
      "Step: 110, Loss: 0.05028076097369194\n",
      "Step: 111, Loss: 0.04999812692403793\n",
      "Step: 112, Loss: 0.05028653144836426\n",
      "Step: 113, Loss: 0.049870193004608154\n",
      "Step: 114, Loss: 0.050006184726953506\n",
      "Step: 115, Loss: 0.05020562931895256\n",
      "Step: 116, Loss: 0.05017656087875366\n",
      "Step: 117, Loss: 0.04973704740405083\n",
      "Step: 118, Loss: 0.0498204231262207\n",
      "Step: 119, Loss: 0.05006483569741249\n",
      "Step: 120, Loss: 0.04990018159151077\n",
      "Step: 121, Loss: 0.050018198788166046\n",
      "Step: 122, Loss: 0.05012066289782524\n",
      "Step: 123, Loss: 0.05001979321241379\n",
      "Step: 124, Loss: 0.05001875385642052\n",
      "Step: 125, Loss: 0.04992726817727089\n",
      "Step: 126, Loss: 0.049571871757507324\n",
      "Step: 127, Loss: 0.04983736202120781\n",
      "Step: 128, Loss: 0.049625933170318604\n",
      "Step: 129, Loss: 0.04985472559928894\n",
      "Step: 130, Loss: 0.049701105803251266\n",
      "Step: 131, Loss: 0.04979587718844414\n",
      "Step: 132, Loss: 0.049795523285865784\n",
      "Step: 133, Loss: 0.049758654087781906\n",
      "Step: 134, Loss: 0.04988984018564224\n",
      "Step: 135, Loss: 0.04971577599644661\n",
      "Step: 136, Loss: 0.049498964101076126\n",
      "Step: 137, Loss: 0.04974815249443054\n",
      "Step: 138, Loss: 0.04961487650871277\n",
      "Step: 139, Loss: 0.04982231184840202\n",
      "Step: 140, Loss: 0.04988933727145195\n",
      "Step: 141, Loss: 0.04964261129498482\n",
      "Step: 142, Loss: 0.04974684119224548\n",
      "Step: 143, Loss: 0.04960951954126358\n",
      "Step: 144, Loss: 0.04969898611307144\n",
      "Step: 145, Loss: 0.049418531358242035\n",
      "Step: 146, Loss: 0.04969470575451851\n",
      "Step: 147, Loss: 0.049887146800756454\n",
      "Step: 148, Loss: 0.04960082843899727\n",
      "Step: 149, Loss: 0.04968085512518883\n",
      "Step: 150, Loss: 0.049566566944122314\n",
      "Step: 151, Loss: 0.04975394532084465\n",
      "Step: 152, Loss: 0.04964481666684151\n",
      "Step: 153, Loss: 0.04972405359148979\n",
      "Step: 154, Loss: 0.04957198351621628\n",
      "Step: 155, Loss: 0.04963544383645058\n",
      "Step: 156, Loss: 0.049608148634433746\n",
      "Step: 157, Loss: 0.0496632382273674\n",
      "Step: 158, Loss: 0.04958730936050415\n",
      "Step: 159, Loss: 0.049672409892082214\n",
      "Step: 160, Loss: 0.04950534179806709\n",
      "Step: 161, Loss: 0.04969156160950661\n",
      "Step: 162, Loss: 0.04950116202235222\n",
      "Step: 163, Loss: 0.04948285222053528\n",
      "Step: 164, Loss: 0.04953451454639435\n",
      "Step: 165, Loss: 0.04975740984082222\n",
      "Step: 166, Loss: 0.04957292228937149\n",
      "Step: 167, Loss: 0.049540288746356964\n",
      "Step: 168, Loss: 0.049506593495607376\n",
      "Step: 169, Loss: 0.04968971014022827\n",
      "Step: 170, Loss: 0.0494690015912056\n",
      "Step: 171, Loss: 0.04968497157096863\n",
      "Step: 172, Loss: 0.04952824488282204\n",
      "Step: 173, Loss: 0.04934743791818619\n",
      "Step: 174, Loss: 0.04941783845424652\n",
      "Step: 175, Loss: 0.04933352395892143\n",
      "Step: 176, Loss: 0.049588385969400406\n",
      "Step: 177, Loss: 0.04951215907931328\n",
      "Step: 178, Loss: 0.0495431050658226\n",
      "Step: 179, Loss: 0.04958855360746384\n",
      "Step: 180, Loss: 0.04961365833878517\n",
      "Step: 181, Loss: 0.049537543207407\n",
      "Step: 182, Loss: 0.04958532005548477\n",
      "Step: 183, Loss: 0.04951874539256096\n",
      "Step: 184, Loss: 0.04951333999633789\n",
      "Step: 185, Loss: 0.04958678036928177\n",
      "Step: 186, Loss: 0.04967278987169266\n",
      "Step: 187, Loss: 0.04950328171253204\n",
      "Step: 188, Loss: 0.04961520805954933\n",
      "Step: 189, Loss: 0.049593228846788406\n",
      "Step: 190, Loss: 0.0494731068611145\n",
      "Step: 191, Loss: 0.049326710402965546\n",
      "Step: 192, Loss: 0.04932596907019615\n",
      "Step: 193, Loss: 0.04951782524585724\n",
      "Step: 194, Loss: 0.04916383698582649\n",
      "Step: 195, Loss: 0.049453262239694595\n",
      "Step: 196, Loss: 0.049469172954559326\n",
      "Step: 197, Loss: 0.04937167838215828\n",
      "Step: 198, Loss: 0.04936597868800163\n",
      "Step: 199, Loss: 0.04938812181353569\n",
      "Step: 200, Loss: 0.049250438809394836\n",
      "Step: 201, Loss: 0.049633704125881195\n",
      "Step: 202, Loss: 0.04954320564866066\n",
      "Step: 203, Loss: 0.04949784651398659\n",
      "Step: 204, Loss: 0.04939919710159302\n",
      "Step: 205, Loss: 0.04951097443699837\n",
      "Step: 206, Loss: 0.04946422949433327\n",
      "Step: 207, Loss: 0.04947139695286751\n",
      "Step: 208, Loss: 0.04951094090938568\n",
      "Step: 209, Loss: 0.04937172681093216\n",
      "Step: 210, Loss: 0.04931255057454109\n",
      "Step: 211, Loss: 0.049303796142339706\n",
      "Step: 212, Loss: 0.04927728697657585\n",
      "Step: 213, Loss: 0.049452804028987885\n",
      "Step: 214, Loss: 0.049280520528554916\n",
      "Step: 215, Loss: 0.049260981380939484\n",
      "Step: 216, Loss: 0.04920804873108864\n",
      "Step: 217, Loss: 0.04950033128261566\n",
      "Step: 218, Loss: 0.04936791956424713\n",
      "Step: 219, Loss: 0.049231164157390594\n",
      "Step: 220, Loss: 0.049377717077732086\n",
      "Step: 221, Loss: 0.0493922233581543\n",
      "Step: 222, Loss: 0.04945780336856842\n",
      "Step: 223, Loss: 0.049406055361032486\n",
      "Step: 224, Loss: 0.04913542792201042\n",
      "Step: 225, Loss: 0.049269817769527435\n",
      "Step: 226, Loss: 0.04947170615196228\n",
      "Step: 227, Loss: 0.04942977428436279\n",
      "Step: 228, Loss: 0.049411989748477936\n",
      "Step: 229, Loss: 0.04933638498187065\n",
      "Step: 230, Loss: 0.04929117485880852\n",
      "Step: 231, Loss: 0.04938540235161781\n",
      "Step: 232, Loss: 0.04943729192018509\n",
      "Step: 233, Loss: 0.04915519058704376\n",
      "Step: 234, Loss: 0.0493876114487648\n",
      "Step: 235, Loss: 0.04944914951920509\n",
      "Step: 236, Loss: 0.04920704662799835\n",
      "Step: 237, Loss: 0.049370311200618744\n",
      "Step: 238, Loss: 0.04911097139120102\n",
      "Step: 239, Loss: 0.04925670847296715\n",
      "Step: 240, Loss: 0.049277231097221375\n",
      "Step: 241, Loss: 0.04923242703080177\n",
      "Step: 242, Loss: 0.049197807908058167\n",
      "Step: 243, Loss: 0.04953247308731079\n",
      "Step: 244, Loss: 0.04930323734879494\n",
      "Step: 245, Loss: 0.04929870739579201\n",
      "Step: 246, Loss: 0.04941212385892868\n",
      "Step: 247, Loss: 0.049391135573387146\n",
      "Step: 248, Loss: 0.04935329407453537\n",
      "Step: 249, Loss: 0.049370959401130676\n",
      "Step: 250, Loss: 0.04952102154493332\n",
      "Step: 251, Loss: 0.04955911263823509\n",
      "Step: 252, Loss: 0.049551352858543396\n",
      "Step: 253, Loss: 0.04923787713050842\n",
      "Step: 254, Loss: 0.049471788108348846\n",
      "Step: 255, Loss: 0.049316562712192535\n",
      "Step: 256, Loss: 0.04919842258095741\n",
      "Step: 257, Loss: 0.049519967287778854\n",
      "Step: 258, Loss: 0.049190979450941086\n",
      "Step: 259, Loss: 0.04954163730144501\n",
      "Step: 260, Loss: 0.049073681235313416\n",
      "Step: 261, Loss: 0.04922248423099518\n",
      "Step: 262, Loss: 0.0492783859372139\n",
      "Step: 263, Loss: 0.04944896697998047\n",
      "Step: 264, Loss: 0.049452800303697586\n",
      "Step: 265, Loss: 0.04924391582608223\n",
      "Step: 266, Loss: 0.049115363508462906\n",
      "Step: 267, Loss: 0.04954197630286217\n",
      "Step: 268, Loss: 0.04920446500182152\n",
      "Step: 269, Loss: 0.04946231469511986\n",
      "Step: 270, Loss: 0.04946990683674812\n",
      "Step: 271, Loss: 0.049472320824861526\n",
      "Step: 272, Loss: 0.04943699389696121\n",
      "Step: 273, Loss: 0.04941584914922714\n",
      "Step: 274, Loss: 0.04919649288058281\n",
      "Step: 275, Loss: 0.04919997602701187\n",
      "Step: 276, Loss: 0.04944569244980812\n",
      "Step: 277, Loss: 0.049197375774383545\n",
      "Step: 278, Loss: 0.04955406114459038\n",
      "Step: 279, Loss: 0.04940534010529518\n",
      "Step: 280, Loss: 0.049289315938949585\n",
      "Step: 281, Loss: 0.04928186908364296\n",
      "Step: 282, Loss: 0.049249742180109024\n",
      "Step: 283, Loss: 0.04908231273293495\n",
      "Step: 284, Loss: 0.04933330416679382\n",
      "Step: 285, Loss: 0.04951181635260582\n",
      "Step: 286, Loss: 0.04932103306055069\n",
      "Step: 287, Loss: 0.04939008504152298\n",
      "Step: 288, Loss: 0.04938973858952522\n",
      "Step: 289, Loss: 0.049297209829092026\n",
      "Step: 290, Loss: 0.049273453652858734\n",
      "Step: 291, Loss: 0.04914373159408569\n",
      "Step: 292, Loss: 0.04910514876246452\n",
      "Step: 293, Loss: 0.04918071627616882\n",
      "Step: 294, Loss: 0.049122389405965805\n",
      "Step: 295, Loss: 0.04925593361258507\n",
      "Step: 296, Loss: 0.04921727627515793\n",
      "Step: 297, Loss: 0.04941723495721817\n",
      "Step: 298, Loss: 0.04932141304016113\n",
      "Step: 299, Loss: 0.049150485545396805\n",
      "Step: 300, Loss: 0.04928242042660713\n",
      "Step: 301, Loss: 0.04945887625217438\n",
      "Step: 302, Loss: 0.04924129694700241\n",
      "Step: 303, Loss: 0.04934267699718475\n",
      "Step: 304, Loss: 0.04908910021185875\n",
      "Step: 305, Loss: 0.04923104867339134\n",
      "Step: 306, Loss: 0.04933089390397072\n",
      "Step: 307, Loss: 0.04947151243686676\n",
      "Step: 308, Loss: 0.04911210387945175\n",
      "Step: 309, Loss: 0.0493280366063118\n",
      "Step: 310, Loss: 0.04944894462823868\n",
      "Step: 311, Loss: 0.04922501742839813\n",
      "Step: 312, Loss: 0.049244124442338943\n",
      "Step: 313, Loss: 0.04919100180268288\n",
      "Step: 314, Loss: 0.04918406158685684\n",
      "Step: 315, Loss: 0.049112431704998016\n",
      "Step: 316, Loss: 0.04929671809077263\n",
      "Step: 317, Loss: 0.049295730888843536\n",
      "Step: 318, Loss: 0.04904050752520561\n",
      "Step: 319, Loss: 0.04932266101241112\n",
      "Step: 320, Loss: 0.04936131462454796\n",
      "Step: 321, Loss: 0.04928399249911308\n",
      "Step: 322, Loss: 0.0493110790848732\n",
      "Step: 323, Loss: 0.04922253265976906\n",
      "Step: 324, Loss: 0.04937717318534851\n",
      "Step: 325, Loss: 0.04904770106077194\n",
      "Step: 326, Loss: 0.048968978226184845\n",
      "Step: 327, Loss: 0.04925299808382988\n",
      "Step: 328, Loss: 0.049314774572849274\n",
      "Step: 329, Loss: 0.04945972561836243\n",
      "Step: 330, Loss: 0.04946216568350792\n",
      "Step: 331, Loss: 0.049222998321056366\n",
      "Step: 332, Loss: 0.04935314133763313\n",
      "Step: 333, Loss: 0.04922792688012123\n",
      "Step: 334, Loss: 0.049290936440229416\n",
      "Step: 335, Loss: 0.04909083619713783\n",
      "Step: 336, Loss: 0.04898475483059883\n",
      "Step: 337, Loss: 0.049183160066604614\n",
      "Step: 338, Loss: 0.04897807165980339\n",
      "Step: 339, Loss: 0.04951253533363342\n",
      "Step: 340, Loss: 0.04946445673704147\n",
      "Step: 341, Loss: 0.04902166500687599\n",
      "Step: 342, Loss: 0.04927786812186241\n",
      "Step: 343, Loss: 0.049323029816150665\n",
      "Step: 344, Loss: 0.04921649768948555\n",
      "Step: 345, Loss: 0.049066513776779175\n",
      "Step: 346, Loss: 0.04930175840854645\n",
      "Step: 347, Loss: 0.049004148691892624\n",
      "Step: 348, Loss: 0.04924311488866806\n",
      "Step: 349, Loss: 0.04911958798766136\n",
      "Step: 350, Loss: 0.04919683560729027\n",
      "Step: 351, Loss: 0.04918709024786949\n",
      "Step: 352, Loss: 0.0492318831384182\n",
      "Step: 353, Loss: 0.04906307905912399\n",
      "Step: 354, Loss: 0.0490778423845768\n",
      "Step: 355, Loss: 0.049138639122247696\n",
      "Step: 356, Loss: 0.04926691949367523\n",
      "Step: 357, Loss: 0.049342043697834015\n",
      "Step: 358, Loss: 0.04908076673746109\n",
      "Step: 359, Loss: 0.049197543412446976\n",
      "Step: 360, Loss: 0.04914398118853569\n",
      "Step: 361, Loss: 0.04929773882031441\n",
      "Step: 362, Loss: 0.04940403625369072\n",
      "Step: 363, Loss: 0.049218982458114624\n",
      "Step: 364, Loss: 0.04914580658078194\n",
      "Step: 365, Loss: 0.04902118816971779\n",
      "Step: 366, Loss: 0.04933590441942215\n",
      "Step: 367, Loss: 0.04922031983733177\n",
      "Step: 368, Loss: 0.04918180778622627\n",
      "Step: 369, Loss: 0.049197591841220856\n",
      "Step: 370, Loss: 0.0493478998541832\n",
      "Step: 371, Loss: 0.049285076558589935\n",
      "Step: 372, Loss: 0.049006614834070206\n",
      "Step: 373, Loss: 0.04934249818325043\n",
      "Step: 374, Loss: 0.04939136281609535\n",
      "Step: 375, Loss: 0.049324702471494675\n",
      "Step: 376, Loss: 0.04915735870599747\n",
      "Step: 377, Loss: 0.04925144836306572\n",
      "Step: 378, Loss: 0.04901648685336113\n",
      "Step: 379, Loss: 0.049282532185316086\n",
      "Step: 380, Loss: 0.04906125366687775\n",
      "Step: 381, Loss: 0.04924791678786278\n",
      "Step: 382, Loss: 0.049127787351608276\n",
      "Step: 383, Loss: 0.04929622635245323\n",
      "Step: 384, Loss: 0.049157530069351196\n",
      "Step: 385, Loss: 0.04907409846782684\n",
      "Step: 386, Loss: 0.049052003771066666\n",
      "Step: 387, Loss: 0.04914013668894768\n",
      "Step: 388, Loss: 0.049211494624614716\n",
      "Step: 389, Loss: 0.04930826649069786\n",
      "Step: 390, Loss: 0.04928566887974739\n",
      "Step: 391, Loss: 0.04912858083844185\n",
      "Step: 392, Loss: 0.0492221936583519\n",
      "Step: 393, Loss: 0.04924273118376732\n",
      "Step: 394, Loss: 0.04927338287234306\n",
      "Step: 395, Loss: 0.049165911972522736\n",
      "Step: 396, Loss: 0.04906373843550682\n",
      "Step: 397, Loss: 0.0491330660879612\n",
      "Step: 398, Loss: 0.04927373677492142\n",
      "Step: 399, Loss: 0.04927881434559822\n",
      "Final loss:  0.049278814\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/frechet/frechet_twcrps_mean16.0_std5.0.pkl\n"
     ]
    }
   ],
   "source": [
    "gev_model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    parameter_names,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
