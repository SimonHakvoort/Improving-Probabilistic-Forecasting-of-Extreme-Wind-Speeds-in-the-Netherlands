{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 09:06:48.159770: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-19 09:06:48.186008: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-19 09:06:48.186036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-19 09:06:48.186726: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-19 09:06:48.190804: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-19 09:06:48.191123: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-19 09:06:52.621022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.training.training import train_model, train_and_save, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NORMAL models\n",
    "## default epochs = 400\n",
    "\n",
    "\n",
    "forecast_distribution = 'distr_mixture_linear'\n",
    "distribution_1 = 'distr_trunc_normal'\n",
    "distribution_2 = 'distr_gev'\n",
    "\n",
    "loss = 'loss_twCRPS_sample' # options: loss_CRPS_sample, loss_twCRPS_sample, loss_log_likelihood\n",
    "\n",
    "chain_function = 'chain_function_normal_cdf' # options: chain_function_normal_cdf, chain_function_indicator\n",
    "chain_function_mean = 13\n",
    "chain_function_std = 4\n",
    "chain_function_threshold = 15 # 12 / 15\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.01\n",
    "folds = [1,2]\n",
    "parameter_names = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "neighbourhood_size = 11\n",
    "ignore = ['229', '285', '323']\n",
    "epochs = 600\n",
    "\n",
    "samples = 200\n",
    "printing = True\n",
    "pretrained = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Final loss:  0.09315294\n",
      "Final loss:  0.0932779\n",
      "Parameter weight_a set to [-0.00328053]\n",
      "Parameter weight_b set to [-0.00550719]\n",
      "Parameter a_tn set to [0.76516694]\n",
      "Parameter b_tn set to [0.9678727  0.57571775 0.55929804 0.5807468  0.5916399 ]\n",
      "Parameter c_tn set to [1.28186]\n",
      "Parameter d_tn set to [1.1735147]\n",
      "Parameter a_gev set to [1.5283477]\n",
      "Parameter b_gev set to [ 0.5559641  -0.4535207   0.04105094 -0.5118235  -0.40433723]\n",
      "Parameter c_gev set to [0.8322212]\n",
      "Parameter d_gev set to [ 0.13341351 -0.19697519 -0.37959307 -0.20656599 -0.16846871]\n",
      "Parameter e_gev set to [0.04361422]\n",
      "Parameter weight_a set to [-0.04002464]\n",
      "Parameter weight_b set to [0.01179469]\n",
      "Parameter a_tn set to [0.7025059]\n",
      "Parameter b_tn set to [0.96010286 0.5815277  0.55271727 0.5899862  0.5939879 ]\n",
      "Parameter c_tn set to [1.2146323]\n",
      "Parameter d_tn set to [1.0139679]\n",
      "Parameter a_gev set to [1.5158894]\n",
      "Parameter b_gev set to [ 0.53719604 -0.44251543  0.10760346 -0.49975896 -0.39066234]\n",
      "Parameter c_gev set to [0.858815]\n",
      "Parameter d_gev set to [ 0.14075164 -0.16055182 -0.4513594  -0.20682955 -0.13440332]\n",
      "Parameter e_gev set to [0.04048987]\n",
      "Step: 0, Loss: 0.09289199113845825\n",
      "Step: 1, Loss: 0.09275996685028076\n",
      "Step: 2, Loss: 0.09213501960039139\n",
      "Step: 3, Loss: 0.0916425958275795\n",
      "Step: 4, Loss: 0.09206146001815796\n",
      "Step: 5, Loss: 0.09152565151453018\n",
      "Step: 6, Loss: 0.09165336191654205\n",
      "Step: 7, Loss: 0.09092940390110016\n",
      "Step: 8, Loss: 0.09095531702041626\n",
      "Step: 9, Loss: 0.09039154648780823\n",
      "Step: 10, Loss: 0.09026702493429184\n",
      "Step: 11, Loss: 0.09005970507860184\n",
      "Step: 12, Loss: 0.08953516185283661\n",
      "Step: 13, Loss: 0.08960843831300735\n",
      "Step: 14, Loss: 0.08958251029253006\n",
      "Step: 15, Loss: 0.08930942416191101\n",
      "Step: 16, Loss: 0.0893203541636467\n",
      "Step: 17, Loss: 0.08920186012983322\n",
      "Step: 18, Loss: 0.0888407975435257\n",
      "Step: 19, Loss: 0.08882874995470047\n",
      "Step: 20, Loss: 0.0887162908911705\n",
      "Step: 21, Loss: 0.08878811448812485\n",
      "Step: 22, Loss: 0.08879225701093674\n",
      "Step: 23, Loss: 0.08828750997781754\n",
      "Step: 24, Loss: 0.08844458311796188\n",
      "Step: 25, Loss: 0.08819638192653656\n",
      "Step: 26, Loss: 0.08797357976436615\n",
      "Step: 27, Loss: 0.08817785978317261\n",
      "Step: 28, Loss: 0.08772624284029007\n",
      "Step: 29, Loss: 0.08788192272186279\n",
      "Step: 30, Loss: 0.08784845471382141\n",
      "Step: 31, Loss: 0.08789613097906113\n",
      "Step: 32, Loss: 0.08770119398832321\n",
      "Step: 33, Loss: 0.08775493502616882\n",
      "Step: 34, Loss: 0.08752843737602234\n",
      "Step: 35, Loss: 0.08758993446826935\n",
      "Step: 36, Loss: 0.08765868097543716\n",
      "Step: 37, Loss: 0.08736812323331833\n",
      "Step: 38, Loss: 0.08734893053770065\n",
      "Step: 39, Loss: 0.08765222877264023\n",
      "Step: 40, Loss: 0.08723047375679016\n",
      "Step: 41, Loss: 0.0873410552740097\n",
      "Step: 42, Loss: 0.08752414584159851\n",
      "Step: 43, Loss: 0.08723681420087814\n",
      "Step: 44, Loss: 0.08719735592603683\n",
      "Step: 45, Loss: 0.08683443814516068\n",
      "Step: 46, Loss: 0.08726833760738373\n",
      "Step: 47, Loss: 0.08700678497552872\n",
      "Step: 48, Loss: 0.08724721521139145\n",
      "Step: 49, Loss: 0.08703967183828354\n",
      "Step: 50, Loss: 0.08721546083688736\n",
      "Step: 51, Loss: 0.08710066229104996\n",
      "Step: 52, Loss: 0.08717916160821915\n",
      "Step: 53, Loss: 0.08702138811349869\n",
      "Step: 54, Loss: 0.0869656652212143\n",
      "Step: 55, Loss: 0.08727549761533737\n",
      "Step: 56, Loss: 0.08714614063501358\n",
      "Step: 57, Loss: 0.0868445634841919\n",
      "Step: 58, Loss: 0.08697959035634995\n",
      "Step: 59, Loss: 0.08721719682216644\n",
      "Step: 60, Loss: 0.08699356764554977\n",
      "Step: 61, Loss: 0.08678064495325089\n",
      "Step: 62, Loss: 0.0868961438536644\n",
      "Step: 63, Loss: 0.08708846569061279\n",
      "Step: 64, Loss: 0.08710844069719315\n",
      "Step: 65, Loss: 0.08684544265270233\n",
      "Step: 66, Loss: 0.08692169189453125\n",
      "Step: 67, Loss: 0.0869324579834938\n",
      "Step: 68, Loss: 0.08728624135255814\n",
      "Step: 69, Loss: 0.0870753675699234\n",
      "Step: 70, Loss: 0.08679889887571335\n",
      "Step: 71, Loss: 0.0868888571858406\n",
      "Step: 72, Loss: 0.08693996071815491\n",
      "Step: 73, Loss: 0.08701133728027344\n",
      "Step: 74, Loss: 0.08681260049343109\n",
      "Step: 75, Loss: 0.08667872846126556\n",
      "Step: 76, Loss: 0.08667336404323578\n",
      "Step: 77, Loss: 0.08686405420303345\n",
      "Step: 78, Loss: 0.08698693662881851\n",
      "Step: 79, Loss: 0.08689934015274048\n",
      "Step: 80, Loss: 0.08705747127532959\n",
      "Step: 81, Loss: 0.0868973433971405\n",
      "Step: 82, Loss: 0.08677791059017181\n",
      "Step: 83, Loss: 0.08667505532503128\n",
      "Step: 84, Loss: 0.08701885491609573\n",
      "Step: 85, Loss: 0.08653102815151215\n",
      "Step: 86, Loss: 0.08651301264762878\n",
      "Step: 87, Loss: 0.08663886785507202\n",
      "Step: 88, Loss: 0.08673610538244247\n",
      "Step: 89, Loss: 0.0866069495677948\n",
      "Step: 90, Loss: 0.08632418513298035\n",
      "Step: 91, Loss: 0.08672326058149338\n",
      "Step: 92, Loss: 0.08665958791971207\n",
      "Step: 93, Loss: 0.08664603531360626\n",
      "Step: 94, Loss: 0.08667925745248795\n",
      "Step: 95, Loss: 0.08664260804653168\n",
      "Step: 96, Loss: 0.08681683242321014\n",
      "Step: 97, Loss: 0.08664358407258987\n",
      "Step: 98, Loss: 0.08660565316677094\n",
      "Step: 99, Loss: 0.08671043068170547\n",
      "Step: 100, Loss: 0.0866001546382904\n",
      "Step: 101, Loss: 0.08679427951574326\n",
      "Step: 102, Loss: 0.08671297878026962\n",
      "Step: 103, Loss: 0.0867956355214119\n",
      "Step: 104, Loss: 0.08647496998310089\n",
      "Step: 105, Loss: 0.08611223846673965\n",
      "Step: 106, Loss: 0.08638503402471542\n",
      "Step: 107, Loss: 0.08658619225025177\n",
      "Step: 108, Loss: 0.08663714677095413\n",
      "Step: 109, Loss: 0.08687924593687057\n",
      "Step: 110, Loss: 0.08689241856336594\n",
      "Step: 111, Loss: 0.08666188269853592\n",
      "Step: 112, Loss: 0.08678917586803436\n",
      "Step: 113, Loss: 0.08650518953800201\n",
      "Step: 114, Loss: 0.08682046830654144\n",
      "Step: 115, Loss: 0.08660485595464706\n",
      "Step: 116, Loss: 0.08643074333667755\n",
      "Step: 117, Loss: 0.08647210896015167\n",
      "Step: 118, Loss: 0.08635290712118149\n",
      "Step: 119, Loss: 0.0866289734840393\n",
      "Step: 120, Loss: 0.08671703189611435\n",
      "Step: 121, Loss: 0.08664429932832718\n",
      "Step: 122, Loss: 0.08679534494876862\n",
      "Step: 123, Loss: 0.08644414693117142\n",
      "Step: 124, Loss: 0.08669363707304001\n",
      "Step: 125, Loss: 0.08657829463481903\n",
      "Step: 126, Loss: 0.08674458414316177\n",
      "Step: 127, Loss: 0.08648776262998581\n",
      "Step: 128, Loss: 0.08631382882595062\n",
      "Step: 129, Loss: 0.08647280931472778\n",
      "Step: 130, Loss: 0.08657659590244293\n",
      "Step: 131, Loss: 0.08665946125984192\n",
      "Step: 132, Loss: 0.08652541786432266\n",
      "Step: 133, Loss: 0.08660326898097992\n",
      "Step: 134, Loss: 0.08631660044193268\n",
      "Step: 135, Loss: 0.08638634532690048\n",
      "Step: 136, Loss: 0.08676981180906296\n",
      "Step: 137, Loss: 0.08657282590866089\n",
      "Step: 138, Loss: 0.08651593327522278\n",
      "Step: 139, Loss: 0.08647340536117554\n",
      "Step: 140, Loss: 0.08624405413866043\n",
      "Step: 141, Loss: 0.08640385419130325\n",
      "Step: 142, Loss: 0.08665087819099426\n",
      "Step: 143, Loss: 0.08641757816076279\n",
      "Step: 144, Loss: 0.08657880127429962\n",
      "Step: 145, Loss: 0.08667361736297607\n",
      "Step: 146, Loss: 0.08628831058740616\n",
      "Step: 147, Loss: 0.08670487254858017\n",
      "Step: 148, Loss: 0.08644404262304306\n",
      "Step: 149, Loss: 0.0864950641989708\n",
      "Step: 150, Loss: 0.08674389868974686\n",
      "Step: 151, Loss: 0.08664236962795258\n",
      "Step: 152, Loss: 0.0865289568901062\n",
      "Step: 153, Loss: 0.08647341281175613\n",
      "Step: 154, Loss: 0.08656004816293716\n",
      "Step: 155, Loss: 0.08662468194961548\n",
      "Step: 156, Loss: 0.08662004768848419\n",
      "Step: 157, Loss: 0.08623993396759033\n",
      "Step: 158, Loss: 0.0865129828453064\n",
      "Step: 159, Loss: 0.08641636371612549\n",
      "Step: 160, Loss: 0.08617151528596878\n",
      "Step: 161, Loss: 0.08695460855960846\n",
      "Step: 162, Loss: 0.08651791512966156\n",
      "Step: 163, Loss: 0.08677992969751358\n",
      "Step: 164, Loss: 0.08653432875871658\n",
      "Step: 165, Loss: 0.08677332103252411\n",
      "Step: 166, Loss: 0.0865621417760849\n",
      "Step: 167, Loss: 0.08632618933916092\n",
      "Step: 168, Loss: 0.0865471139550209\n",
      "Step: 169, Loss: 0.08640430867671967\n",
      "Step: 170, Loss: 0.08642891049385071\n",
      "Step: 171, Loss: 0.08636055141687393\n",
      "Step: 172, Loss: 0.0863763839006424\n",
      "Step: 173, Loss: 0.08627922087907791\n",
      "Step: 174, Loss: 0.08661387115716934\n",
      "Step: 175, Loss: 0.08625777065753937\n",
      "Step: 176, Loss: 0.08660803735256195\n",
      "Step: 177, Loss: 0.08663812279701233\n",
      "Step: 178, Loss: 0.08647114038467407\n",
      "Step: 179, Loss: 0.08625774085521698\n",
      "Step: 180, Loss: 0.08622276782989502\n",
      "Step: 181, Loss: 0.08629941940307617\n",
      "Step: 182, Loss: 0.08670061081647873\n",
      "Step: 183, Loss: 0.08640918880701065\n",
      "Step: 184, Loss: 0.08659808337688446\n",
      "Step: 185, Loss: 0.08621284365653992\n",
      "Step: 186, Loss: 0.08645166456699371\n",
      "Step: 187, Loss: 0.08616941422224045\n",
      "Step: 188, Loss: 0.08605871349573135\n",
      "Step: 189, Loss: 0.08620348572731018\n",
      "Step: 190, Loss: 0.08663871139287949\n",
      "Step: 191, Loss: 0.086225226521492\n",
      "Step: 192, Loss: 0.08645281195640564\n",
      "Step: 193, Loss: 0.08636976033449173\n",
      "Step: 194, Loss: 0.08635470271110535\n",
      "Step: 195, Loss: 0.08640411496162415\n",
      "Step: 196, Loss: 0.08628237247467041\n",
      "Step: 197, Loss: 0.08608260750770569\n",
      "Step: 198, Loss: 0.08632300794124603\n",
      "Step: 199, Loss: 0.08673649281263351\n",
      "Step: 200, Loss: 0.08600487560033798\n",
      "Step: 201, Loss: 0.08628920465707779\n",
      "Step: 202, Loss: 0.08644525706768036\n",
      "Step: 203, Loss: 0.08640296012163162\n",
      "Step: 204, Loss: 0.0861879512667656\n",
      "Step: 205, Loss: 0.08636018633842468\n",
      "Step: 206, Loss: 0.08656887710094452\n",
      "Step: 207, Loss: 0.08630527555942535\n",
      "Step: 208, Loss: 0.08637861907482147\n",
      "Step: 209, Loss: 0.08650099486112595\n",
      "Step: 210, Loss: 0.08628091216087341\n",
      "Step: 211, Loss: 0.08625877648591995\n",
      "Step: 212, Loss: 0.08615775406360626\n",
      "Step: 213, Loss: 0.08629848062992096\n",
      "Step: 214, Loss: 0.08639698475599289\n",
      "Step: 215, Loss: 0.08643771708011627\n",
      "Step: 216, Loss: 0.08665081858634949\n",
      "Step: 217, Loss: 0.08658622205257416\n",
      "Step: 218, Loss: 0.08647283166646957\n",
      "Step: 219, Loss: 0.08623804152011871\n",
      "Step: 220, Loss: 0.08613354712724686\n",
      "Step: 221, Loss: 0.0862254798412323\n",
      "Step: 222, Loss: 0.08639049530029297\n",
      "Step: 223, Loss: 0.0863160714507103\n",
      "Step: 224, Loss: 0.08617424219846725\n",
      "Step: 225, Loss: 0.08634704351425171\n",
      "Step: 226, Loss: 0.086403988301754\n",
      "Step: 227, Loss: 0.0862518772482872\n",
      "Step: 228, Loss: 0.08615627884864807\n",
      "Step: 229, Loss: 0.08621981739997864\n",
      "Step: 230, Loss: 0.08634112775325775\n",
      "Step: 231, Loss: 0.08612766861915588\n",
      "Step: 232, Loss: 0.08630064129829407\n",
      "Step: 233, Loss: 0.08611316233873367\n",
      "Step: 234, Loss: 0.08609239012002945\n",
      "Step: 235, Loss: 0.08604323863983154\n",
      "Step: 236, Loss: 0.08609866350889206\n",
      "Step: 237, Loss: 0.08613484352827072\n",
      "Step: 238, Loss: 0.08641758561134338\n",
      "Step: 239, Loss: 0.08632763475179672\n",
      "Step: 240, Loss: 0.08632725477218628\n",
      "Step: 241, Loss: 0.08624070882797241\n",
      "Step: 242, Loss: 0.08647766709327698\n",
      "Step: 243, Loss: 0.08646673709154129\n",
      "Step: 244, Loss: 0.08627709001302719\n",
      "Step: 245, Loss: 0.08624538779258728\n",
      "Step: 246, Loss: 0.08621851354837418\n",
      "Step: 247, Loss: 0.08603870868682861\n",
      "Step: 248, Loss: 0.08611367642879486\n",
      "Step: 249, Loss: 0.08605290204286575\n",
      "Step: 250, Loss: 0.08653262257575989\n",
      "Step: 251, Loss: 0.08599614351987839\n",
      "Step: 252, Loss: 0.08636604249477386\n",
      "Step: 253, Loss: 0.08587239682674408\n",
      "Step: 254, Loss: 0.08625631779432297\n",
      "Step: 255, Loss: 0.08625439554452896\n",
      "Step: 256, Loss: 0.08602727949619293\n",
      "Step: 257, Loss: 0.08608602732419968\n",
      "Step: 258, Loss: 0.08634258061647415\n",
      "Step: 259, Loss: 0.08630172908306122\n",
      "Step: 260, Loss: 0.08621189743280411\n",
      "Step: 261, Loss: 0.08628915250301361\n",
      "Step: 262, Loss: 0.08618375658988953\n",
      "Step: 263, Loss: 0.08623866736888885\n",
      "Step: 264, Loss: 0.08619602024555206\n",
      "Step: 265, Loss: 0.08633878082036972\n",
      "Step: 266, Loss: 0.0861923098564148\n",
      "Step: 267, Loss: 0.08595610409975052\n",
      "Step: 268, Loss: 0.08637025207281113\n",
      "Step: 269, Loss: 0.08602280914783478\n",
      "Step: 270, Loss: 0.08619234710931778\n",
      "Step: 271, Loss: 0.08596482127904892\n",
      "Step: 272, Loss: 0.08625049144029617\n",
      "Step: 273, Loss: 0.086208775639534\n",
      "Step: 274, Loss: 0.08622729778289795\n",
      "Step: 275, Loss: 0.08622854202985764\n",
      "Step: 276, Loss: 0.08606123924255371\n",
      "Step: 277, Loss: 0.08609659969806671\n",
      "Step: 278, Loss: 0.08603481948375702\n",
      "Step: 279, Loss: 0.08613599836826324\n",
      "Step: 280, Loss: 0.08620379865169525\n",
      "Step: 281, Loss: 0.08607176691293716\n",
      "Step: 282, Loss: 0.08630122989416122\n",
      "Step: 283, Loss: 0.08579745888710022\n",
      "Step: 284, Loss: 0.08599165827035904\n",
      "Step: 285, Loss: 0.08619996160268784\n",
      "Step: 286, Loss: 0.0861547589302063\n",
      "Step: 287, Loss: 0.08664268255233765\n",
      "Step: 288, Loss: 0.08662423491477966\n",
      "Step: 289, Loss: 0.08662392199039459\n",
      "Step: 290, Loss: 0.08632228523492813\n",
      "Step: 291, Loss: 0.08611450344324112\n",
      "Step: 292, Loss: 0.08634564280509949\n",
      "Step: 293, Loss: 0.08624932169914246\n",
      "Step: 294, Loss: 0.08638076484203339\n",
      "Step: 295, Loss: 0.08637993037700653\n",
      "Step: 296, Loss: 0.08623815327882767\n",
      "Step: 297, Loss: 0.08624017238616943\n",
      "Step: 298, Loss: 0.0863652303814888\n",
      "Step: 299, Loss: 0.08605237305164337\n",
      "Step: 300, Loss: 0.08651109784841537\n",
      "Step: 301, Loss: 0.08618714660406113\n",
      "Step: 302, Loss: 0.0858549028635025\n",
      "Step: 303, Loss: 0.08646563440561295\n",
      "Step: 304, Loss: 0.08582977205514908\n",
      "Step: 305, Loss: 0.08638226240873337\n",
      "Step: 306, Loss: 0.08636961877346039\n",
      "Step: 307, Loss: 0.08656075596809387\n",
      "Step: 308, Loss: 0.08620619773864746\n",
      "Step: 309, Loss: 0.08582349121570587\n",
      "Step: 310, Loss: 0.08606758713722229\n",
      "Step: 311, Loss: 0.08648649603128433\n",
      "Step: 312, Loss: 0.08626292645931244\n",
      "Step: 313, Loss: 0.08597373962402344\n",
      "Step: 314, Loss: 0.08605002611875534\n",
      "Step: 315, Loss: 0.08623669296503067\n",
      "Step: 316, Loss: 0.08605200797319412\n",
      "Step: 317, Loss: 0.08592283725738525\n",
      "Step: 318, Loss: 0.08576913923025131\n",
      "Step: 319, Loss: 0.08626142144203186\n",
      "Step: 320, Loss: 0.08621222525835037\n",
      "Step: 321, Loss: 0.0860847532749176\n",
      "Step: 322, Loss: 0.0859655886888504\n",
      "Step: 323, Loss: 0.08607083559036255\n",
      "Step: 324, Loss: 0.08633050322532654\n",
      "Step: 325, Loss: 0.08614510297775269\n",
      "Step: 326, Loss: 0.08646971732378006\n",
      "Step: 327, Loss: 0.08621323853731155\n",
      "Step: 328, Loss: 0.08588195592164993\n",
      "Step: 329, Loss: 0.08622199296951294\n",
      "Step: 330, Loss: 0.08635910600423813\n",
      "Step: 331, Loss: 0.08607140183448792\n",
      "Step: 332, Loss: 0.08634025603532791\n",
      "Step: 333, Loss: 0.0862841084599495\n",
      "Step: 334, Loss: 0.08620033413171768\n",
      "Step: 335, Loss: 0.08601095527410507\n",
      "Step: 336, Loss: 0.08643233776092529\n",
      "Step: 337, Loss: 0.0858176201581955\n",
      "Step: 338, Loss: 0.08622128516435623\n",
      "Step: 339, Loss: 0.0859065055847168\n",
      "Step: 340, Loss: 0.08597296476364136\n",
      "Step: 341, Loss: 0.08613696694374084\n",
      "Step: 342, Loss: 0.0861036479473114\n",
      "Step: 343, Loss: 0.0858515202999115\n",
      "Step: 344, Loss: 0.08610441535711288\n",
      "Step: 345, Loss: 0.08612843602895737\n",
      "Step: 346, Loss: 0.0861850455403328\n",
      "Step: 347, Loss: 0.086020328104496\n",
      "Step: 348, Loss: 0.0860242247581482\n",
      "Step: 349, Loss: 0.08613713085651398\n",
      "Step: 350, Loss: 0.08596386015415192\n",
      "Step: 351, Loss: 0.08612418174743652\n",
      "Step: 352, Loss: 0.08630376309156418\n",
      "Step: 353, Loss: 0.08623095601797104\n",
      "Step: 354, Loss: 0.08591677248477936\n",
      "Step: 355, Loss: 0.08617774397134781\n",
      "Step: 356, Loss: 0.08617249876260757\n",
      "Step: 357, Loss: 0.08652431517839432\n",
      "Step: 358, Loss: 0.08596359938383102\n",
      "Step: 359, Loss: 0.08619339019060135\n",
      "Step: 360, Loss: 0.08584224432706833\n",
      "Step: 361, Loss: 0.08603338897228241\n",
      "Step: 362, Loss: 0.08604686707258224\n",
      "Step: 363, Loss: 0.08621995151042938\n",
      "Step: 364, Loss: 0.08597744256258011\n",
      "Step: 365, Loss: 0.08617519587278366\n",
      "Step: 366, Loss: 0.0863109603524208\n",
      "Step: 367, Loss: 0.08630774915218353\n",
      "Step: 368, Loss: 0.0860036313533783\n",
      "Step: 369, Loss: 0.0860559269785881\n",
      "Step: 370, Loss: 0.08634039759635925\n",
      "Step: 371, Loss: 0.08609627932310104\n",
      "Step: 372, Loss: 0.08617764711380005\n",
      "Step: 373, Loss: 0.08607389032840729\n",
      "Step: 374, Loss: 0.08616014569997787\n",
      "Step: 375, Loss: 0.08613653481006622\n",
      "Step: 376, Loss: 0.0861986055970192\n",
      "Step: 377, Loss: 0.08580482006072998\n",
      "Step: 378, Loss: 0.08600790798664093\n",
      "Step: 379, Loss: 0.08600511401891708\n",
      "Step: 380, Loss: 0.08599194884300232\n",
      "Step: 381, Loss: 0.08617493510246277\n",
      "Step: 382, Loss: 0.08613183349370956\n",
      "Step: 383, Loss: 0.08620235323905945\n",
      "Step: 384, Loss: 0.08603447675704956\n",
      "Step: 385, Loss: 0.08621235936880112\n",
      "Step: 386, Loss: 0.08587274700403214\n",
      "Step: 387, Loss: 0.08607844263315201\n",
      "Step: 388, Loss: 0.08569078147411346\n",
      "Step: 389, Loss: 0.08592528849840164\n",
      "Step: 390, Loss: 0.08623833954334259\n",
      "Step: 391, Loss: 0.08598779886960983\n",
      "Step: 392, Loss: 0.08609072864055634\n",
      "Step: 393, Loss: 0.08587455004453659\n",
      "Step: 394, Loss: 0.08595050871372223\n",
      "Step: 395, Loss: 0.0858914777636528\n",
      "Step: 396, Loss: 0.0858495905995369\n",
      "Step: 397, Loss: 0.0860624834895134\n",
      "Step: 398, Loss: 0.08617658168077469\n",
      "Step: 399, Loss: 0.08616453409194946\n",
      "Step: 400, Loss: 0.08628839999437332\n",
      "Step: 401, Loss: 0.08620531111955643\n",
      "Step: 402, Loss: 0.08598833531141281\n",
      "Step: 403, Loss: 0.08610093593597412\n",
      "Step: 404, Loss: 0.08626073598861694\n",
      "Step: 405, Loss: 0.08603902161121368\n",
      "Step: 406, Loss: 0.08592838048934937\n",
      "Step: 407, Loss: 0.08597052097320557\n",
      "Step: 408, Loss: 0.08583435416221619\n",
      "Step: 409, Loss: 0.08605074137449265\n",
      "Step: 410, Loss: 0.08637607097625732\n",
      "Step: 411, Loss: 0.08624208718538284\n",
      "Step: 412, Loss: 0.08619079738855362\n",
      "Step: 413, Loss: 0.08613939583301544\n",
      "Step: 414, Loss: 0.08605699986219406\n",
      "Step: 415, Loss: 0.0859329029917717\n",
      "Step: 416, Loss: 0.0858779028058052\n",
      "Step: 417, Loss: 0.08602570742368698\n",
      "Step: 418, Loss: 0.08606833219528198\n",
      "Step: 419, Loss: 0.08576401323080063\n",
      "Step: 420, Loss: 0.0860767737030983\n",
      "Step: 421, Loss: 0.08614305406808853\n",
      "Step: 422, Loss: 0.08571192622184753\n",
      "Step: 423, Loss: 0.08594218641519547\n",
      "Step: 424, Loss: 0.08570682257413864\n",
      "Step: 425, Loss: 0.08605321496725082\n",
      "Step: 426, Loss: 0.08606530725955963\n",
      "Step: 427, Loss: 0.08624441921710968\n",
      "Step: 428, Loss: 0.085977703332901\n",
      "Step: 429, Loss: 0.08584916591644287\n",
      "Step: 430, Loss: 0.08593978732824326\n",
      "Step: 431, Loss: 0.08595404028892517\n",
      "Step: 432, Loss: 0.08595408499240875\n",
      "Step: 433, Loss: 0.08592325448989868\n",
      "Step: 434, Loss: 0.08634352684020996\n",
      "Step: 435, Loss: 0.08590035885572433\n",
      "Step: 436, Loss: 0.08596237748861313\n",
      "Step: 437, Loss: 0.08593424409627914\n",
      "Step: 438, Loss: 0.08584064245223999\n",
      "Step: 439, Loss: 0.08595716208219528\n",
      "Step: 440, Loss: 0.08602926880121231\n",
      "Step: 441, Loss: 0.0859801173210144\n",
      "Step: 442, Loss: 0.08565190434455872\n",
      "Step: 443, Loss: 0.08606737852096558\n",
      "Step: 444, Loss: 0.08552959561347961\n",
      "Step: 445, Loss: 0.08623574674129486\n",
      "Step: 446, Loss: 0.08660121262073517\n",
      "Step: 447, Loss: 0.08610199391841888\n",
      "Step: 448, Loss: 0.08604660630226135\n",
      "Step: 449, Loss: 0.0861118957400322\n",
      "Step: 450, Loss: 0.08610518276691437\n",
      "Step: 451, Loss: 0.08615665882825851\n",
      "Step: 452, Loss: 0.08610891550779343\n",
      "Step: 453, Loss: 0.08609820157289505\n",
      "Step: 454, Loss: 0.08576013147830963\n",
      "Step: 455, Loss: 0.08597040176391602\n",
      "Step: 456, Loss: 0.08611021190881729\n",
      "Step: 457, Loss: 0.08603993058204651\n",
      "Step: 458, Loss: 0.08591783046722412\n",
      "Step: 459, Loss: 0.08600319176912308\n",
      "Step: 460, Loss: 0.085939422249794\n",
      "Step: 461, Loss: 0.08601382374763489\n",
      "Step: 462, Loss: 0.08601180464029312\n",
      "Step: 463, Loss: 0.08624673634767532\n",
      "Step: 464, Loss: 0.08617226034402847\n",
      "Step: 465, Loss: 0.08599507063627243\n",
      "Step: 466, Loss: 0.08597680181264877\n",
      "Step: 467, Loss: 0.08607906848192215\n",
      "Step: 468, Loss: 0.0858781561255455\n",
      "Step: 469, Loss: 0.08596373349428177\n",
      "Step: 470, Loss: 0.08579419553279877\n",
      "Step: 471, Loss: 0.08621391654014587\n",
      "Step: 472, Loss: 0.08595989644527435\n",
      "Step: 473, Loss: 0.08637293428182602\n",
      "Step: 474, Loss: 0.08634968847036362\n",
      "Step: 475, Loss: 0.08597221225500107\n",
      "Step: 476, Loss: 0.08600886166095734\n",
      "Step: 477, Loss: 0.0859047919511795\n",
      "Step: 478, Loss: 0.08593778312206268\n",
      "Step: 479, Loss: 0.08615445345640182\n",
      "Step: 480, Loss: 0.08587860316038132\n",
      "Step: 481, Loss: 0.08591774851083755\n",
      "Step: 482, Loss: 0.08602426201105118\n",
      "Step: 483, Loss: 0.08629454672336578\n",
      "Step: 484, Loss: 0.08607229590415955\n",
      "Step: 485, Loss: 0.0858880952000618\n",
      "Step: 486, Loss: 0.08580412715673447\n",
      "Step: 487, Loss: 0.08594652265310287\n",
      "Step: 488, Loss: 0.08601114153862\n",
      "Step: 489, Loss: 0.08608993887901306\n",
      "Step: 490, Loss: 0.08612839132547379\n",
      "Step: 491, Loss: 0.08603347837924957\n",
      "Step: 492, Loss: 0.08596847951412201\n",
      "Step: 493, Loss: 0.08594293892383575\n",
      "Step: 494, Loss: 0.08600912988185883\n",
      "Step: 495, Loss: 0.08599705994129181\n",
      "Step: 496, Loss: 0.08606436103582382\n",
      "Step: 497, Loss: 0.08568177372217178\n",
      "Step: 498, Loss: 0.08607169985771179\n",
      "Step: 499, Loss: 0.08556650578975677\n",
      "Step: 500, Loss: 0.08559157699346542\n",
      "Step: 501, Loss: 0.08609313517808914\n",
      "Step: 502, Loss: 0.08600928634405136\n",
      "Step: 503, Loss: 0.08589467406272888\n",
      "Step: 504, Loss: 0.08628656715154648\n",
      "Step: 505, Loss: 0.08583671599626541\n",
      "Step: 506, Loss: 0.08608905971050262\n",
      "Step: 507, Loss: 0.0859026238322258\n",
      "Step: 508, Loss: 0.08575613051652908\n",
      "Step: 509, Loss: 0.08595636487007141\n",
      "Step: 510, Loss: 0.08608207106590271\n",
      "Step: 511, Loss: 0.08606670051813126\n",
      "Step: 512, Loss: 0.0862409695982933\n",
      "Step: 513, Loss: 0.08583168685436249\n",
      "Step: 514, Loss: 0.08594737946987152\n",
      "Step: 515, Loss: 0.08614332973957062\n",
      "Step: 516, Loss: 0.0857231393456459\n",
      "Step: 517, Loss: 0.08583546429872513\n",
      "Step: 518, Loss: 0.08592434227466583\n",
      "Step: 519, Loss: 0.08609543740749359\n",
      "Step: 520, Loss: 0.0860588550567627\n",
      "Step: 521, Loss: 0.08579108119010925\n",
      "Step: 522, Loss: 0.08622034639120102\n",
      "Step: 523, Loss: 0.08589762449264526\n",
      "Step: 524, Loss: 0.08577274531126022\n",
      "Step: 525, Loss: 0.08607415854930878\n",
      "Step: 526, Loss: 0.08588389307260513\n",
      "Step: 527, Loss: 0.0859321728348732\n",
      "Step: 528, Loss: 0.08615674078464508\n",
      "Step: 529, Loss: 0.0861157700419426\n",
      "Step: 530, Loss: 0.08610060811042786\n",
      "Step: 531, Loss: 0.0862434059381485\n",
      "Step: 532, Loss: 0.08599581569433212\n",
      "Step: 533, Loss: 0.0859612300992012\n",
      "Step: 534, Loss: 0.08594617247581482\n",
      "Step: 535, Loss: 0.085884690284729\n",
      "Step: 536, Loss: 0.08577882498502731\n",
      "Step: 537, Loss: 0.08598163723945618\n",
      "Step: 538, Loss: 0.0861717164516449\n",
      "Step: 539, Loss: 0.0860542356967926\n",
      "Step: 540, Loss: 0.08609920740127563\n",
      "Step: 541, Loss: 0.08592213690280914\n",
      "Step: 542, Loss: 0.08584079891443253\n",
      "Step: 543, Loss: 0.08576476573944092\n",
      "Step: 544, Loss: 0.08604445308446884\n",
      "Step: 545, Loss: 0.08605609834194183\n",
      "Step: 546, Loss: 0.0860416367650032\n",
      "Step: 547, Loss: 0.08618315309286118\n",
      "Step: 548, Loss: 0.08605223894119263\n",
      "Step: 549, Loss: 0.08595842123031616\n",
      "Step: 550, Loss: 0.085903599858284\n",
      "Step: 551, Loss: 0.08597435057163239\n",
      "Step: 552, Loss: 0.08573228120803833\n",
      "Step: 553, Loss: 0.08569002151489258\n",
      "Step: 554, Loss: 0.08591727912425995\n",
      "Step: 555, Loss: 0.08587148040533066\n",
      "Step: 556, Loss: 0.08586767315864563\n",
      "Step: 557, Loss: 0.08600601553916931\n",
      "Step: 558, Loss: 0.08598963916301727\n",
      "Step: 559, Loss: 0.08607658743858337\n",
      "Step: 560, Loss: 0.08615714311599731\n",
      "Step: 561, Loss: 0.0860472247004509\n",
      "Step: 562, Loss: 0.08597204089164734\n",
      "Step: 563, Loss: 0.08609243482351303\n",
      "Step: 564, Loss: 0.08566989749670029\n",
      "Step: 565, Loss: 0.085825115442276\n",
      "Step: 566, Loss: 0.08589569479227066\n",
      "Step: 567, Loss: 0.0861172005534172\n",
      "Step: 568, Loss: 0.08606290072202682\n",
      "Step: 569, Loss: 0.08616392314434052\n",
      "Step: 570, Loss: 0.08573393523693085\n",
      "Step: 571, Loss: 0.08607619255781174\n",
      "Step: 572, Loss: 0.08617757260799408\n",
      "Step: 573, Loss: 0.08597525954246521\n",
      "Step: 574, Loss: 0.0859566181898117\n",
      "Step: 575, Loss: 0.08588315546512604\n",
      "Step: 576, Loss: 0.08582662791013718\n",
      "Step: 577, Loss: 0.08601114153862\n",
      "Step: 578, Loss: 0.08593665063381195\n",
      "Step: 579, Loss: 0.08583918958902359\n",
      "Step: 580, Loss: 0.08584170788526535\n",
      "Step: 581, Loss: 0.08587232977151871\n",
      "Step: 582, Loss: 0.08616424351930618\n",
      "Step: 583, Loss: 0.08597110956907272\n",
      "Step: 584, Loss: 0.08579779416322708\n",
      "Step: 585, Loss: 0.08611243963241577\n",
      "Step: 586, Loss: 0.08589313924312592\n",
      "Step: 587, Loss: 0.08585252612829208\n",
      "Step: 588, Loss: 0.08564052730798721\n",
      "Step: 589, Loss: 0.08605732023715973\n",
      "Step: 590, Loss: 0.08600183576345444\n",
      "Step: 591, Loss: 0.08622319996356964\n",
      "Step: 592, Loss: 0.08579361438751221\n",
      "Step: 593, Loss: 0.08605089783668518\n",
      "Step: 594, Loss: 0.08597481995820999\n",
      "Step: 595, Loss: 0.08553225547075272\n",
      "Step: 596, Loss: 0.08600436896085739\n",
      "Step: 597, Loss: 0.08575976639986038\n",
      "Step: 598, Loss: 0.08584616333246231\n",
      "Step: 599, Loss: 0.08610618859529495\n",
      "Final loss:  0.08610619\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/mixturelinear_tn_gev_twcrps_mean13.0_std4.0_epochs600.pkl\n"
     ]
    }
   ],
   "source": [
    "gev_model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    parameter_names,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOS Model Information:\n",
      "Loss function: loss_twCRPS_sample (Samples: 200)\n",
      "Forecast distribution: distr_mixture_linear\n",
      "Distribution 1: distr_trunc_normal\n",
      "Distribution 2: distr_gev\n",
      "Mixture weight a: [0.2763836]\n",
      "Mixture weight b: [-0.09696082]\n",
      "Parameters:\n",
      "  weight_a: [0.2763836]\n",
      "  weight_b: [-0.09696082]\n",
      "  a_tn: [0.07189032]\n",
      "  b_tn: [ 1.0263587   0.07583297 -0.15023845 -0.05140674 -0.28388438]\n",
      "  c_tn: [0.4209389]\n",
      "  d_tn: [1.4092917]\n",
      "  a_gev: [0.25509107]\n",
      "  b_gev: [ 0.874036   -0.66822815 -0.14253157 -0.01954244  1.3759203 ]\n",
      "  c_gev: [1.9641547]\n",
      "  d_gev: [ 0.01067699 -0.3056696   0.04036129  0.0185829  -0.3973121 ]\n",
      "  e_gev: [-0.3299676]\n",
      "Features: wind_speed, press, kinetic, humid, geopot\n",
      "Number of features: 5\n",
      "Neighbourhood size: 11\n",
      "Chaining function: chain_function_normal_cdf (Mean: 13.0, Std: 4.0)\n",
      "Optimizer: Adam\n",
      "Learning rate: 0.009999999776482582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gev_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
