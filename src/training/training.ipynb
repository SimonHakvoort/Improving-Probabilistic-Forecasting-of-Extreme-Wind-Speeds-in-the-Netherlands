{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 13:21:23.958785: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-19 13:21:23.985665: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-19 13:21:23.985694: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-19 13:21:23.986407: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-19 13:21:23.990659: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-19 13:21:23.991124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-19 13:21:34.298117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.training.training import train_model, train_and_save, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NORMAL models\n",
    "## default epochs = 400\n",
    "\n",
    "\n",
    "forecast_distribution = 'distr_log_normal'\n",
    "distribution_1 = 'distr_trunc_normal'\n",
    "distribution_2 = 'distr_gev'\n",
    "\n",
    "loss = 'loss_twCRPS_sample' # options: loss_CRPS_sample, loss_twCRPS_sample, loss_log_likelihood\n",
    "\n",
    "chain_function = 'chain_function_normal_cdf' # options: chain_function_normal_cdf, chain_function_indicator\n",
    "chain_function_mean = 13\n",
    "chain_function_std = 4\n",
    "chain_function_threshold = 15 # 12 / 15\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.01\n",
    "folds = [1,2]\n",
    "parameter_names = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "neighbourhood_size = 11\n",
    "ignore = ['229', '285', '323']\n",
    "epochs = 600\n",
    "\n",
    "samples = 200\n",
    "printing = True\n",
    "pretrained = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for Log Normal distribution\n",
      "Step: 0, Loss: 0.19217613339424133\n",
      "Step: 1, Loss: 0.19195681810379028\n",
      "Step: 2, Loss: 0.19182904064655304\n",
      "Step: 3, Loss: 0.19161060452461243\n",
      "Step: 4, Loss: 0.19142818450927734\n",
      "Step: 5, Loss: 0.19119274616241455\n",
      "Step: 6, Loss: 0.19091686606407166\n",
      "Step: 7, Loss: 0.19076977670192719\n",
      "Step: 8, Loss: 0.19042474031448364\n",
      "Step: 9, Loss: 0.19011519849300385\n",
      "Step: 10, Loss: 0.18984711170196533\n",
      "Step: 11, Loss: 0.18938137590885162\n",
      "Step: 12, Loss: 0.18897980451583862\n",
      "Step: 13, Loss: 0.18857547640800476\n",
      "Step: 14, Loss: 0.1880519986152649\n",
      "Step: 15, Loss: 0.18754607439041138\n",
      "Step: 16, Loss: 0.18687567114830017\n",
      "Step: 17, Loss: 0.1862669289112091\n",
      "Step: 18, Loss: 0.1855546534061432\n",
      "Step: 19, Loss: 0.18472768366336823\n",
      "Step: 20, Loss: 0.18382716178894043\n",
      "Step: 21, Loss: 0.18289169669151306\n",
      "Step: 22, Loss: 0.18184621632099152\n",
      "Step: 23, Loss: 0.180601105093956\n",
      "Step: 24, Loss: 0.17923367023468018\n",
      "Step: 25, Loss: 0.17787209153175354\n",
      "Step: 26, Loss: 0.17630615830421448\n",
      "Step: 27, Loss: 0.17461508512496948\n",
      "Step: 28, Loss: 0.17275579273700714\n",
      "Step: 29, Loss: 0.1706317514181137\n",
      "Step: 30, Loss: 0.16851328313350677\n",
      "Step: 31, Loss: 0.16606533527374268\n",
      "Step: 32, Loss: 0.1635165810585022\n",
      "Step: 33, Loss: 0.16080474853515625\n",
      "Step: 34, Loss: 0.1580251008272171\n",
      "Step: 35, Loss: nan\n",
      "Step: 36, Loss: nan\n",
      "Step: 37, Loss: nan\n",
      "Step: 38, Loss: nan\n",
      "Step: 39, Loss: nan\n",
      "Step: 40, Loss: nan\n",
      "Step: 41, Loss: nan\n",
      "Step: 42, Loss: nan\n",
      "Step: 43, Loss: nan\n",
      "Step: 44, Loss: nan\n",
      "Step: 45, Loss: nan\n",
      "Step: 46, Loss: nan\n",
      "Step: 47, Loss: nan\n",
      "Step: 48, Loss: nan\n",
      "Step: 49, Loss: nan\n",
      "Step: 50, Loss: nan\n",
      "Step: 51, Loss: nan\n",
      "Step: 52, Loss: nan\n",
      "Step: 53, Loss: nan\n",
      "Step: 54, Loss: nan\n",
      "Step: 55, Loss: nan\n",
      "Step: 56, Loss: nan\n",
      "Step: 57, Loss: nan\n",
      "Step: 58, Loss: nan\n",
      "Step: 59, Loss: nan\n",
      "Step: 60, Loss: nan\n",
      "Step: 61, Loss: nan\n",
      "Step: 62, Loss: nan\n",
      "Step: 63, Loss: nan\n",
      "Step: 64, Loss: nan\n",
      "Step: 65, Loss: nan\n",
      "Step: 66, Loss: nan\n",
      "Step: 67, Loss: nan\n",
      "Step: 68, Loss: nan\n",
      "Step: 69, Loss: nan\n",
      "Step: 70, Loss: nan\n",
      "Step: 71, Loss: nan\n",
      "Step: 72, Loss: nan\n",
      "Step: 73, Loss: nan\n",
      "Step: 74, Loss: nan\n",
      "Step: 75, Loss: nan\n",
      "Step: 76, Loss: nan\n",
      "Step: 77, Loss: nan\n",
      "Step: 78, Loss: nan\n",
      "Step: 79, Loss: nan\n",
      "Step: 80, Loss: nan\n",
      "Step: 81, Loss: nan\n",
      "Step: 82, Loss: nan\n",
      "Step: 83, Loss: nan\n",
      "Step: 84, Loss: nan\n",
      "Step: 85, Loss: nan\n",
      "Step: 86, Loss: nan\n",
      "Step: 87, Loss: nan\n",
      "Step: 88, Loss: nan\n",
      "Step: 89, Loss: nan\n",
      "Step: 90, Loss: nan\n",
      "Step: 91, Loss: nan\n",
      "Step: 92, Loss: nan\n",
      "Step: 93, Loss: nan\n",
      "Step: 94, Loss: nan\n",
      "Step: 95, Loss: nan\n",
      "Step: 96, Loss: nan\n",
      "Step: 97, Loss: nan\n",
      "Step: 98, Loss: nan\n",
      "Step: 99, Loss: nan\n",
      "Step: 100, Loss: nan\n",
      "Step: 101, Loss: nan\n",
      "Step: 102, Loss: nan\n",
      "Step: 103, Loss: nan\n",
      "Step: 104, Loss: nan\n",
      "Step: 105, Loss: nan\n",
      "Step: 106, Loss: nan\n",
      "Step: 107, Loss: nan\n",
      "Step: 108, Loss: nan\n",
      "Step: 109, Loss: nan\n",
      "Step: 110, Loss: nan\n",
      "Step: 111, Loss: nan\n",
      "Step: 112, Loss: nan\n",
      "Step: 113, Loss: nan\n",
      "Step: 114, Loss: nan\n",
      "Step: 115, Loss: nan\n",
      "Step: 116, Loss: nan\n",
      "Step: 117, Loss: nan\n",
      "Step: 118, Loss: nan\n",
      "Step: 119, Loss: nan\n",
      "Step: 120, Loss: nan\n",
      "Step: 121, Loss: nan\n",
      "Step: 122, Loss: nan\n",
      "Step: 123, Loss: nan\n",
      "Step: 124, Loss: nan\n",
      "Step: 125, Loss: nan\n",
      "Step: 126, Loss: nan\n",
      "Step: 127, Loss: nan\n",
      "Step: 128, Loss: nan\n",
      "Step: 129, Loss: nan\n",
      "Step: 130, Loss: nan\n",
      "Step: 131, Loss: nan\n",
      "Step: 132, Loss: nan\n",
      "Step: 133, Loss: nan\n",
      "Step: 134, Loss: nan\n",
      "Step: 135, Loss: nan\n",
      "Step: 136, Loss: nan\n",
      "Step: 137, Loss: nan\n",
      "Step: 138, Loss: nan\n",
      "Step: 139, Loss: nan\n",
      "Step: 140, Loss: nan\n",
      "Step: 141, Loss: nan\n",
      "Step: 142, Loss: nan\n",
      "Step: 143, Loss: nan\n",
      "Step: 144, Loss: nan\n",
      "Step: 145, Loss: nan\n",
      "Step: 146, Loss: nan\n",
      "Step: 147, Loss: nan\n",
      "Step: 148, Loss: nan\n",
      "Step: 149, Loss: nan\n",
      "Step: 150, Loss: nan\n",
      "Step: 151, Loss: nan\n",
      "Step: 152, Loss: nan\n",
      "Step: 153, Loss: nan\n",
      "Step: 154, Loss: nan\n",
      "Step: 155, Loss: nan\n",
      "Step: 156, Loss: nan\n",
      "Step: 157, Loss: nan\n",
      "Step: 158, Loss: nan\n",
      "Step: 159, Loss: nan\n",
      "Step: 160, Loss: nan\n",
      "Step: 161, Loss: nan\n",
      "Step: 162, Loss: nan\n",
      "Step: 163, Loss: nan\n",
      "Step: 164, Loss: nan\n",
      "Step: 165, Loss: nan\n",
      "Step: 166, Loss: nan\n",
      "Step: 167, Loss: nan\n",
      "Step: 168, Loss: nan\n",
      "Step: 169, Loss: nan\n",
      "Step: 170, Loss: nan\n",
      "Step: 171, Loss: nan\n",
      "Step: 172, Loss: nan\n",
      "Step: 173, Loss: nan\n",
      "Step: 174, Loss: nan\n",
      "Step: 175, Loss: nan\n",
      "Step: 176, Loss: nan\n",
      "Step: 177, Loss: nan\n",
      "Step: 178, Loss: nan\n",
      "Step: 179, Loss: nan\n",
      "Step: 180, Loss: nan\n",
      "Step: 181, Loss: nan\n",
      "Step: 182, Loss: nan\n",
      "Step: 183, Loss: nan\n",
      "Step: 184, Loss: nan\n",
      "Step: 185, Loss: nan\n",
      "Step: 186, Loss: nan\n",
      "Step: 187, Loss: nan\n",
      "Step: 188, Loss: nan\n",
      "Step: 189, Loss: nan\n",
      "Step: 190, Loss: nan\n",
      "Step: 191, Loss: nan\n",
      "Step: 192, Loss: nan\n",
      "Step: 193, Loss: nan\n",
      "Step: 194, Loss: nan\n",
      "Step: 195, Loss: nan\n",
      "Step: 196, Loss: nan\n",
      "Step: 197, Loss: nan\n",
      "Step: 198, Loss: nan\n",
      "Step: 199, Loss: nan\n",
      "Step: 200, Loss: nan\n",
      "Step: 201, Loss: nan\n",
      "Step: 202, Loss: nan\n",
      "Step: 203, Loss: nan\n",
      "Step: 204, Loss: nan\n",
      "Step: 205, Loss: nan\n",
      "Step: 206, Loss: nan\n",
      "Step: 207, Loss: nan\n",
      "Step: 208, Loss: nan\n",
      "Step: 209, Loss: nan\n",
      "Step: 210, Loss: nan\n",
      "Step: 211, Loss: nan\n",
      "Step: 212, Loss: nan\n",
      "Step: 213, Loss: nan\n",
      "Step: 214, Loss: nan\n",
      "Step: 215, Loss: nan\n",
      "Step: 216, Loss: nan\n",
      "Step: 217, Loss: nan\n",
      "Step: 218, Loss: nan\n",
      "Step: 219, Loss: nan\n",
      "Step: 220, Loss: nan\n",
      "Step: 221, Loss: nan\n",
      "Step: 222, Loss: nan\n",
      "Step: 223, Loss: nan\n",
      "Step: 224, Loss: nan\n",
      "Step: 225, Loss: nan\n",
      "Step: 226, Loss: nan\n",
      "Step: 227, Loss: nan\n",
      "Step: 228, Loss: nan\n",
      "Step: 229, Loss: nan\n",
      "Step: 230, Loss: nan\n",
      "Step: 231, Loss: nan\n",
      "Step: 232, Loss: nan\n",
      "Step: 233, Loss: nan\n",
      "Step: 234, Loss: nan\n",
      "Step: 235, Loss: nan\n",
      "Step: 236, Loss: nan\n",
      "Step: 237, Loss: nan\n",
      "Step: 238, Loss: nan\n",
      "Step: 239, Loss: nan\n",
      "Step: 240, Loss: nan\n",
      "Step: 241, Loss: nan\n",
      "Step: 242, Loss: nan\n",
      "Step: 243, Loss: nan\n",
      "Step: 244, Loss: nan\n",
      "Step: 245, Loss: nan\n",
      "Step: 246, Loss: nan\n",
      "Step: 247, Loss: nan\n",
      "Step: 248, Loss: nan\n",
      "Step: 249, Loss: nan\n",
      "Step: 250, Loss: nan\n",
      "Step: 251, Loss: nan\n",
      "Step: 252, Loss: nan\n",
      "Step: 253, Loss: nan\n",
      "Step: 254, Loss: nan\n",
      "Step: 255, Loss: nan\n",
      "Step: 256, Loss: nan\n",
      "Step: 257, Loss: nan\n",
      "Step: 258, Loss: nan\n",
      "Step: 259, Loss: nan\n",
      "Step: 260, Loss: nan\n",
      "Step: 261, Loss: nan\n",
      "Step: 262, Loss: nan\n",
      "Step: 263, Loss: nan\n",
      "Step: 264, Loss: nan\n",
      "Step: 265, Loss: nan\n",
      "Step: 266, Loss: nan\n",
      "Step: 267, Loss: nan\n",
      "Step: 268, Loss: nan\n",
      "Step: 269, Loss: nan\n",
      "Step: 270, Loss: nan\n",
      "Step: 271, Loss: nan\n",
      "Step: 272, Loss: nan\n",
      "Step: 273, Loss: nan\n",
      "Step: 274, Loss: nan\n",
      "Step: 275, Loss: nan\n",
      "Step: 276, Loss: nan\n",
      "Step: 277, Loss: nan\n",
      "Step: 278, Loss: nan\n",
      "Step: 279, Loss: nan\n",
      "Step: 280, Loss: nan\n",
      "Step: 281, Loss: nan\n",
      "Step: 282, Loss: nan\n",
      "Step: 283, Loss: nan\n",
      "Step: 284, Loss: nan\n",
      "Step: 285, Loss: nan\n",
      "Step: 286, Loss: nan\n",
      "Step: 287, Loss: nan\n",
      "Step: 288, Loss: nan\n",
      "Step: 289, Loss: nan\n",
      "Step: 290, Loss: nan\n",
      "Step: 291, Loss: nan\n",
      "Step: 292, Loss: nan\n",
      "Step: 293, Loss: nan\n",
      "Step: 294, Loss: nan\n",
      "Step: 295, Loss: nan\n",
      "Step: 296, Loss: nan\n",
      "Step: 297, Loss: nan\n",
      "Step: 298, Loss: nan\n",
      "Step: 299, Loss: nan\n",
      "Step: 300, Loss: nan\n",
      "Step: 301, Loss: nan\n",
      "Step: 302, Loss: nan\n",
      "Step: 303, Loss: nan\n",
      "Step: 304, Loss: nan\n",
      "Step: 305, Loss: nan\n",
      "Step: 306, Loss: nan\n",
      "Step: 307, Loss: nan\n",
      "Step: 308, Loss: nan\n",
      "Step: 309, Loss: nan\n",
      "Step: 310, Loss: nan\n",
      "Step: 311, Loss: nan\n",
      "Step: 312, Loss: nan\n",
      "Step: 313, Loss: nan\n",
      "Step: 314, Loss: nan\n",
      "Step: 315, Loss: nan\n",
      "Step: 316, Loss: nan\n",
      "Step: 317, Loss: nan\n",
      "Step: 318, Loss: nan\n",
      "Step: 319, Loss: nan\n",
      "Step: 320, Loss: nan\n",
      "Step: 321, Loss: nan\n",
      "Step: 322, Loss: nan\n",
      "Step: 323, Loss: nan\n",
      "Step: 324, Loss: nan\n",
      "Step: 325, Loss: nan\n",
      "Step: 326, Loss: nan\n",
      "Step: 327, Loss: nan\n",
      "Step: 328, Loss: nan\n",
      "Step: 329, Loss: nan\n",
      "Step: 330, Loss: nan\n",
      "Step: 331, Loss: nan\n",
      "Step: 332, Loss: nan\n",
      "Step: 333, Loss: nan\n",
      "Step: 334, Loss: nan\n",
      "Step: 335, Loss: nan\n",
      "Step: 336, Loss: nan\n",
      "Step: 337, Loss: nan\n",
      "Step: 338, Loss: nan\n",
      "Step: 339, Loss: nan\n",
      "Step: 340, Loss: nan\n",
      "Step: 341, Loss: nan\n",
      "Step: 342, Loss: nan\n",
      "Step: 343, Loss: nan\n",
      "Step: 344, Loss: nan\n",
      "Step: 345, Loss: nan\n",
      "Step: 346, Loss: nan\n",
      "Step: 347, Loss: nan\n",
      "Step: 348, Loss: nan\n",
      "Step: 349, Loss: nan\n",
      "Step: 350, Loss: nan\n",
      "Step: 351, Loss: nan\n",
      "Step: 352, Loss: nan\n",
      "Step: 353, Loss: nan\n",
      "Step: 354, Loss: nan\n",
      "Step: 355, Loss: nan\n",
      "Step: 356, Loss: nan\n",
      "Step: 357, Loss: nan\n",
      "Step: 358, Loss: nan\n",
      "Step: 359, Loss: nan\n",
      "Step: 360, Loss: nan\n",
      "Step: 361, Loss: nan\n",
      "Step: 362, Loss: nan\n",
      "Step: 363, Loss: nan\n",
      "Step: 364, Loss: nan\n",
      "Step: 365, Loss: nan\n",
      "Step: 366, Loss: nan\n",
      "Step: 367, Loss: nan\n",
      "Step: 368, Loss: nan\n",
      "Step: 369, Loss: nan\n",
      "Step: 370, Loss: nan\n",
      "Step: 371, Loss: nan\n",
      "Step: 372, Loss: nan\n",
      "Step: 373, Loss: nan\n",
      "Step: 374, Loss: nan\n",
      "Step: 375, Loss: nan\n",
      "Step: 376, Loss: nan\n",
      "Step: 377, Loss: nan\n",
      "Step: 378, Loss: nan\n",
      "Step: 379, Loss: nan\n",
      "Step: 380, Loss: nan\n",
      "Step: 381, Loss: nan\n",
      "Step: 382, Loss: nan\n",
      "Step: 383, Loss: nan\n",
      "Step: 384, Loss: nan\n",
      "Step: 385, Loss: nan\n",
      "Step: 386, Loss: nan\n",
      "Step: 387, Loss: nan\n",
      "Step: 388, Loss: nan\n",
      "Step: 389, Loss: nan\n",
      "Step: 390, Loss: nan\n",
      "Step: 391, Loss: nan\n",
      "Step: 392, Loss: nan\n",
      "Step: 393, Loss: nan\n",
      "Step: 394, Loss: nan\n",
      "Step: 395, Loss: nan\n",
      "Step: 396, Loss: nan\n",
      "Step: 397, Loss: nan\n",
      "Step: 398, Loss: nan\n",
      "Step: 399, Loss: nan\n",
      "Step: 400, Loss: nan\n",
      "Step: 401, Loss: nan\n",
      "Step: 402, Loss: nan\n",
      "Step: 403, Loss: nan\n",
      "Step: 404, Loss: nan\n",
      "Step: 405, Loss: nan\n",
      "Step: 406, Loss: nan\n",
      "Step: 407, Loss: nan\n",
      "Step: 408, Loss: nan\n",
      "Step: 409, Loss: nan\n",
      "Step: 410, Loss: nan\n",
      "Step: 411, Loss: nan\n",
      "Step: 412, Loss: nan\n",
      "Step: 413, Loss: nan\n",
      "Step: 414, Loss: nan\n",
      "Step: 415, Loss: nan\n",
      "Step: 416, Loss: nan\n",
      "Step: 417, Loss: nan\n",
      "Step: 418, Loss: nan\n",
      "Step: 419, Loss: nan\n",
      "Step: 420, Loss: nan\n",
      "Step: 421, Loss: nan\n",
      "Step: 422, Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecast_distribution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbourhood_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchain_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_function_mean\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchain_function_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_function_std\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchain_function_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_function_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchain_function_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprinting\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprinting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdistribution_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdistribution_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesiscode/src/training/training.py:71\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(forecast_distribution, loss, optimizer, learning_rate, folds, parameter_names, neighbourhood_size, ignore, epochs, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprinting\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     67\u001b[0m     printing \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprinting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 71\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprinting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/thesiscode/src/models/emos.py:461\u001b[0m, in \u001b[0;36mEMOS.fit\u001b[0;34m(self, X, y, variance, steps, printing)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_made \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m steps\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m--> 461\u001b[0m     loss_value, grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss_and_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# # check if gradient contains nan\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# if tf.math.reduce_any(tf.math.is_nan(grads[0])):\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m#     print(\"Gradient contains NaN\")\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m#     continue\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# hist.append(loss_value)\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforecast_distribution\u001b[38;5;241m.\u001b[39mget_parameter_dict()\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "File \u001b[0;32m~/thesiscode/src/models/emos.py:436\u001b[0m, in \u001b[0;36mEMOS.compute_loss_and_gradient\u001b[0;34m(self, X, y, variance)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m    435\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(X, y, variance)\n\u001b[0;32m--> 436\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast_distribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameter_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_value, grads\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1066\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1061\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1062\u001b[0m           output_gradients))\n\u001b[1;32m   1063\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1066\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1075\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:148\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    146\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/ops/math_grad.py:1401\u001b[0m, in \u001b[0;36m_SubGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad, \u001b[38;5;241m-\u001b[39mgrad\n\u001b[1;32m   1400\u001b[0m gx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m skip_input_indices \u001b[38;5;28;01melse\u001b[39;00m grad\n\u001b[0;32m-> 1401\u001b[0m gy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m skip_input_indices \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241;43m-\u001b[39;49m\u001b[43mgrad\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ReduceGradientArgs(x, y, gx, gy)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py:6930\u001b[0m, in \u001b[0;36mneg\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   6928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   6929\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6930\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6931\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   6933\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    parameter_names,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOS Model Information:\n",
      "Loss function: loss_twCRPS_sample (Samples: 200)\n",
      "Forecast distribution: distr_mixture\n",
      "Distribution 1: distr_trunc_normal\n",
      "Distribution 2: distr_gev\n",
      "Mixture weight: [0.6714824]\n",
      "Parameters:\n",
      "  weight: [0.6714824]\n",
      "  a_tn: [0.8587427]\n",
      "  b_tn: [ 0.9033643  -0.8743947  -0.10445029 -0.27649736  0.85969   ]\n",
      "  c_tn: [1.8735183]\n",
      "  d_tn: [0.7428804]\n",
      "  a_gev: [-0.4236689]\n",
      "  b_gev: [ 0.9333376   0.5409048  -0.3650169   0.3564078   0.61552596]\n",
      "  c_gev: [1.4305779]\n",
      "  d_gev: [ 0.09890767 -0.5423227   0.16434298  0.31506124 -0.69037396]\n",
      "  e_gev: [-0.31247506]\n",
      "Features: wind_speed, press, kinetic, humid, geopot\n",
      "Number of features: 5\n",
      "Neighbourhood size: 11\n",
      "Chaining function: chain_function_normal_cdf (Mean: 13.0, Std: 4.0)\n",
      "Optimizer: Adam\n",
      "Learning rate: 0.009999999776482582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
