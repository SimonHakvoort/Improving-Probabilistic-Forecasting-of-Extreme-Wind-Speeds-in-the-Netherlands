{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 13:46:32.483407: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-26 13:46:32.509897: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-26 13:46:32.509916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-26 13:46:32.510629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-26 13:46:32.515026: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-26 13:46:32.515458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-26 13:46:35.611986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.training.training import train_model, train_and_save, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_distribution = 'distr_mixture_linear'\n",
    "distribution_1 = 'distr_trunc_normal'\n",
    "distribution_2 = 'distr_gev'\n",
    "\n",
    "loss = 'loss_twCRPS_sample' # options: loss_CRPS_sample, loss_twCRPS_sample, loss_log_likelihood\n",
    "\n",
    "chain_function = 'chain_function_normal_cdf_plus_constant' # options: chain_function_normal_cdf, chain_function_indicator, chain_function_normal_cdf_plus_constant\n",
    "chain_function_mean = 13\n",
    "chain_function_std = 2\n",
    "chain_function_threshold = 15 # 12 / 15\n",
    "chain_function_constant = 0.03\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.03\n",
    "folds = [2,3]\n",
    "parameter_names = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "neighbourhood_size = 11\n",
    "ignore = ['229', '285', '323']\n",
    "epochs = 600\n",
    "\n",
    "samples = 100\n",
    "printing = True\n",
    "pretrained = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Final loss:  0.06998815\n",
      "Final loss:  0.07047026\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using given parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 0.06919030845165253\n",
      "Step: 1, Loss: 0.06881923228502274\n",
      "Step: 2, Loss: 0.06873297691345215\n",
      "Step: 3, Loss: 0.06858780235052109\n",
      "Step: 4, Loss: 0.06803023815155029\n",
      "Step: 5, Loss: 0.0681430846452713\n",
      "Step: 6, Loss: 0.06811676919460297\n",
      "Step: 7, Loss: 0.0684497207403183\n",
      "Step: 8, Loss: 0.06840939819812775\n",
      "Step: 9, Loss: 0.06842011213302612\n",
      "Step: 10, Loss: 0.06783004105091095\n",
      "Step: 11, Loss: 0.06799700856208801\n",
      "Step: 12, Loss: 0.06799973547458649\n",
      "Step: 13, Loss: 0.06811422854661942\n",
      "Step: 14, Loss: 0.06802449375391006\n",
      "Step: 15, Loss: 0.06782737374305725\n",
      "Step: 16, Loss: 0.0676170289516449\n",
      "Step: 17, Loss: 0.06799600273370743\n",
      "Step: 18, Loss: 0.06809829920530319\n",
      "Step: 19, Loss: 0.06824146956205368\n",
      "Step: 20, Loss: 0.06830243766307831\n",
      "Step: 21, Loss: 0.0678078681230545\n",
      "Step: 22, Loss: 0.06802967190742493\n",
      "Step: 23, Loss: 0.06763077527284622\n",
      "Step: 24, Loss: 0.06780686974525452\n",
      "Step: 25, Loss: 0.06791634112596512\n",
      "Step: 26, Loss: 0.06781492382287979\n",
      "Step: 27, Loss: 0.06774839758872986\n",
      "Step: 28, Loss: 0.06775529682636261\n",
      "Step: 29, Loss: 0.06744720041751862\n",
      "Step: 30, Loss: 0.06764034181833267\n",
      "Step: 31, Loss: 0.06776980310678482\n",
      "Step: 32, Loss: 0.0678185522556305\n",
      "Step: 33, Loss: 0.06746488809585571\n",
      "Step: 34, Loss: 0.06771238148212433\n",
      "Step: 35, Loss: 0.06732086837291718\n",
      "Step: 36, Loss: 0.06758494675159454\n",
      "Step: 37, Loss: 0.06767308712005615\n",
      "Step: 38, Loss: 0.06721266359090805\n",
      "Step: 39, Loss: 0.0674026608467102\n",
      "Step: 40, Loss: 0.06745844334363937\n",
      "Step: 41, Loss: 0.06719435006380081\n",
      "Step: 42, Loss: 0.06730403751134872\n",
      "Step: 43, Loss: 0.06761176884174347\n",
      "Step: 44, Loss: 0.06725160777568817\n",
      "Step: 45, Loss: 0.06748758256435394\n",
      "Step: 46, Loss: 0.06724205613136292\n",
      "Step: 47, Loss: 0.06702633202075958\n",
      "Step: 48, Loss: 0.06738351285457611\n",
      "Step: 49, Loss: 0.0673157200217247\n",
      "Step: 50, Loss: 0.06735295057296753\n",
      "Step: 51, Loss: 0.06762981414794922\n",
      "Step: 52, Loss: 0.0675596371293068\n",
      "Step: 53, Loss: 0.06734831631183624\n",
      "Step: 54, Loss: 0.06704983115196228\n",
      "Step: 55, Loss: 0.06762019544839859\n",
      "Step: 56, Loss: 0.0671331137418747\n",
      "Step: 57, Loss: 0.06733483821153641\n",
      "Step: 58, Loss: 0.06731393933296204\n",
      "Step: 59, Loss: 0.06693816930055618\n",
      "Step: 60, Loss: 0.06755682826042175\n",
      "Step: 61, Loss: 0.06744153052568436\n",
      "Step: 62, Loss: 0.06747627258300781\n",
      "Step: 63, Loss: 0.06687155365943909\n",
      "Step: 64, Loss: 0.0673408955335617\n",
      "Step: 65, Loss: 0.06735348701477051\n",
      "Step: 66, Loss: 0.06747989356517792\n",
      "Step: 67, Loss: 0.06708060204982758\n",
      "Step: 68, Loss: 0.0670274868607521\n",
      "Step: 69, Loss: 0.0674392431974411\n",
      "Step: 70, Loss: 0.06745227426290512\n",
      "Step: 71, Loss: 0.06723456084728241\n",
      "Step: 72, Loss: 0.06727224588394165\n",
      "Step: 73, Loss: 0.06721484661102295\n",
      "Step: 74, Loss: 0.06689229607582092\n",
      "Step: 75, Loss: 0.06717167794704437\n",
      "Step: 76, Loss: 0.06702001392841339\n",
      "Step: 77, Loss: 0.06740955263376236\n",
      "Step: 78, Loss: 0.06707403808832169\n",
      "Step: 79, Loss: 0.06698761135339737\n",
      "Step: 80, Loss: 0.06710733473300934\n",
      "Step: 81, Loss: 0.06728057563304901\n",
      "Step: 82, Loss: 0.06719204038381577\n",
      "Step: 83, Loss: 0.06763093918561935\n",
      "Step: 84, Loss: 0.06720306724309921\n",
      "Step: 85, Loss: 0.0671185553073883\n",
      "Step: 86, Loss: 0.06735973060131073\n",
      "Step: 87, Loss: 0.06735711544752121\n",
      "Step: 88, Loss: 0.06748868525028229\n",
      "Step: 89, Loss: 0.0681169405579567\n",
      "Step: 90, Loss: 0.0671718642115593\n",
      "Step: 91, Loss: 0.0673433169722557\n",
      "Step: 92, Loss: 0.06761230528354645\n",
      "Step: 93, Loss: 0.06727981567382812\n",
      "Step: 94, Loss: 0.06685978174209595\n",
      "Step: 95, Loss: 0.06724408268928528\n",
      "Step: 96, Loss: 0.06749624758958817\n",
      "Step: 97, Loss: 0.0674586370587349\n",
      "Step: 98, Loss: 0.06697158515453339\n",
      "Step: 99, Loss: 0.06759803742170334\n",
      "Step: 100, Loss: 0.06715885549783707\n",
      "Step: 101, Loss: 0.0675109252333641\n",
      "Step: 102, Loss: 0.06729064136743546\n",
      "Step: 103, Loss: 0.06714536994695663\n",
      "Step: 104, Loss: 0.06764741986989975\n",
      "Step: 105, Loss: 0.06743261218070984\n",
      "Step: 106, Loss: 0.06726931780576706\n",
      "Step: 107, Loss: 0.06751221418380737\n",
      "Step: 108, Loss: 0.0675600916147232\n",
      "Step: 109, Loss: 0.0671987384557724\n",
      "Step: 110, Loss: 0.06735411286354065\n",
      "Step: 111, Loss: 0.06719402223825455\n",
      "Step: 112, Loss: 0.06723629683256149\n",
      "Step: 113, Loss: 0.06742924451828003\n",
      "Step: 114, Loss: 0.0671972706913948\n",
      "Step: 115, Loss: 0.06698218733072281\n",
      "Step: 116, Loss: 0.06715432554483414\n",
      "Step: 117, Loss: 0.06703286617994308\n",
      "Step: 118, Loss: 0.06730473041534424\n",
      "Step: 119, Loss: 0.06740239262580872\n",
      "Step: 120, Loss: 0.06699231266975403\n",
      "Step: 121, Loss: 0.06708324700593948\n",
      "Step: 122, Loss: 0.06721888482570648\n",
      "Step: 123, Loss: 0.06664633005857468\n",
      "Step: 124, Loss: 0.06732013821601868\n",
      "Step: 125, Loss: 0.06712303310632706\n",
      "Step: 126, Loss: 0.06685661524534225\n",
      "Step: 127, Loss: 0.06690437346696854\n",
      "Step: 128, Loss: 0.06722140312194824\n",
      "Step: 129, Loss: 0.06731001287698746\n",
      "Step: 130, Loss: 0.06682661175727844\n",
      "Step: 131, Loss: 0.06665100157260895\n",
      "Step: 132, Loss: 0.06741350144147873\n",
      "Step: 133, Loss: 0.06689619272947311\n",
      "Step: 134, Loss: 0.06757674366235733\n",
      "Step: 135, Loss: 0.06732700765132904\n",
      "Step: 136, Loss: 0.06717903912067413\n",
      "Step: 137, Loss: 0.06707222014665604\n",
      "Step: 138, Loss: 0.06722809374332428\n",
      "Step: 139, Loss: 0.06708569824695587\n",
      "Step: 140, Loss: 0.06719227880239487\n",
      "Step: 141, Loss: 0.06688828766345978\n",
      "Step: 142, Loss: 0.0671013742685318\n",
      "Step: 143, Loss: 0.0668555274605751\n",
      "Step: 144, Loss: 0.06719346344470978\n",
      "Step: 145, Loss: 0.06708832085132599\n",
      "Step: 146, Loss: 0.06684452295303345\n",
      "Step: 147, Loss: 0.06686504930257797\n",
      "Step: 148, Loss: 0.06676687300205231\n",
      "Step: 149, Loss: 0.06712769716978073\n",
      "Step: 150, Loss: 0.06710343807935715\n",
      "Step: 151, Loss: 0.0672651082277298\n",
      "Step: 152, Loss: 0.06716955453157425\n",
      "Step: 153, Loss: 0.06701609492301941\n",
      "Step: 154, Loss: 0.06747720390558243\n",
      "Step: 155, Loss: 0.06691209971904755\n",
      "Step: 156, Loss: 0.0670342966914177\n",
      "Step: 157, Loss: 0.06709308177232742\n",
      "Step: 158, Loss: 0.0671517476439476\n",
      "Step: 159, Loss: 0.06678739190101624\n",
      "Step: 160, Loss: 0.06732269376516342\n",
      "Step: 161, Loss: 0.06713731586933136\n",
      "Step: 162, Loss: 0.06761365383863449\n",
      "Step: 163, Loss: 0.06703456491231918\n",
      "Step: 164, Loss: 0.06751912087202072\n",
      "Step: 165, Loss: 0.06752792000770569\n",
      "Step: 166, Loss: 0.06696810573339462\n",
      "Step: 167, Loss: 0.06720495223999023\n",
      "Step: 168, Loss: 0.06693229079246521\n",
      "Step: 169, Loss: 0.06735013425350189\n",
      "Step: 170, Loss: 0.06706243008375168\n",
      "Step: 171, Loss: 0.0671197921037674\n",
      "Step: 172, Loss: 0.0673278421163559\n",
      "Step: 173, Loss: 0.06743137538433075\n",
      "Step: 174, Loss: 0.06738951057195663\n",
      "Step: 175, Loss: 0.0673648789525032\n",
      "Step: 176, Loss: 0.06716274470090866\n",
      "Step: 177, Loss: 0.06743384897708893\n",
      "Step: 178, Loss: 0.06709647923707962\n",
      "Step: 179, Loss: 0.06723666936159134\n",
      "Step: 180, Loss: 0.06717963516712189\n",
      "Step: 181, Loss: 0.0672178566455841\n",
      "Step: 182, Loss: 0.06736066937446594\n",
      "Step: 183, Loss: 0.06744396686553955\n",
      "Step: 184, Loss: 0.06724654138088226\n",
      "Step: 185, Loss: 0.06721187382936478\n",
      "Step: 186, Loss: 0.06735336780548096\n",
      "Step: 187, Loss: 0.06698291748762131\n",
      "Step: 188, Loss: 0.0668618381023407\n",
      "Step: 189, Loss: 0.06708914041519165\n",
      "Step: 190, Loss: 0.06710758805274963\n",
      "Step: 191, Loss: 0.06735336780548096\n",
      "Step: 192, Loss: 0.06699840724468231\n",
      "Step: 193, Loss: 0.06717516481876373\n",
      "Step: 194, Loss: 0.06728824973106384\n",
      "Step: 195, Loss: 0.06712442636489868\n",
      "Step: 196, Loss: 0.0668562650680542\n",
      "Step: 197, Loss: 0.06721998006105423\n",
      "Step: 198, Loss: 0.06717957556247711\n",
      "Step: 199, Loss: 0.06708274036645889\n",
      "Step: 200, Loss: 0.06689512729644775\n",
      "Step: 201, Loss: 0.06730145961046219\n",
      "Step: 202, Loss: 0.06713544577360153\n",
      "Step: 203, Loss: 0.06713487952947617\n",
      "Step: 204, Loss: 0.06700953096151352\n",
      "Step: 205, Loss: 0.0669650137424469\n",
      "Step: 206, Loss: 0.06701841950416565\n",
      "Step: 207, Loss: 0.06708522886037827\n",
      "Step: 208, Loss: 0.06736814975738525\n",
      "Step: 209, Loss: 0.06726863980293274\n",
      "Step: 210, Loss: 0.06706796586513519\n",
      "Step: 211, Loss: 0.06736545264720917\n",
      "Step: 212, Loss: 0.06728780269622803\n",
      "Step: 213, Loss: 0.06703229248523712\n",
      "Step: 214, Loss: 0.06738568097352982\n",
      "Step: 215, Loss: 0.06747802346944809\n",
      "Step: 216, Loss: 0.06731182336807251\n",
      "Step: 217, Loss: 0.06715292483568192\n",
      "Step: 218, Loss: 0.06744129955768585\n",
      "Step: 219, Loss: 0.06724192202091217\n",
      "Step: 220, Loss: 0.067354217171669\n",
      "Step: 221, Loss: 0.06750384718179703\n",
      "Step: 222, Loss: 0.06706163287162781\n",
      "Step: 223, Loss: 0.06724631786346436\n",
      "Step: 224, Loss: 0.06725168973207474\n",
      "Step: 225, Loss: 0.06723077595233917\n",
      "Step: 226, Loss: 0.06716600060462952\n",
      "Step: 227, Loss: 0.06743477284908295\n",
      "Step: 228, Loss: 0.06722478568553925\n",
      "Step: 229, Loss: 0.06689140200614929\n",
      "Step: 230, Loss: 0.06689315289258957\n",
      "Step: 231, Loss: 0.06687750667333603\n",
      "Step: 232, Loss: 0.06707274168729782\n",
      "Step: 233, Loss: 0.06716029345989227\n",
      "Step: 234, Loss: 0.06675897538661957\n",
      "Step: 235, Loss: 0.06724701076745987\n",
      "Step: 236, Loss: 0.06716086715459824\n",
      "Step: 237, Loss: 0.06666148453950882\n",
      "Step: 238, Loss: 0.0670127421617508\n",
      "Step: 239, Loss: 0.06698844581842422\n",
      "Step: 240, Loss: 0.06673703342676163\n",
      "Step: 241, Loss: 0.06736354529857635\n",
      "Step: 242, Loss: 0.06728827953338623\n",
      "Step: 243, Loss: 0.06704403460025787\n",
      "Step: 244, Loss: 0.06738440692424774\n",
      "Step: 245, Loss: 0.06716115772724152\n",
      "Step: 246, Loss: 0.06706039607524872\n",
      "Step: 247, Loss: 0.06718406826257706\n",
      "Step: 248, Loss: 0.06701163947582245\n",
      "Step: 249, Loss: 0.06726311892271042\n",
      "Step: 250, Loss: 0.06697383522987366\n",
      "Step: 251, Loss: 0.06699134409427643\n",
      "Step: 252, Loss: 0.06704936921596527\n",
      "Step: 253, Loss: 0.06700471043586731\n",
      "Step: 254, Loss: 0.0670328289270401\n",
      "Step: 255, Loss: 0.06709345430135727\n",
      "Step: 256, Loss: 0.06709416955709457\n",
      "Step: 257, Loss: 0.06709368526935577\n",
      "Step: 258, Loss: 0.06732311844825745\n",
      "Step: 259, Loss: 0.06755159050226212\n",
      "Step: 260, Loss: 0.06724302470684052\n",
      "Step: 261, Loss: 0.06652689725160599\n",
      "Step: 262, Loss: 0.06734213978052139\n",
      "Step: 263, Loss: 0.06671778857707977\n",
      "Step: 264, Loss: 0.06679369509220123\n",
      "Step: 265, Loss: 0.06697741895914078\n",
      "Step: 266, Loss: 0.06706719100475311\n",
      "Step: 267, Loss: 0.0666612759232521\n",
      "Step: 268, Loss: 0.0668429583311081\n",
      "Step: 269, Loss: 0.06693635135889053\n",
      "Step: 270, Loss: 0.06684225797653198\n",
      "Step: 271, Loss: 0.067318856716156\n",
      "Step: 272, Loss: 0.06696581840515137\n",
      "Step: 273, Loss: 0.06672786176204681\n",
      "Step: 274, Loss: 0.06696416437625885\n",
      "Step: 275, Loss: 0.06696085631847382\n",
      "Step: 276, Loss: 0.06699763238430023\n",
      "Step: 277, Loss: 0.06724731624126434\n",
      "Step: 278, Loss: 0.06711263209581375\n",
      "Step: 279, Loss: 0.06674833595752716\n",
      "Step: 280, Loss: 0.06664351373910904\n",
      "Step: 281, Loss: 0.06699468940496445\n",
      "Step: 282, Loss: 0.06682552397251129\n",
      "Step: 283, Loss: 0.06703688949346542\n",
      "Step: 284, Loss: 0.06715842336416245\n",
      "Step: 285, Loss: 0.06692475825548172\n",
      "Step: 286, Loss: 0.06675249338150024\n",
      "Step: 287, Loss: 0.06689471751451492\n",
      "Step: 288, Loss: 0.06695627421140671\n",
      "Step: 289, Loss: 0.06703004240989685\n",
      "Step: 290, Loss: 0.06701336055994034\n",
      "Step: 291, Loss: 0.06707895547151566\n",
      "Step: 292, Loss: 0.06696607172489166\n",
      "Step: 293, Loss: 0.06692970544099808\n",
      "Step: 294, Loss: 0.06675650924444199\n",
      "Step: 295, Loss: 0.06707415729761124\n",
      "Step: 296, Loss: 0.06712234020233154\n",
      "Step: 297, Loss: 0.06728728115558624\n",
      "Step: 298, Loss: 0.06720584630966187\n",
      "Step: 299, Loss: 0.06742499768733978\n",
      "Step: 300, Loss: 0.0667843148112297\n",
      "Step: 301, Loss: 0.06691838800907135\n",
      "Step: 302, Loss: 0.06696230918169022\n",
      "Step: 303, Loss: 0.06666164845228195\n",
      "Step: 304, Loss: 0.06684882193803787\n",
      "Step: 305, Loss: 0.06706354767084122\n",
      "Step: 306, Loss: 0.06681135296821594\n",
      "Step: 307, Loss: 0.0668734759092331\n",
      "Step: 308, Loss: 0.066916823387146\n",
      "Step: 309, Loss: 0.06713459640741348\n",
      "Step: 310, Loss: 0.06690800935029984\n",
      "Step: 311, Loss: 0.06690622121095657\n",
      "Step: 312, Loss: 0.06724195927381516\n",
      "Step: 313, Loss: 0.0669839009642601\n",
      "Step: 314, Loss: 0.06660830229520798\n",
      "Step: 315, Loss: 0.06693059951066971\n",
      "Step: 316, Loss: 0.0667242631316185\n",
      "Step: 317, Loss: 0.06693147122859955\n",
      "Step: 318, Loss: 0.06672648340463638\n",
      "Step: 319, Loss: 0.06690333783626556\n",
      "Step: 320, Loss: 0.06690949946641922\n",
      "Step: 321, Loss: 0.06729456037282944\n",
      "Step: 322, Loss: 0.06707998365163803\n",
      "Step: 323, Loss: 0.06727015972137451\n",
      "Step: 324, Loss: 0.06694594025611877\n",
      "Step: 325, Loss: 0.06683704257011414\n",
      "Step: 326, Loss: 0.06654150784015656\n",
      "Step: 327, Loss: 0.06727488338947296\n",
      "Step: 328, Loss: 0.0666918233036995\n",
      "Step: 329, Loss: 0.067495197057724\n",
      "Step: 330, Loss: 0.0664731040596962\n",
      "Step: 331, Loss: 0.06692742556333542\n",
      "Step: 332, Loss: 0.06684502959251404\n",
      "Step: 333, Loss: 0.06709699332714081\n",
      "Step: 334, Loss: 0.0669940859079361\n",
      "Step: 335, Loss: 0.06687430292367935\n",
      "Step: 336, Loss: 0.06666861474514008\n",
      "Step: 337, Loss: 0.06684758514165878\n",
      "Step: 338, Loss: 0.0669560357928276\n",
      "Step: 339, Loss: 0.06685873866081238\n",
      "Step: 340, Loss: 0.06656690686941147\n",
      "Step: 341, Loss: 0.06713993102312088\n",
      "Step: 342, Loss: 0.06670330464839935\n",
      "Step: 343, Loss: 0.06657138466835022\n",
      "Step: 344, Loss: 0.06680319458246231\n",
      "Step: 345, Loss: 0.06703446060419083\n",
      "Step: 346, Loss: 0.06664153188467026\n",
      "Step: 347, Loss: 0.0669885128736496\n",
      "Step: 348, Loss: 0.0670631006360054\n",
      "Step: 349, Loss: 0.06710975617170334\n",
      "Step: 350, Loss: 0.06687265634536743\n",
      "Step: 351, Loss: 0.06694337725639343\n",
      "Step: 352, Loss: 0.06665205210447311\n",
      "Step: 353, Loss: 0.06690136343240738\n",
      "Step: 354, Loss: 0.06697220355272293\n",
      "Step: 355, Loss: 0.06707495450973511\n",
      "Step: 356, Loss: 0.06689441204071045\n",
      "Step: 357, Loss: 0.06703385710716248\n",
      "Step: 358, Loss: 0.06693367660045624\n",
      "Step: 359, Loss: 0.06697941571474075\n",
      "Step: 360, Loss: 0.06697805970907211\n",
      "Step: 361, Loss: 0.06688026338815689\n",
      "Step: 362, Loss: 0.06684495508670807\n",
      "Step: 363, Loss: 0.06693927943706512\n",
      "Step: 364, Loss: 0.06723374873399734\n",
      "Step: 365, Loss: 0.06728807091712952\n",
      "Step: 366, Loss: 0.06684458255767822\n",
      "Step: 367, Loss: 0.06704718619585037\n",
      "Step: 368, Loss: 0.06703230738639832\n",
      "Step: 369, Loss: 0.06692923605442047\n",
      "Step: 370, Loss: 0.06682710349559784\n",
      "Step: 371, Loss: 0.06695733964443207\n",
      "Step: 372, Loss: 0.06683562695980072\n",
      "Step: 373, Loss: 0.06699050217866898\n",
      "Step: 374, Loss: 0.06678898632526398\n",
      "Step: 375, Loss: 0.06712643057107925\n",
      "Step: 376, Loss: 0.06677410006523132\n",
      "Step: 377, Loss: 0.06669361144304276\n",
      "Step: 378, Loss: 0.06695754081010818\n",
      "Step: 379, Loss: 0.06698516011238098\n",
      "Step: 380, Loss: 0.06728880852460861\n",
      "Step: 381, Loss: 0.06701633334159851\n",
      "Step: 382, Loss: 0.06677531450986862\n",
      "Step: 383, Loss: 0.06682772189378738\n",
      "Step: 384, Loss: 0.06697592884302139\n",
      "Step: 385, Loss: 0.0670897513628006\n",
      "Step: 386, Loss: 0.0668724924325943\n",
      "Step: 387, Loss: 0.06682580709457397\n",
      "Step: 388, Loss: 0.0668102353811264\n",
      "Step: 389, Loss: 0.0668981522321701\n",
      "Step: 390, Loss: 0.06718699634075165\n",
      "Step: 391, Loss: 0.0669519305229187\n",
      "Step: 392, Loss: 0.06690936535596848\n",
      "Step: 393, Loss: 0.0669436827301979\n",
      "Step: 394, Loss: 0.0669742003083229\n",
      "Step: 395, Loss: 0.06717651337385178\n",
      "Step: 396, Loss: 0.06703519076108932\n",
      "Step: 397, Loss: 0.0670180544257164\n",
      "Step: 398, Loss: 0.06706260144710541\n",
      "Step: 399, Loss: 0.06690898537635803\n",
      "Step: 400, Loss: 0.0667930468916893\n",
      "Step: 401, Loss: 0.06679300963878632\n",
      "Step: 402, Loss: 0.06687130779027939\n",
      "Step: 403, Loss: 0.06693562865257263\n",
      "Step: 404, Loss: 0.06692973524332047\n",
      "Step: 405, Loss: 0.06707942485809326\n",
      "Step: 406, Loss: 0.06713151186704636\n",
      "Step: 407, Loss: 0.06713961064815521\n",
      "Step: 408, Loss: 0.06720612943172455\n",
      "Step: 409, Loss: 0.06664110720157623\n",
      "Step: 410, Loss: 0.0668218582868576\n",
      "Step: 411, Loss: 0.06704077124595642\n",
      "Step: 412, Loss: 0.06683123856782913\n",
      "Step: 413, Loss: 0.06728040426969528\n",
      "Step: 414, Loss: 0.06690521538257599\n",
      "Step: 415, Loss: 0.0668833926320076\n",
      "Step: 416, Loss: 0.06721735745668411\n",
      "Step: 417, Loss: 0.06711141765117645\n",
      "Step: 418, Loss: 0.06691068410873413\n",
      "Step: 419, Loss: 0.06703105568885803\n",
      "Step: 420, Loss: 0.06691089272499084\n",
      "Step: 421, Loss: 0.06694398075342178\n",
      "Step: 422, Loss: 0.0668463408946991\n",
      "Step: 423, Loss: 0.06683310866355896\n",
      "Step: 424, Loss: 0.0666835755109787\n",
      "Step: 425, Loss: 0.06715761870145798\n",
      "Step: 426, Loss: 0.06711749732494354\n",
      "Step: 427, Loss: 0.06692390143871307\n",
      "Step: 428, Loss: 0.06696083396673203\n",
      "Step: 429, Loss: 0.06709391623735428\n",
      "Step: 430, Loss: 0.06690618395805359\n",
      "Step: 431, Loss: 0.06688512116670609\n",
      "Step: 432, Loss: 0.06715080142021179\n",
      "Step: 433, Loss: 0.0666462853550911\n",
      "Step: 434, Loss: 0.06690426915884018\n",
      "Step: 435, Loss: 0.06703439354896545\n",
      "Step: 436, Loss: 0.06688198447227478\n",
      "Step: 437, Loss: 0.0669851303100586\n",
      "Step: 438, Loss: 0.06701567023992538\n",
      "Step: 439, Loss: 0.06709553301334381\n",
      "Step: 440, Loss: 0.06728467345237732\n",
      "Step: 441, Loss: 0.06724876165390015\n",
      "Step: 442, Loss: 0.0669715628027916\n",
      "Step: 443, Loss: 0.06733499467372894\n",
      "Step: 444, Loss: 0.0670003667473793\n",
      "Step: 445, Loss: 0.06731826812028885\n",
      "Step: 446, Loss: 0.06681142747402191\n",
      "Step: 447, Loss: 0.06709668040275574\n",
      "Step: 448, Loss: 0.06720694154500961\n",
      "Step: 449, Loss: 0.06694164127111435\n",
      "Step: 450, Loss: 0.06718723475933075\n",
      "Step: 451, Loss: 0.06675182282924652\n",
      "Step: 452, Loss: 0.0667547658085823\n",
      "Step: 453, Loss: 0.06690686196088791\n",
      "Step: 454, Loss: 0.06695641577243805\n",
      "Step: 455, Loss: 0.06701674312353134\n",
      "Step: 456, Loss: 0.06703577190637589\n",
      "Step: 457, Loss: 0.06727283447980881\n",
      "Step: 458, Loss: 0.06675056368112564\n",
      "Step: 459, Loss: 0.06678453087806702\n",
      "Step: 460, Loss: 0.06650194525718689\n",
      "Step: 461, Loss: 0.06685066968202591\n",
      "Step: 462, Loss: 0.06681112945079803\n",
      "Step: 463, Loss: 0.06713774800300598\n",
      "Step: 464, Loss: 0.06686470657587051\n",
      "Step: 465, Loss: 0.06677914410829544\n",
      "Step: 466, Loss: 0.06708301603794098\n",
      "Step: 467, Loss: 0.0669436827301979\n",
      "Step: 468, Loss: 0.0670938640832901\n",
      "Step: 469, Loss: 0.06697192788124084\n",
      "Step: 470, Loss: 0.06693686544895172\n",
      "Step: 471, Loss: 0.06660199910402298\n",
      "Step: 472, Loss: 0.06660397350788116\n",
      "Step: 473, Loss: 0.06687028706073761\n",
      "Step: 474, Loss: 0.06684640049934387\n",
      "Step: 475, Loss: 0.06673524528741837\n",
      "Step: 476, Loss: 0.06692124903202057\n",
      "Step: 477, Loss: 0.0668385848402977\n",
      "Step: 478, Loss: 0.06706260144710541\n",
      "Step: 479, Loss: 0.06699120998382568\n",
      "Step: 480, Loss: 0.06704158335924149\n",
      "Step: 481, Loss: 0.06689875572919846\n",
      "Step: 482, Loss: 0.06661174446344376\n",
      "Step: 483, Loss: 0.06693608313798904\n",
      "Step: 484, Loss: 0.06701058894395828\n",
      "Step: 485, Loss: 0.0669160932302475\n",
      "Step: 486, Loss: 0.06698627024888992\n",
      "Step: 487, Loss: 0.06708893179893494\n",
      "Step: 488, Loss: 0.06673172861337662\n",
      "Step: 489, Loss: 0.06698916852474213\n",
      "Step: 490, Loss: 0.06682667136192322\n",
      "Step: 491, Loss: 0.06716732680797577\n",
      "Step: 492, Loss: 0.06699825823307037\n",
      "Step: 493, Loss: 0.06710778176784515\n",
      "Step: 494, Loss: 0.0666348934173584\n",
      "Step: 495, Loss: 0.06728941947221756\n",
      "Step: 496, Loss: 0.0672396868467331\n",
      "Step: 497, Loss: 0.06676219403743744\n",
      "Step: 498, Loss: 0.06696715205907822\n",
      "Step: 499, Loss: 0.06696345657110214\n",
      "Step: 500, Loss: 0.06687456369400024\n",
      "Step: 501, Loss: 0.06689457595348358\n",
      "Step: 502, Loss: 0.06664062291383743\n",
      "Step: 503, Loss: 0.0668567344546318\n",
      "Step: 504, Loss: 0.06652235984802246\n",
      "Step: 505, Loss: 0.06679541617631912\n",
      "Step: 506, Loss: 0.06675281375646591\n",
      "Step: 507, Loss: 0.0667215958237648\n",
      "Step: 508, Loss: 0.06693053990602493\n",
      "Step: 509, Loss: 0.06703775376081467\n",
      "Step: 510, Loss: 0.0671861544251442\n",
      "Step: 511, Loss: 0.06667400151491165\n",
      "Step: 512, Loss: 0.06669966131448746\n",
      "Step: 513, Loss: 0.06674985587596893\n",
      "Step: 514, Loss: 0.06680405139923096\n",
      "Step: 515, Loss: 0.06682009249925613\n",
      "Step: 516, Loss: 0.06662589311599731\n",
      "Step: 517, Loss: 0.06696527451276779\n",
      "Step: 518, Loss: 0.06713731586933136\n",
      "Step: 519, Loss: 0.0668548047542572\n",
      "Step: 520, Loss: 0.0670652836561203\n",
      "Step: 521, Loss: 0.0672161728143692\n",
      "Step: 522, Loss: 0.06706049293279648\n",
      "Step: 523, Loss: 0.067052461206913\n",
      "Step: 524, Loss: 0.06699910014867783\n",
      "Step: 525, Loss: 0.06697910279035568\n",
      "Step: 526, Loss: 0.06695040315389633\n",
      "Step: 527, Loss: 0.06713438779115677\n",
      "Step: 528, Loss: 0.06662069261074066\n",
      "Step: 529, Loss: 0.06665144115686417\n",
      "Step: 530, Loss: 0.06654572486877441\n",
      "Step: 531, Loss: 0.06690319627523422\n",
      "Step: 532, Loss: 0.06659391522407532\n",
      "Step: 533, Loss: 0.0666738897562027\n",
      "Step: 534, Loss: 0.06731666624546051\n",
      "Step: 535, Loss: 0.06715547293424606\n",
      "Step: 536, Loss: 0.06736721843481064\n",
      "Step: 537, Loss: 0.06816056370735168\n",
      "Step: 538, Loss: 0.06858763843774796\n",
      "Step: 539, Loss: 0.06860259175300598\n",
      "Step: 540, Loss: 0.06852110475301743\n",
      "Step: 541, Loss: 0.06877991557121277\n",
      "Step: 542, Loss: 0.06922594457864761\n",
      "Step: 543, Loss: 0.06910373270511627\n",
      "Step: 544, Loss: 0.06908123940229416\n",
      "Step: 545, Loss: 0.06886795163154602\n",
      "Step: 546, Loss: 0.06946757435798645\n",
      "Step: 547, Loss: 0.06918710470199585\n",
      "Step: 548, Loss: 0.06905064731836319\n",
      "Step: 549, Loss: 0.06916666775941849\n",
      "Step: 550, Loss: 0.06881964951753616\n",
      "Step: 551, Loss: 0.06908928602933884\n",
      "Step: 552, Loss: 0.06868937611579895\n",
      "Step: 553, Loss: 0.06901606917381287\n",
      "Step: 554, Loss: 0.06887701898813248\n",
      "Step: 555, Loss: 0.06889841705560684\n",
      "Step: 556, Loss: 0.0687551200389862\n",
      "Step: 557, Loss: 0.0692102462053299\n",
      "Step: 558, Loss: 0.06879222393035889\n",
      "Step: 559, Loss: 0.0689840316772461\n",
      "Step: 560, Loss: 0.06868357956409454\n",
      "Step: 561, Loss: 0.06887657195329666\n",
      "Step: 562, Loss: 0.06911786645650864\n",
      "Step: 563, Loss: 0.06897472590208054\n",
      "Step: 564, Loss: 0.06899960339069366\n",
      "Step: 565, Loss: 0.06891698390245438\n",
      "Step: 566, Loss: 0.06901591271162033\n",
      "Step: 567, Loss: 0.0688055083155632\n",
      "Step: 568, Loss: 0.06888460367918015\n",
      "Step: 569, Loss: 0.06877365708351135\n",
      "Step: 570, Loss: 0.06875938922166824\n",
      "Step: 571, Loss: 0.06879104673862457\n",
      "Step: 572, Loss: 0.06877996027469635\n",
      "Step: 573, Loss: 0.06896145641803741\n",
      "Step: 574, Loss: 0.06905854493379593\n",
      "Step: 575, Loss: 0.06878381222486496\n",
      "Step: 576, Loss: 0.06908225268125534\n",
      "Step: 577, Loss: 0.0692381039261818\n",
      "Step: 578, Loss: 0.0689261183142662\n",
      "Step: 579, Loss: 0.06864166259765625\n",
      "Step: 580, Loss: 0.06900102645158768\n",
      "Step: 581, Loss: 0.0687774047255516\n",
      "Step: 582, Loss: 0.06880094110965729\n",
      "Step: 583, Loss: 0.06907761842012405\n",
      "Step: 584, Loss: 0.06888850033283234\n",
      "Step: 585, Loss: 0.06867861747741699\n",
      "Step: 586, Loss: 0.0688641294836998\n",
      "Step: 587, Loss: 0.06890513002872467\n",
      "Step: 588, Loss: 0.0689188539981842\n",
      "Step: 589, Loss: 0.06867486983537674\n",
      "Step: 590, Loss: 0.06870746612548828\n",
      "Step: 591, Loss: 0.06874862313270569\n",
      "Step: 592, Loss: 0.0688953548669815\n",
      "Step: 593, Loss: 0.06904808431863785\n",
      "Step: 594, Loss: 0.0685286596417427\n",
      "Step: 595, Loss: 0.06890920549631119\n",
      "Step: 596, Loss: 0.06890370696783066\n",
      "Step: 597, Loss: 0.06867614388465881\n",
      "Step: 598, Loss: 0.06857125461101532\n",
      "Step: 599, Loss: 0.06859362125396729\n",
      "Final loss:  0.06859362\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/mixturelinear_tn_gev_twcrps_mean13.0_std2.0_constant0.029999999329447746_epochs600_folds_2_3.pkl\n"
     ]
    }
   ],
   "source": [
    "model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    parameter_names,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_constant = chain_function_constant,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Final loss:  0.10335002\n",
      "Final loss:  0.101744905\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using given parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 0.10101192444562912\n",
      "Step: 1, Loss: 0.1045975387096405\n",
      "Step: 2, Loss: 0.10153059661388397\n",
      "Step: 3, Loss: 0.10202322900295258\n",
      "Step: 4, Loss: 0.1033298522233963\n",
      "Step: 5, Loss: 0.10229266434907913\n",
      "Step: 6, Loss: 0.1007765606045723\n",
      "Step: 7, Loss: 0.10148461908102036\n",
      "Step: 8, Loss: 0.10205074399709702\n",
      "Step: 9, Loss: 0.10181190073490143\n",
      "Step: 10, Loss: 0.10107484459877014\n",
      "Step: 11, Loss: 0.10078710317611694\n",
      "Step: 12, Loss: 0.1010366752743721\n",
      "Step: 13, Loss: 0.10091517120599747\n",
      "Step: 14, Loss: 0.1011858657002449\n",
      "Step: 15, Loss: 0.10120128095149994\n",
      "Step: 16, Loss: 0.10065849870443344\n",
      "Step: 17, Loss: 0.10093311220407486\n",
      "Step: 18, Loss: 0.10144681483507156\n",
      "Step: 19, Loss: 0.10081422328948975\n",
      "Step: 20, Loss: 0.100248321890831\n",
      "Step: 21, Loss: 0.10068947076797485\n",
      "Step: 22, Loss: 0.10042747110128403\n",
      "Step: 23, Loss: 0.10088712722063065\n",
      "Step: 24, Loss: 0.1005655974149704\n",
      "Step: 25, Loss: 0.10123177617788315\n",
      "Step: 26, Loss: 0.10017429292201996\n",
      "Step: 27, Loss: 0.10028838366270065\n",
      "Step: 28, Loss: 0.10079685598611832\n",
      "Step: 29, Loss: 0.1003103256225586\n",
      "Step: 30, Loss: 0.10113375633955002\n",
      "Step: 31, Loss: 0.10008753091096878\n",
      "Step: 32, Loss: 0.10060267895460129\n",
      "Step: 33, Loss: 0.09993112087249756\n",
      "Step: 34, Loss: 0.10006285458803177\n",
      "Step: 35, Loss: 0.10051048547029495\n",
      "Step: 36, Loss: 0.10016465932130814\n",
      "Step: 37, Loss: 0.1008554995059967\n",
      "Step: 38, Loss: 0.10040871053934097\n",
      "Step: 39, Loss: 0.10027137398719788\n",
      "Step: 40, Loss: 0.10040444135665894\n",
      "Step: 41, Loss: 0.10068489611148834\n",
      "Step: 42, Loss: 0.10042519867420197\n",
      "Step: 43, Loss: 0.1005309671163559\n",
      "Step: 44, Loss: 0.100456103682518\n",
      "Step: 45, Loss: 0.10045760124921799\n",
      "Step: 46, Loss: 0.1009252518415451\n",
      "Step: 47, Loss: 0.10039106011390686\n",
      "Step: 48, Loss: 0.10039254277944565\n",
      "Step: 49, Loss: 0.10032002627849579\n",
      "Step: 50, Loss: 0.10016867518424988\n",
      "Step: 51, Loss: 0.10017027705907822\n",
      "Step: 52, Loss: 0.10000988841056824\n",
      "Step: 53, Loss: 0.10024469345808029\n",
      "Step: 54, Loss: 0.09989137947559357\n",
      "Step: 55, Loss: 0.10024677962064743\n",
      "Step: 56, Loss: 0.0998087152838707\n",
      "Step: 57, Loss: 0.10010010749101639\n",
      "Step: 58, Loss: 0.10027453303337097\n",
      "Step: 59, Loss: 0.10038748383522034\n",
      "Step: 60, Loss: 0.09995193034410477\n",
      "Step: 61, Loss: 0.10020797699689865\n",
      "Step: 62, Loss: 0.10054070502519608\n",
      "Step: 63, Loss: 0.09990429133176804\n",
      "Step: 64, Loss: 0.10013564676046371\n",
      "Step: 65, Loss: 0.10048423707485199\n",
      "Step: 66, Loss: 0.10039737820625305\n",
      "Step: 67, Loss: 0.10024860501289368\n",
      "Step: 68, Loss: 0.10020972788333893\n",
      "Step: 69, Loss: 0.1000327616930008\n",
      "Step: 70, Loss: 0.10040455311536789\n",
      "Step: 71, Loss: 0.10010264068841934\n",
      "Step: 72, Loss: 0.100120410323143\n",
      "Step: 73, Loss: 0.10007312148809433\n",
      "Step: 74, Loss: 0.09965398907661438\n",
      "Step: 75, Loss: 0.10011614859104156\n",
      "Step: 76, Loss: 0.1004413589835167\n",
      "Step: 77, Loss: 0.10010948032140732\n",
      "Step: 78, Loss: 0.10064411908388138\n",
      "Step: 79, Loss: 0.10035866498947144\n",
      "Step: 80, Loss: 0.10028905421495438\n",
      "Step: 81, Loss: 0.10032825917005539\n",
      "Step: 82, Loss: 0.09956841170787811\n",
      "Step: 83, Loss: 0.1001385971903801\n",
      "Step: 84, Loss: 0.10013581067323685\n",
      "Step: 85, Loss: 0.10000420361757278\n",
      "Step: 86, Loss: 0.09996264427900314\n",
      "Step: 87, Loss: 0.10030550509691238\n",
      "Step: 88, Loss: 0.09974067658185959\n",
      "Step: 89, Loss: 0.10065500438213348\n",
      "Step: 90, Loss: 0.10021546483039856\n",
      "Step: 91, Loss: 0.10012366622686386\n",
      "Step: 92, Loss: 0.1002422347664833\n",
      "Step: 93, Loss: 0.09985672682523727\n",
      "Step: 94, Loss: 0.10030867159366608\n",
      "Step: 95, Loss: 0.09985712915658951\n",
      "Step: 96, Loss: 0.10033926367759705\n",
      "Step: 97, Loss: 0.10037868469953537\n",
      "Step: 98, Loss: 0.099961057305336\n",
      "Step: 99, Loss: 0.10020636022090912\n",
      "Step: 100, Loss: 0.09966293722391129\n",
      "Step: 101, Loss: 0.10012321919202805\n",
      "Step: 102, Loss: 0.10028219223022461\n",
      "Step: 103, Loss: 0.10012897104024887\n",
      "Step: 104, Loss: 0.10037192702293396\n",
      "Step: 105, Loss: 0.10018285363912582\n",
      "Step: 106, Loss: 0.10033261775970459\n",
      "Step: 107, Loss: 0.10020512342453003\n",
      "Step: 108, Loss: 0.09996239095926285\n",
      "Step: 109, Loss: 0.09985598176717758\n",
      "Step: 110, Loss: 0.10007429122924805\n",
      "Step: 111, Loss: 0.10003470629453659\n",
      "Step: 112, Loss: 0.10045289248228073\n",
      "Step: 113, Loss: 0.1001242995262146\n",
      "Step: 114, Loss: 0.1000967025756836\n",
      "Step: 115, Loss: 0.10052905231714249\n",
      "Step: 116, Loss: 0.10022064298391342\n",
      "Step: 117, Loss: 0.10005631297826767\n",
      "Step: 118, Loss: 0.1000601276755333\n",
      "Step: 119, Loss: 0.09992896765470505\n",
      "Step: 120, Loss: 0.10031336545944214\n",
      "Step: 121, Loss: 0.10004186630249023\n",
      "Step: 122, Loss: 0.09985711425542831\n",
      "Step: 123, Loss: 0.09995575249195099\n",
      "Step: 124, Loss: 0.0998339056968689\n",
      "Step: 125, Loss: 0.10030561685562134\n",
      "Step: 126, Loss: 0.10000582784414291\n",
      "Step: 127, Loss: 0.1001092940568924\n",
      "Step: 128, Loss: 0.09972014278173447\n",
      "Step: 129, Loss: 0.09994105994701385\n",
      "Step: 130, Loss: 0.1002492755651474\n",
      "Step: 131, Loss: 0.0996551588177681\n",
      "Step: 132, Loss: 0.10011706501245499\n",
      "Step: 133, Loss: 0.10062992572784424\n",
      "Step: 134, Loss: 0.10021651536226273\n",
      "Step: 135, Loss: 0.10009622573852539\n",
      "Step: 136, Loss: 0.09982169419527054\n",
      "Step: 137, Loss: 0.1003158912062645\n",
      "Step: 138, Loss: 0.10016825795173645\n",
      "Step: 139, Loss: 0.10030484199523926\n",
      "Step: 140, Loss: 0.09976093471050262\n",
      "Step: 141, Loss: 0.10006352514028549\n",
      "Step: 142, Loss: 0.1001281887292862\n",
      "Step: 143, Loss: 0.0998583659529686\n",
      "Step: 144, Loss: 0.09987346082925797\n",
      "Step: 145, Loss: 0.09980994462966919\n",
      "Step: 146, Loss: 0.0999317392706871\n",
      "Step: 147, Loss: 0.09982917457818985\n",
      "Step: 148, Loss: 0.09982779622077942\n",
      "Step: 149, Loss: 0.10061391443014145\n",
      "Step: 150, Loss: 0.09979795664548874\n",
      "Step: 151, Loss: 0.09970931708812714\n",
      "Step: 152, Loss: 0.09963594377040863\n",
      "Step: 153, Loss: 0.09963522106409073\n",
      "Step: 154, Loss: 0.10000116378068924\n",
      "Step: 155, Loss: 0.09996750950813293\n",
      "Step: 156, Loss: 0.09971670806407928\n",
      "Step: 157, Loss: 0.10017547011375427\n",
      "Step: 158, Loss: 0.10012545436620712\n",
      "Step: 159, Loss: 0.1003195270895958\n",
      "Step: 160, Loss: 0.09978775680065155\n",
      "Step: 161, Loss: 0.0996885895729065\n",
      "Step: 162, Loss: 0.09987661987543106\n",
      "Step: 163, Loss: 0.09971199184656143\n",
      "Step: 164, Loss: 0.09981833398342133\n",
      "Step: 165, Loss: 0.09961768984794617\n",
      "Step: 166, Loss: 0.099788136780262\n",
      "Step: 167, Loss: 0.09958240389823914\n",
      "Step: 168, Loss: 0.0994737520813942\n",
      "Step: 169, Loss: 0.09954477846622467\n",
      "Step: 170, Loss: 0.09974025934934616\n",
      "Step: 171, Loss: 0.09960589557886124\n",
      "Step: 172, Loss: 0.09965706616640091\n",
      "Step: 173, Loss: 0.10000422596931458\n",
      "Step: 174, Loss: 0.09968196600675583\n",
      "Step: 175, Loss: 0.09961492568254471\n",
      "Step: 176, Loss: 0.09962505102157593\n",
      "Step: 177, Loss: 0.09971467405557632\n",
      "Step: 178, Loss: 0.09962061792612076\n",
      "Step: 179, Loss: 0.10008540749549866\n",
      "Step: 180, Loss: 0.09984629601240158\n",
      "Step: 181, Loss: 0.09999776631593704\n",
      "Step: 182, Loss: 0.09971991181373596\n",
      "Step: 183, Loss: 0.09985431283712387\n",
      "Step: 184, Loss: 0.09943965077400208\n",
      "Step: 185, Loss: 0.09947750717401505\n",
      "Step: 186, Loss: 0.09945524483919144\n",
      "Step: 187, Loss: 0.09984668344259262\n",
      "Step: 188, Loss: 0.09993193298578262\n",
      "Step: 189, Loss: 0.10006702691316605\n",
      "Step: 190, Loss: 0.09995385259389877\n",
      "Step: 191, Loss: 0.09992042928934097\n",
      "Step: 192, Loss: 0.09999304264783859\n",
      "Step: 193, Loss: 0.09988008439540863\n",
      "Step: 194, Loss: 0.10006505995988846\n",
      "Step: 195, Loss: 0.0996040552854538\n",
      "Step: 196, Loss: 0.09975820779800415\n",
      "Step: 197, Loss: 0.0998237207531929\n",
      "Step: 198, Loss: 0.10023599117994308\n",
      "Step: 199, Loss: 0.09960661828517914\n",
      "Step: 200, Loss: 0.10003604739904404\n",
      "Step: 201, Loss: 0.09946393966674805\n",
      "Step: 202, Loss: 0.100131094455719\n",
      "Step: 203, Loss: 0.09991443157196045\n",
      "Step: 204, Loss: 0.09985866397619247\n",
      "Step: 205, Loss: 0.09977446496486664\n",
      "Step: 206, Loss: 0.09983466565608978\n",
      "Step: 207, Loss: 0.09991797804832458\n",
      "Step: 208, Loss: 0.10013588517904282\n",
      "Step: 209, Loss: 0.09971050173044205\n",
      "Step: 210, Loss: 0.09985184669494629\n",
      "Step: 211, Loss: 0.10012660920619965\n",
      "Step: 212, Loss: 0.10011982917785645\n",
      "Step: 213, Loss: 0.10008106380701065\n",
      "Step: 214, Loss: 0.0996464341878891\n",
      "Step: 215, Loss: 0.09952999651432037\n",
      "Step: 216, Loss: 0.09975336492061615\n",
      "Step: 217, Loss: 0.10034859925508499\n",
      "Step: 218, Loss: 0.10001468658447266\n",
      "Step: 219, Loss: 0.09998709708452225\n",
      "Step: 220, Loss: 0.09959638863801956\n",
      "Step: 221, Loss: 0.10007799416780472\n",
      "Step: 222, Loss: 0.09981153905391693\n",
      "Step: 223, Loss: 0.09976130723953247\n",
      "Step: 224, Loss: 0.09986881166696548\n",
      "Step: 225, Loss: 0.1004132404923439\n",
      "Step: 226, Loss: 0.10018006712198257\n",
      "Step: 227, Loss: 0.09992454946041107\n",
      "Step: 228, Loss: 0.09985758364200592\n",
      "Step: 229, Loss: 0.10003688186407089\n",
      "Step: 230, Loss: 0.09983201324939728\n",
      "Step: 231, Loss: 0.09984076023101807\n",
      "Step: 232, Loss: 0.09968853741884232\n",
      "Step: 233, Loss: 0.10021143406629562\n",
      "Step: 234, Loss: 0.10005252063274384\n",
      "Step: 235, Loss: 0.09953856468200684\n",
      "Step: 236, Loss: 0.09988480806350708\n",
      "Step: 237, Loss: 0.09968262910842896\n",
      "Step: 238, Loss: 0.09969595074653625\n",
      "Step: 239, Loss: 0.09975699335336685\n",
      "Step: 240, Loss: 0.09970810264348984\n",
      "Step: 241, Loss: 0.09993459284305573\n",
      "Step: 242, Loss: 0.10000336170196533\n",
      "Step: 243, Loss: 0.10025124996900558\n",
      "Step: 244, Loss: 0.09998960793018341\n",
      "Step: 245, Loss: 0.09986984729766846\n",
      "Step: 246, Loss: 0.09957825392484665\n",
      "Step: 247, Loss: 0.09991302341222763\n",
      "Step: 248, Loss: 0.09960884600877762\n",
      "Step: 249, Loss: 0.10005608946084976\n",
      "Step: 250, Loss: 0.10001975297927856\n",
      "Step: 251, Loss: 0.10007541626691818\n",
      "Step: 252, Loss: 0.09981565922498703\n",
      "Step: 253, Loss: 0.0997411459684372\n",
      "Step: 254, Loss: 0.09985959529876709\n",
      "Step: 255, Loss: 0.09977702796459198\n",
      "Step: 256, Loss: 0.09994734078645706\n",
      "Step: 257, Loss: 0.09973761439323425\n",
      "Step: 258, Loss: 0.09974502772092819\n",
      "Step: 259, Loss: 0.09975292533636093\n",
      "Step: 260, Loss: 0.0995725467801094\n",
      "Step: 261, Loss: 0.09971857070922852\n",
      "Step: 262, Loss: 0.09960460662841797\n",
      "Step: 263, Loss: 0.09983714669942856\n",
      "Step: 264, Loss: 0.0994969978928566\n",
      "Step: 265, Loss: 0.09993026405572891\n",
      "Step: 266, Loss: 0.10000336170196533\n",
      "Step: 267, Loss: 0.0997394397854805\n",
      "Step: 268, Loss: 0.0997757539153099\n",
      "Step: 269, Loss: 0.10010257363319397\n",
      "Step: 270, Loss: 0.10022033751010895\n",
      "Step: 271, Loss: 0.09982910007238388\n",
      "Step: 272, Loss: 0.1000540629029274\n",
      "Step: 273, Loss: 0.09944810718297958\n",
      "Step: 274, Loss: 0.09984477609395981\n",
      "Step: 275, Loss: 0.0992569848895073\n",
      "Step: 276, Loss: 0.09955143183469772\n",
      "Step: 277, Loss: 0.09981421381235123\n",
      "Step: 278, Loss: 0.09991719573736191\n",
      "Step: 279, Loss: 0.0994381308555603\n",
      "Step: 280, Loss: 0.09965036064386368\n",
      "Step: 281, Loss: 0.09970209002494812\n",
      "Step: 282, Loss: 0.10018698871135712\n",
      "Step: 283, Loss: 0.09987757354974747\n",
      "Step: 284, Loss: 0.0996500626206398\n",
      "Step: 285, Loss: 0.100091852247715\n",
      "Step: 286, Loss: 0.09989342093467712\n",
      "Step: 287, Loss: 0.10021798312664032\n",
      "Step: 288, Loss: 0.0999566838145256\n",
      "Step: 289, Loss: 0.0995660126209259\n",
      "Step: 290, Loss: 0.09934667497873306\n",
      "Step: 291, Loss: 0.0996696874499321\n",
      "Step: 292, Loss: 0.09985460340976715\n",
      "Step: 293, Loss: 0.09978452324867249\n",
      "Step: 294, Loss: 0.09970322996377945\n",
      "Step: 295, Loss: 0.09984125941991806\n",
      "Step: 296, Loss: 0.09951096773147583\n",
      "Step: 297, Loss: 0.09961052238941193\n",
      "Step: 298, Loss: 0.09987451136112213\n",
      "Step: 299, Loss: 0.09990452975034714\n",
      "Step: 300, Loss: 0.10009047389030457\n",
      "Step: 301, Loss: 0.09997782856225967\n",
      "Step: 302, Loss: 0.09924758970737457\n",
      "Step: 303, Loss: 0.09946586191654205\n",
      "Step: 304, Loss: 0.0998670756816864\n",
      "Step: 305, Loss: 0.10005007684230804\n",
      "Step: 306, Loss: 0.10001236945390701\n",
      "Step: 307, Loss: 0.09990290552377701\n",
      "Step: 308, Loss: 0.09959187358617783\n",
      "Step: 309, Loss: 0.1002177894115448\n",
      "Step: 310, Loss: 0.09970739483833313\n",
      "Step: 311, Loss: 0.09943041950464249\n",
      "Step: 312, Loss: 0.0995108038187027\n",
      "Step: 313, Loss: 0.09992336481809616\n",
      "Step: 314, Loss: 0.09930036962032318\n",
      "Step: 315, Loss: 0.09970066696405411\n",
      "Step: 316, Loss: 0.09972327202558517\n",
      "Step: 317, Loss: 0.09958726167678833\n",
      "Step: 318, Loss: 0.09994520246982574\n",
      "Step: 319, Loss: 0.09952209144830704\n",
      "Step: 320, Loss: 0.09948418289422989\n",
      "Step: 321, Loss: 0.09951978921890259\n",
      "Step: 322, Loss: 0.09943463653326035\n",
      "Step: 323, Loss: 0.09984159469604492\n",
      "Step: 324, Loss: 0.09962151199579239\n",
      "Step: 325, Loss: 0.0996653139591217\n",
      "Step: 326, Loss: 0.09927761554718018\n",
      "Step: 327, Loss: 0.09962236881256104\n",
      "Step: 328, Loss: 0.0997888371348381\n",
      "Step: 329, Loss: 0.09921903908252716\n",
      "Step: 330, Loss: 0.10006149858236313\n",
      "Step: 331, Loss: 0.0994364321231842\n",
      "Step: 332, Loss: 0.10026399791240692\n",
      "Step: 333, Loss: 0.09971509873867035\n",
      "Step: 334, Loss: 0.10039117187261581\n",
      "Step: 335, Loss: 0.09976818412542343\n",
      "Step: 336, Loss: 0.09957128018140793\n",
      "Step: 337, Loss: 0.10015113651752472\n",
      "Step: 338, Loss: 0.10032059252262115\n",
      "Step: 339, Loss: 0.09994091838598251\n",
      "Step: 340, Loss: 0.10010998696088791\n",
      "Step: 341, Loss: 0.09978180378675461\n",
      "Step: 342, Loss: 0.09978150576353073\n",
      "Step: 343, Loss: 0.10012223571538925\n",
      "Step: 344, Loss: 0.09989391267299652\n",
      "Step: 345, Loss: 0.09989049285650253\n",
      "Step: 346, Loss: 0.10021966695785522\n",
      "Step: 347, Loss: 0.10012882947921753\n",
      "Step: 348, Loss: 0.09990854561328888\n",
      "Step: 349, Loss: 0.09975525736808777\n",
      "Step: 350, Loss: 0.10012611001729965\n",
      "Step: 351, Loss: 0.09984101355075836\n",
      "Step: 352, Loss: 0.09986074268817902\n",
      "Step: 353, Loss: 0.09975353628396988\n",
      "Step: 354, Loss: 0.09982657432556152\n",
      "Step: 355, Loss: 0.09984082728624344\n",
      "Step: 356, Loss: 0.10024143010377884\n",
      "Step: 357, Loss: 0.10007154941558838\n",
      "Step: 358, Loss: 0.10003611445426941\n",
      "Step: 359, Loss: 0.10013946890830994\n",
      "Step: 360, Loss: 0.10002493113279343\n",
      "Step: 361, Loss: 0.10018520057201385\n",
      "Step: 362, Loss: 0.09982316195964813\n",
      "Step: 363, Loss: 0.1001284196972847\n",
      "Step: 364, Loss: 0.10002555698156357\n",
      "Step: 365, Loss: 0.09970288723707199\n",
      "Step: 366, Loss: 0.09989278018474579\n",
      "Step: 367, Loss: 0.1003192737698555\n",
      "Step: 368, Loss: 0.10013553500175476\n",
      "Step: 369, Loss: 0.09986501932144165\n",
      "Step: 370, Loss: 0.10005918145179749\n",
      "Step: 371, Loss: 0.10015542060136795\n",
      "Step: 372, Loss: 0.09939827024936676\n",
      "Step: 373, Loss: 0.10003519803285599\n",
      "Step: 374, Loss: 0.0998372957110405\n",
      "Step: 375, Loss: 0.10035349428653717\n",
      "Step: 376, Loss: 0.09981882572174072\n",
      "Step: 377, Loss: 0.09971339255571365\n",
      "Step: 378, Loss: 0.09950613230466843\n",
      "Step: 379, Loss: 0.0999230444431305\n",
      "Step: 380, Loss: 0.10002359747886658\n",
      "Step: 381, Loss: 0.10012052953243256\n",
      "Step: 382, Loss: 0.10015081614255905\n",
      "Step: 383, Loss: 0.09993571788072586\n",
      "Step: 384, Loss: 0.10028791427612305\n",
      "Step: 385, Loss: 0.09959328919649124\n",
      "Step: 386, Loss: 0.09993324428796768\n",
      "Step: 387, Loss: 0.09984131157398224\n",
      "Step: 388, Loss: 0.09991424530744553\n",
      "Step: 389, Loss: 0.10016492754220963\n",
      "Step: 390, Loss: 0.09998331218957901\n",
      "Step: 391, Loss: 0.09986256062984467\n",
      "Step: 392, Loss: 0.09996448457241058\n",
      "Step: 393, Loss: 0.09943445771932602\n",
      "Step: 394, Loss: 0.09974827617406845\n",
      "Step: 395, Loss: 0.10029754787683487\n",
      "Step: 396, Loss: 0.10010799765586853\n",
      "Step: 397, Loss: 0.09959585964679718\n",
      "Step: 398, Loss: 0.09976475685834885\n",
      "Step: 399, Loss: 0.09986741095781326\n",
      "Step: 400, Loss: 0.09970524162054062\n",
      "Step: 401, Loss: 0.09987582266330719\n",
      "Step: 402, Loss: 0.09949187934398651\n",
      "Step: 403, Loss: 0.09993467479944229\n",
      "Step: 404, Loss: 0.10013909637928009\n",
      "Step: 405, Loss: 0.09980069845914841\n",
      "Step: 406, Loss: 0.10038300603628159\n",
      "Step: 407, Loss: 0.09998966008424759\n",
      "Step: 408, Loss: 0.09987952560186386\n",
      "Step: 409, Loss: 0.0997140109539032\n",
      "Step: 410, Loss: 0.09966640919446945\n",
      "Step: 411, Loss: 0.09958469867706299\n",
      "Step: 412, Loss: 0.10002169013023376\n",
      "Step: 413, Loss: 0.09982061386108398\n",
      "Step: 414, Loss: 0.09994416683912277\n",
      "Step: 415, Loss: 0.09970349818468094\n",
      "Step: 416, Loss: 0.10004322975873947\n",
      "Step: 417, Loss: 0.0996941328048706\n",
      "Step: 418, Loss: 0.09992799162864685\n",
      "Step: 419, Loss: 0.10003471374511719\n",
      "Step: 420, Loss: 0.09996713697910309\n",
      "Step: 421, Loss: 0.09973790496587753\n",
      "Step: 422, Loss: 0.09977637976408005\n",
      "Step: 423, Loss: 0.10000914335250854\n",
      "Step: 424, Loss: 0.10004518181085587\n",
      "Step: 425, Loss: 0.10023046284914017\n",
      "Step: 426, Loss: 0.09995432198047638\n",
      "Step: 427, Loss: 0.09925954788923264\n",
      "Step: 428, Loss: 0.09991728514432907\n",
      "Step: 429, Loss: 0.09946966916322708\n",
      "Step: 430, Loss: 0.09996330738067627\n",
      "Step: 431, Loss: 0.09970986098051071\n",
      "Step: 432, Loss: 0.09970537573099136\n",
      "Step: 433, Loss: 0.10017500817775726\n",
      "Step: 434, Loss: 0.0995701476931572\n",
      "Step: 435, Loss: 0.09996020793914795\n",
      "Step: 436, Loss: 0.09986601769924164\n",
      "Step: 437, Loss: 0.09947854280471802\n",
      "Step: 438, Loss: 0.09946966916322708\n",
      "Step: 439, Loss: 0.09941795468330383\n",
      "Step: 440, Loss: 0.0998176634311676\n",
      "Step: 441, Loss: 0.0996798649430275\n",
      "Step: 442, Loss: 0.0996241644024849\n",
      "Step: 443, Loss: 0.09985533356666565\n",
      "Step: 444, Loss: 0.09977034479379654\n",
      "Step: 445, Loss: 0.09954078495502472\n",
      "Step: 446, Loss: 0.09973353892564774\n",
      "Step: 447, Loss: 0.09921430796384811\n",
      "Step: 448, Loss: 0.09962459653615952\n",
      "Step: 449, Loss: 0.09963539242744446\n",
      "Step: 450, Loss: 0.09927268326282501\n",
      "Step: 451, Loss: 0.09973824769258499\n",
      "Step: 452, Loss: 0.10009041428565979\n",
      "Step: 453, Loss: 0.09966740012168884\n",
      "Step: 454, Loss: 0.10032093524932861\n",
      "Step: 455, Loss: 0.10001173615455627\n",
      "Step: 456, Loss: 0.09977711737155914\n",
      "Step: 457, Loss: 0.09990888833999634\n",
      "Step: 458, Loss: 0.10095333307981491\n",
      "Step: 459, Loss: 0.10029976814985275\n",
      "Step: 460, Loss: 0.10055486112833023\n",
      "Step: 461, Loss: 0.100510373711586\n",
      "Step: 462, Loss: 0.1001894623041153\n",
      "Step: 463, Loss: 0.1000116840004921\n",
      "Step: 464, Loss: 0.10017205774784088\n",
      "Step: 465, Loss: 0.09987734258174896\n",
      "Step: 466, Loss: 0.09993784874677658\n",
      "Step: 467, Loss: 0.10050167143344879\n",
      "Step: 468, Loss: 0.10080045461654663\n",
      "Step: 469, Loss: 0.10109151899814606\n",
      "Step: 470, Loss: 0.10154446959495544\n",
      "Step: 471, Loss: 0.101442851126194\n",
      "Step: 472, Loss: 0.10067947208881378\n",
      "Step: 473, Loss: 0.10112825036048889\n",
      "Step: 474, Loss: 0.10031715780496597\n",
      "Step: 475, Loss: 0.10064329952001572\n",
      "Step: 476, Loss: 0.1008189469575882\n",
      "Step: 477, Loss: 0.1007021814584732\n",
      "Step: 478, Loss: 0.10026750713586807\n",
      "Step: 479, Loss: 0.10025402158498764\n",
      "Step: 480, Loss: 0.10001331567764282\n",
      "Step: 481, Loss: 0.10018754005432129\n",
      "Step: 482, Loss: 0.10039862245321274\n",
      "Step: 483, Loss: 0.10031452029943466\n",
      "Step: 484, Loss: 0.10024953633546829\n",
      "Step: 485, Loss: 0.10004188120365143\n",
      "Step: 486, Loss: 0.09993920475244522\n",
      "Step: 487, Loss: 0.10009193420410156\n",
      "Step: 488, Loss: 0.09955208003520966\n",
      "Step: 489, Loss: 0.09983590990304947\n",
      "Step: 490, Loss: 0.10010526329278946\n",
      "Step: 491, Loss: 0.09993577003479004\n",
      "Step: 492, Loss: 0.10001025348901749\n",
      "Step: 493, Loss: 0.09999268501996994\n",
      "Step: 494, Loss: 0.1001076027750969\n",
      "Step: 495, Loss: 0.09974785894155502\n",
      "Step: 496, Loss: 0.10024365037679672\n",
      "Step: 497, Loss: 0.09986631572246552\n",
      "Step: 498, Loss: 0.09976784884929657\n",
      "Step: 499, Loss: 0.10009763389825821\n",
      "Step: 500, Loss: 0.0998651534318924\n",
      "Step: 501, Loss: 0.0996825098991394\n",
      "Step: 502, Loss: 0.0994672030210495\n",
      "Step: 503, Loss: 0.09935992956161499\n",
      "Step: 504, Loss: 0.09979315102100372\n",
      "Step: 505, Loss: 0.09958283603191376\n",
      "Step: 506, Loss: 0.09996163845062256\n",
      "Step: 507, Loss: 0.09993566572666168\n",
      "Step: 508, Loss: 0.1003762036561966\n",
      "Step: 509, Loss: 0.10008284449577332\n",
      "Step: 510, Loss: 0.09933522343635559\n",
      "Step: 511, Loss: 0.0995444506406784\n",
      "Step: 512, Loss: 0.10031702369451523\n",
      "Step: 513, Loss: 0.09976620972156525\n",
      "Step: 514, Loss: 0.09976449608802795\n",
      "Step: 515, Loss: 0.0998372808098793\n",
      "Step: 516, Loss: 0.0995604619383812\n",
      "Step: 517, Loss: 0.09973584115505219\n",
      "Step: 518, Loss: 0.09963072091341019\n",
      "Step: 519, Loss: 0.09962606430053711\n",
      "Step: 520, Loss: 0.09978165477514267\n",
      "Step: 521, Loss: 0.0993596613407135\n",
      "Step: 522, Loss: 0.09998148679733276\n",
      "Step: 523, Loss: 0.1000947579741478\n",
      "Step: 524, Loss: 0.09979057312011719\n",
      "Step: 525, Loss: 0.1000363752245903\n",
      "Step: 526, Loss: 0.099674291908741\n",
      "Step: 527, Loss: 0.09972982108592987\n",
      "Step: 528, Loss: 0.09998524188995361\n",
      "Step: 529, Loss: 0.09960104525089264\n",
      "Step: 530, Loss: 0.09983497858047485\n",
      "Step: 531, Loss: 0.09992524981498718\n",
      "Step: 532, Loss: 0.09966707229614258\n",
      "Step: 533, Loss: 0.09958945959806442\n",
      "Step: 534, Loss: 0.09980438649654388\n",
      "Step: 535, Loss: 0.0994650349020958\n",
      "Step: 536, Loss: 0.09983722120523453\n",
      "Step: 537, Loss: 0.09963468462228775\n",
      "Step: 538, Loss: 0.09971854090690613\n",
      "Step: 539, Loss: 0.09986277669668198\n",
      "Step: 540, Loss: 0.09945572912693024\n",
      "Step: 541, Loss: 0.10004832595586777\n",
      "Step: 542, Loss: 0.09977001696825027\n",
      "Step: 543, Loss: 0.09960256516933441\n",
      "Step: 544, Loss: 0.09927938133478165\n",
      "Step: 545, Loss: 0.09926994144916534\n",
      "Step: 546, Loss: 0.09978073835372925\n",
      "Step: 547, Loss: 0.09961432218551636\n",
      "Step: 548, Loss: 0.09936332702636719\n",
      "Step: 549, Loss: 0.09980623424053192\n",
      "Step: 550, Loss: 0.09983431547880173\n",
      "Step: 551, Loss: 0.09938172250986099\n",
      "Step: 552, Loss: 0.09943331778049469\n",
      "Step: 553, Loss: 0.09966979175806046\n",
      "Step: 554, Loss: 0.09966657310724258\n",
      "Step: 555, Loss: 0.09943845868110657\n",
      "Step: 556, Loss: 0.09954114258289337\n",
      "Step: 557, Loss: 0.09956977516412735\n",
      "Step: 558, Loss: 0.09957275539636612\n",
      "Step: 559, Loss: 0.09975379705429077\n",
      "Step: 560, Loss: 0.09955392777919769\n",
      "Step: 561, Loss: 0.09936036169528961\n",
      "Step: 562, Loss: 0.09943887591362\n",
      "Step: 563, Loss: 0.09995297342538834\n",
      "Step: 564, Loss: 0.0995626151561737\n",
      "Step: 565, Loss: 0.09926672279834747\n",
      "Step: 566, Loss: 0.09947508573532104\n",
      "Step: 567, Loss: 0.09905603528022766\n",
      "Step: 568, Loss: 0.09952205419540405\n",
      "Step: 569, Loss: 0.09928470104932785\n",
      "Step: 570, Loss: 0.09992332756519318\n",
      "Step: 571, Loss: 0.09943296760320663\n",
      "Step: 572, Loss: 0.09956399351358414\n",
      "Step: 573, Loss: 0.09931079298257828\n",
      "Step: 574, Loss: 0.09973365068435669\n",
      "Step: 575, Loss: 0.0994490459561348\n",
      "Step: 576, Loss: 0.09972744435071945\n",
      "Step: 577, Loss: 0.09966512769460678\n",
      "Step: 578, Loss: 0.09981945157051086\n",
      "Step: 579, Loss: 0.0996147096157074\n",
      "Step: 580, Loss: 0.09937168657779694\n",
      "Step: 581, Loss: 0.09961981326341629\n",
      "Step: 582, Loss: 0.09938687086105347\n",
      "Step: 583, Loss: 0.09917938709259033\n",
      "Step: 584, Loss: 0.09949074685573578\n",
      "Step: 585, Loss: 0.09972810000181198\n",
      "Step: 586, Loss: 0.09952537715435028\n",
      "Step: 587, Loss: 0.09971737116575241\n",
      "Step: 588, Loss: 0.09962929040193558\n",
      "Step: 589, Loss: 0.09988339990377426\n",
      "Step: 590, Loss: 0.09978655725717545\n",
      "Step: 591, Loss: 0.09918398410081863\n",
      "Step: 592, Loss: 0.09979411959648132\n",
      "Step: 593, Loss: 0.09905647486448288\n",
      "Step: 594, Loss: 0.09952230006456375\n",
      "Step: 595, Loss: 0.09919173270463943\n",
      "Step: 596, Loss: 0.09927141666412354\n",
      "Step: 597, Loss: 0.09943622350692749\n",
      "Step: 598, Loss: 0.09921099990606308\n",
      "Step: 599, Loss: 0.09971991926431656\n",
      "Final loss:  0.09971992\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/mixturelinear_tn_gev_twcrps_mean13.0_std4.0_epochs600_folds_1_3.pkl\n"
     ]
    }
   ],
   "source": [
    "chain_function_std = 4\n",
    "\n",
    "model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    parameter_names,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_constant = chain_function_constant,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
