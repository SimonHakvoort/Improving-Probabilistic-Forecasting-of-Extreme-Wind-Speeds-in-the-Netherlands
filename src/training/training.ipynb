{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 13:56:22.763409: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-20 13:56:22.789850: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-20 13:56:22.789874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-20 13:56:22.790552: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-20 13:56:22.794565: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-20 13:56:22.794990: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 13:56:28.292157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.training.training import train_model, train_and_save, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_distribution = 'distr_mixture_linear'\n",
    "distribution_1 = 'distr_trunc_normal'\n",
    "distribution_2 = 'distr_gev'\n",
    "\n",
    "loss = 'loss_twCRPS_sample' # options: loss_CRPS_sample, loss_twCRPS_sample, loss_log_likelihood\n",
    "\n",
    "chain_function = 'chain_function_normal_cdf' # options: chain_function_normal_cdf, chain_function_indicator, chain_function_normal_cdf_plus_constant\n",
    "chain_function_mean = 13\n",
    "chain_function_std = 4\n",
    "chain_function_threshold = 15 # 12 / 15\n",
    "chain_function_constant = 0.03\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.01\n",
    "folds = [1,2]\n",
    "parameter_names = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "neighbourhood_size = 11\n",
    "ignore = ['229', '285', '323']\n",
    "epochs = 600\n",
    "\n",
    "samples = 200\n",
    "printing = True\n",
    "pretrained = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Final loss:  0.093196005\n",
      "Final loss:  0.09337053\n",
      "Parameter weight_a set to [-0.00111217]\n",
      "Parameter weight_b set to [0.00991037]\n",
      "Parameter a_tn set to [0.64872897]\n",
      "Parameter b_tn set to [0.98099935 0.5673801  0.5018458  0.5547153  0.5747332 ]\n",
      "Parameter c_tn set to [1.3544422]\n",
      "Parameter d_tn set to [1.0815501]\n",
      "Parameter a_gev set to [1.4723818]\n",
      "Parameter b_gev set to [ 0.4985789  -0.457412   -0.11470897 -0.4884255  -0.4368702 ]\n",
      "Parameter c_gev set to [0.9117371]\n",
      "Parameter d_gev set to [ 0.13919376 -0.1959908  -0.32058188 -0.24977012 -0.1884722 ]\n",
      "Parameter e_gev set to [0.07663793]\n",
      "Parameter weight_a set to [-0.05138934]\n",
      "Parameter weight_b set to [0.01441]\n",
      "Parameter a_tn set to [0.65561455]\n",
      "Parameter b_tn set to [0.9680928  0.58187616 0.5444494  0.58817613 0.5926074 ]\n",
      "Parameter c_tn set to [1.201952]\n",
      "Parameter d_tn set to [0.9832876]\n",
      "Parameter a_gev set to [1.5266334]\n",
      "Parameter b_gev set to [ 0.54692435 -0.46425602  0.07737563 -0.5079265  -0.41863772]\n",
      "Parameter c_gev set to [0.8584015]\n",
      "Parameter d_gev set to [ 0.12245344 -0.15143391 -0.36807635 -0.21414466 -0.12092267]\n",
      "Parameter e_gev set to [0.10541613]\n",
      "Step: 0, Loss: 0.09301720559597015\n",
      "Step: 1, Loss: 0.09230092912912369\n",
      "Step: 2, Loss: 0.09202160686254501\n",
      "Step: 3, Loss: 0.09152472019195557\n",
      "Step: 4, Loss: 0.09177887439727783\n",
      "Step: 5, Loss: 0.09125320613384247\n",
      "Step: 6, Loss: 0.09139358997344971\n",
      "Step: 7, Loss: 0.09114854037761688\n",
      "Step: 8, Loss: 0.09098140150308609\n",
      "Step: 9, Loss: 0.09035790711641312\n",
      "Step: 10, Loss: 0.0902923196554184\n",
      "Step: 11, Loss: 0.08999675512313843\n",
      "Step: 12, Loss: 0.08986769616603851\n",
      "Step: 13, Loss: 0.089484803378582\n",
      "Step: 14, Loss: 0.08966532349586487\n",
      "Step: 15, Loss: 0.08970779925584793\n",
      "Step: 16, Loss: 0.0893804058432579\n",
      "Step: 17, Loss: 0.08914415538311005\n",
      "Step: 18, Loss: 0.08887818455696106\n",
      "Step: 19, Loss: 0.0888240784406662\n",
      "Step: 20, Loss: 0.08893179893493652\n",
      "Step: 21, Loss: 0.0886104479432106\n",
      "Step: 22, Loss: 0.08839467912912369\n",
      "Step: 23, Loss: 0.08824549615383148\n",
      "Step: 24, Loss: 0.08783730864524841\n",
      "Step: 25, Loss: 0.08796994388103485\n",
      "Step: 26, Loss: 0.08808927983045578\n",
      "Step: 27, Loss: 0.0878104418516159\n",
      "Step: 28, Loss: 0.08766528964042664\n",
      "Step: 29, Loss: 0.08763749152421951\n",
      "Step: 30, Loss: 0.08758767694234848\n",
      "Step: 31, Loss: 0.08760266751050949\n",
      "Step: 32, Loss: 0.0876970887184143\n",
      "Step: 33, Loss: 0.08777878433465958\n",
      "Step: 34, Loss: 0.08764969557523727\n",
      "Step: 35, Loss: 0.08751422166824341\n",
      "Step: 36, Loss: 0.0875813290476799\n",
      "Step: 37, Loss: 0.08751290291547775\n",
      "Step: 38, Loss: 0.08735468983650208\n",
      "Step: 39, Loss: 0.08722366392612457\n",
      "Step: 40, Loss: 0.08769988268613815\n",
      "Step: 41, Loss: 0.08720245957374573\n",
      "Step: 42, Loss: 0.08738116919994354\n",
      "Step: 43, Loss: 0.08730588853359222\n",
      "Step: 44, Loss: 0.08724799752235413\n",
      "Step: 45, Loss: 0.08724896609783173\n",
      "Step: 46, Loss: 0.08715377748012543\n",
      "Step: 47, Loss: 0.08726400136947632\n",
      "Step: 48, Loss: 0.08720453828573227\n",
      "Step: 49, Loss: 0.0871502161026001\n",
      "Step: 50, Loss: 0.0869389995932579\n",
      "Step: 51, Loss: 0.08709296584129333\n",
      "Step: 52, Loss: 0.08740507066249847\n",
      "Step: 53, Loss: 0.08685058355331421\n",
      "Step: 54, Loss: 0.08694326877593994\n",
      "Step: 55, Loss: 0.08708486706018448\n",
      "Step: 56, Loss: 0.08697618544101715\n",
      "Step: 57, Loss: 0.0869804322719574\n",
      "Step: 58, Loss: 0.08721540123224258\n",
      "Step: 59, Loss: 0.08691389858722687\n",
      "Step: 60, Loss: 0.08698232471942902\n",
      "Step: 61, Loss: 0.08711235225200653\n",
      "Step: 62, Loss: 0.08666004240512848\n",
      "Step: 63, Loss: 0.08698511868715286\n",
      "Step: 64, Loss: 0.0869530439376831\n",
      "Step: 65, Loss: 0.08699192106723785\n",
      "Step: 66, Loss: 0.08695226162672043\n",
      "Step: 67, Loss: 0.0866541787981987\n",
      "Step: 68, Loss: 0.08682356029748917\n",
      "Step: 69, Loss: 0.08702180534601212\n",
      "Step: 70, Loss: 0.0869879350066185\n",
      "Step: 71, Loss: 0.08674337714910507\n",
      "Step: 72, Loss: 0.08692595362663269\n",
      "Step: 73, Loss: 0.08710111677646637\n",
      "Step: 74, Loss: 0.08701250702142715\n",
      "Step: 75, Loss: 0.08700231462717056\n",
      "Step: 76, Loss: 0.08680533617734909\n",
      "Step: 77, Loss: 0.08734592795372009\n",
      "Step: 78, Loss: 0.08698690682649612\n",
      "Step: 79, Loss: 0.08678662776947021\n",
      "Step: 80, Loss: 0.08705028891563416\n",
      "Step: 81, Loss: 0.08661055564880371\n",
      "Step: 82, Loss: 0.087202288210392\n",
      "Step: 83, Loss: 0.08685179799795151\n",
      "Step: 84, Loss: 0.086915023624897\n",
      "Step: 85, Loss: 0.08667974919080734\n",
      "Step: 86, Loss: 0.08705238997936249\n",
      "Step: 87, Loss: 0.08695211261510849\n",
      "Step: 88, Loss: 0.08684942871332169\n",
      "Step: 89, Loss: 0.08700032532215118\n",
      "Step: 90, Loss: 0.08702024817466736\n",
      "Step: 91, Loss: 0.08701756596565247\n",
      "Step: 92, Loss: 0.08683346211910248\n",
      "Step: 93, Loss: 0.08703384548425674\n",
      "Step: 94, Loss: 0.08701233565807343\n",
      "Step: 95, Loss: 0.08691462874412537\n",
      "Step: 96, Loss: 0.08661989122629166\n",
      "Step: 97, Loss: 0.08660462498664856\n",
      "Step: 98, Loss: 0.08669234812259674\n",
      "Step: 99, Loss: 0.08713804930448532\n",
      "Step: 100, Loss: 0.08697553724050522\n",
      "Step: 101, Loss: 0.08698210120201111\n",
      "Step: 102, Loss: 0.08695294708013535\n",
      "Step: 103, Loss: 0.08679790049791336\n",
      "Step: 104, Loss: 0.08680743724107742\n",
      "Step: 105, Loss: 0.08689044415950775\n",
      "Step: 106, Loss: 0.08703333139419556\n",
      "Step: 107, Loss: 0.08703657984733582\n",
      "Step: 108, Loss: 0.0869888886809349\n",
      "Step: 109, Loss: 0.08654806017875671\n",
      "Step: 110, Loss: 0.0866985023021698\n",
      "Step: 111, Loss: 0.08632025867700577\n",
      "Step: 112, Loss: 0.0869683250784874\n",
      "Step: 113, Loss: 0.0866650715470314\n",
      "Step: 114, Loss: 0.08649062365293503\n",
      "Step: 115, Loss: 0.08661874383687973\n",
      "Step: 116, Loss: 0.08701661229133606\n",
      "Step: 117, Loss: 0.08678611367940903\n",
      "Step: 118, Loss: 0.08668245375156403\n",
      "Step: 119, Loss: 0.08650337159633636\n",
      "Step: 120, Loss: 0.08649114519357681\n",
      "Step: 121, Loss: 0.08646444976329803\n",
      "Step: 122, Loss: 0.08683326095342636\n",
      "Step: 123, Loss: 0.0867876335978508\n",
      "Step: 124, Loss: 0.08680973947048187\n",
      "Step: 125, Loss: 0.08686552941799164\n",
      "Step: 126, Loss: 0.08644811064004898\n",
      "Step: 127, Loss: 0.08671078085899353\n",
      "Step: 128, Loss: 0.0865638330578804\n",
      "Step: 129, Loss: 0.08669887483119965\n",
      "Step: 130, Loss: 0.08659356832504272\n",
      "Step: 131, Loss: 0.08687885850667953\n",
      "Step: 132, Loss: 0.08709973096847534\n",
      "Step: 133, Loss: 0.08667962998151779\n",
      "Step: 134, Loss: 0.08643411099910736\n",
      "Step: 135, Loss: 0.0868261381983757\n",
      "Step: 136, Loss: 0.08641068637371063\n",
      "Step: 137, Loss: 0.08655630052089691\n",
      "Step: 138, Loss: 0.08659262955188751\n",
      "Step: 139, Loss: 0.08647681772708893\n",
      "Step: 140, Loss: 0.08661659806966782\n",
      "Step: 141, Loss: 0.08662585914134979\n",
      "Step: 142, Loss: 0.0867827758193016\n",
      "Step: 143, Loss: 0.08629634976387024\n",
      "Step: 144, Loss: 0.08672720193862915\n",
      "Step: 145, Loss: 0.08651573210954666\n",
      "Step: 146, Loss: 0.0863434225320816\n",
      "Step: 147, Loss: 0.0864512175321579\n",
      "Step: 148, Loss: 0.08662243187427521\n",
      "Step: 149, Loss: 0.08651139587163925\n",
      "Step: 150, Loss: 0.0866350382566452\n",
      "Step: 151, Loss: 0.08660160005092621\n",
      "Step: 152, Loss: 0.08647190034389496\n",
      "Step: 153, Loss: 0.08635053038597107\n",
      "Step: 154, Loss: 0.08670632541179657\n",
      "Step: 155, Loss: 0.08664196729660034\n",
      "Step: 156, Loss: 0.08643350750207901\n",
      "Step: 157, Loss: 0.0864020511507988\n",
      "Step: 158, Loss: 0.08624346554279327\n",
      "Step: 159, Loss: 0.08638765662908554\n",
      "Step: 160, Loss: 0.08666062355041504\n",
      "Step: 161, Loss: 0.08621141314506531\n",
      "Step: 162, Loss: 0.08654845505952835\n",
      "Step: 163, Loss: 0.0864410549402237\n",
      "Step: 164, Loss: 0.08661569654941559\n",
      "Step: 165, Loss: 0.08642550557851791\n",
      "Step: 166, Loss: 0.08646903187036514\n",
      "Step: 167, Loss: 0.08638846129179001\n",
      "Step: 168, Loss: 0.08639403432607651\n",
      "Step: 169, Loss: 0.08665908873081207\n",
      "Step: 170, Loss: 0.08643580228090286\n",
      "Step: 171, Loss: 0.08645539730787277\n",
      "Step: 172, Loss: 0.08656638115644455\n",
      "Step: 173, Loss: 0.08628161996603012\n",
      "Step: 174, Loss: 0.08667095750570297\n",
      "Step: 175, Loss: 0.08661513030529022\n",
      "Step: 176, Loss: 0.0865190327167511\n",
      "Step: 177, Loss: 0.08615463972091675\n",
      "Step: 178, Loss: 0.08673523366451263\n",
      "Step: 179, Loss: 0.08653786778450012\n",
      "Step: 180, Loss: 0.08643818646669388\n",
      "Step: 181, Loss: 0.0863988921046257\n",
      "Step: 182, Loss: 0.08623073250055313\n",
      "Step: 183, Loss: 0.08642049133777618\n",
      "Step: 184, Loss: 0.08618422597646713\n",
      "Step: 185, Loss: 0.08627835661172867\n",
      "Step: 186, Loss: 0.08643826097249985\n",
      "Step: 187, Loss: 0.08629806339740753\n",
      "Step: 188, Loss: 0.08653430640697479\n",
      "Step: 189, Loss: 0.08665772527456284\n",
      "Step: 190, Loss: 0.08615376055240631\n",
      "Step: 191, Loss: 0.08641424775123596\n",
      "Step: 192, Loss: 0.08621169626712799\n",
      "Step: 193, Loss: 0.086419977247715\n",
      "Step: 194, Loss: 0.08634261041879654\n",
      "Step: 195, Loss: 0.0865006148815155\n",
      "Step: 196, Loss: 0.08635968714952469\n",
      "Step: 197, Loss: 0.0861329436302185\n",
      "Step: 198, Loss: 0.08622901886701584\n",
      "Step: 199, Loss: 0.08643703907728195\n",
      "Step: 200, Loss: 0.086312435567379\n",
      "Step: 201, Loss: 0.08605637401342392\n",
      "Step: 202, Loss: 0.08635395020246506\n",
      "Step: 203, Loss: 0.0860796719789505\n",
      "Step: 204, Loss: 0.0860975906252861\n",
      "Step: 205, Loss: 0.08664855360984802\n",
      "Step: 206, Loss: 0.08640433102846146\n",
      "Step: 207, Loss: 0.08633480966091156\n",
      "Step: 208, Loss: 0.08627987653017044\n",
      "Step: 209, Loss: 0.0861072912812233\n",
      "Step: 210, Loss: 0.08606015145778656\n",
      "Step: 211, Loss: 0.0863470658659935\n",
      "Step: 212, Loss: 0.08623828738927841\n",
      "Step: 213, Loss: 0.08615069836378098\n",
      "Step: 214, Loss: 0.08606946468353271\n",
      "Step: 215, Loss: 0.08605669438838959\n",
      "Step: 216, Loss: 0.0862734317779541\n",
      "Step: 217, Loss: 0.08642691373825073\n",
      "Step: 218, Loss: 0.08621746301651001\n",
      "Step: 219, Loss: 0.08659885078668594\n",
      "Step: 220, Loss: 0.08624795079231262\n",
      "Step: 221, Loss: 0.0862891897559166\n",
      "Step: 222, Loss: 0.08624150604009628\n",
      "Step: 223, Loss: 0.08633369952440262\n",
      "Step: 224, Loss: 0.08607019484043121\n",
      "Step: 225, Loss: 0.08616267144680023\n",
      "Step: 226, Loss: 0.08640758693218231\n",
      "Step: 227, Loss: 0.08615732938051224\n",
      "Step: 228, Loss: 0.08613560348749161\n",
      "Step: 229, Loss: 0.08630917221307755\n",
      "Step: 230, Loss: 0.08613728731870651\n",
      "Step: 231, Loss: 0.08657190948724747\n",
      "Step: 232, Loss: 0.0863313227891922\n",
      "Step: 233, Loss: 0.08636053651571274\n",
      "Step: 234, Loss: 0.08627837896347046\n",
      "Step: 235, Loss: 0.0861784815788269\n",
      "Step: 236, Loss: 0.0861072763800621\n",
      "Step: 237, Loss: 0.08623003214597702\n",
      "Step: 238, Loss: 0.08601868897676468\n",
      "Step: 239, Loss: 0.08606449514627457\n",
      "Step: 240, Loss: 0.0859859436750412\n",
      "Step: 241, Loss: 0.08617417514324188\n",
      "Step: 242, Loss: 0.08626139909029007\n",
      "Step: 243, Loss: 0.08604181557893753\n",
      "Step: 244, Loss: 0.08649108558893204\n",
      "Step: 245, Loss: 0.08635810017585754\n",
      "Step: 246, Loss: 0.08620826154947281\n",
      "Step: 247, Loss: 0.08633772283792496\n",
      "Step: 248, Loss: 0.08630125224590302\n",
      "Step: 249, Loss: 0.08599676191806793\n",
      "Step: 250, Loss: 0.08643649518489838\n",
      "Step: 251, Loss: 0.08631173521280289\n",
      "Step: 252, Loss: 0.08626843988895416\n",
      "Step: 253, Loss: 0.08654177188873291\n",
      "Step: 254, Loss: 0.08615535497665405\n",
      "Step: 255, Loss: 0.08645305037498474\n",
      "Step: 256, Loss: 0.08621128648519516\n",
      "Step: 257, Loss: 0.08599106967449188\n",
      "Step: 258, Loss: 0.08598321676254272\n",
      "Step: 259, Loss: 0.08622793108224869\n",
      "Step: 260, Loss: 0.08612794429063797\n",
      "Step: 261, Loss: 0.08597953617572784\n",
      "Step: 262, Loss: 0.08624108135700226\n",
      "Step: 263, Loss: 0.08613760024309158\n",
      "Step: 264, Loss: 0.08617888391017914\n",
      "Step: 265, Loss: 0.08616366237401962\n",
      "Step: 266, Loss: 0.08617380261421204\n",
      "Step: 267, Loss: 0.08632136881351471\n",
      "Step: 268, Loss: 0.08615835011005402\n",
      "Step: 269, Loss: 0.08630207180976868\n",
      "Step: 270, Loss: 0.08617323637008667\n",
      "Step: 271, Loss: 0.08584393560886383\n",
      "Step: 272, Loss: 0.08599905669689178\n",
      "Step: 273, Loss: 0.08588171005249023\n",
      "Step: 274, Loss: 0.0858570858836174\n",
      "Step: 275, Loss: 0.08591596782207489\n",
      "Step: 276, Loss: 0.08650598675012589\n",
      "Step: 277, Loss: 0.08588854223489761\n",
      "Step: 278, Loss: 0.08628201484680176\n",
      "Step: 279, Loss: 0.08638142049312592\n",
      "Step: 280, Loss: 0.08594365417957306\n",
      "Step: 281, Loss: 0.08613001555204391\n",
      "Step: 282, Loss: 0.08598576486110687\n",
      "Step: 283, Loss: 0.08582942932844162\n",
      "Step: 284, Loss: 0.08587633073329926\n",
      "Step: 285, Loss: 0.08586366474628448\n",
      "Step: 286, Loss: 0.08602596074342728\n",
      "Step: 287, Loss: 0.08595592528581619\n",
      "Step: 288, Loss: 0.0859738141298294\n",
      "Step: 289, Loss: 0.08635257184505463\n",
      "Step: 290, Loss: 0.08617648482322693\n",
      "Step: 291, Loss: 0.08602867275476456\n",
      "Step: 292, Loss: 0.0862567350268364\n",
      "Step: 293, Loss: 0.08603724837303162\n",
      "Step: 294, Loss: 0.08612439781427383\n",
      "Step: 295, Loss: 0.08619274199008942\n",
      "Step: 296, Loss: 0.08616843074560165\n",
      "Step: 297, Loss: 0.08591578900814056\n",
      "Step: 298, Loss: 0.08605503290891647\n",
      "Step: 299, Loss: 0.0860954001545906\n",
      "Step: 300, Loss: 0.08630668371915817\n",
      "Step: 301, Loss: 0.08582260459661484\n",
      "Step: 302, Loss: 0.08619663119316101\n",
      "Step: 303, Loss: 0.0861574187874794\n",
      "Step: 304, Loss: 0.08603518456220627\n",
      "Step: 305, Loss: 0.08621004968881607\n",
      "Step: 306, Loss: 0.08616453409194946\n",
      "Step: 307, Loss: 0.08616282045841217\n",
      "Step: 308, Loss: 0.08620939403772354\n",
      "Step: 309, Loss: 0.0863303616642952\n",
      "Step: 310, Loss: 0.08608599752187729\n",
      "Step: 311, Loss: 0.08602640777826309\n",
      "Step: 312, Loss: 0.08601681888103485\n",
      "Step: 313, Loss: 0.08625926077365875\n",
      "Step: 314, Loss: 0.0862022191286087\n",
      "Step: 315, Loss: 0.08606318384408951\n",
      "Step: 316, Loss: 0.08610959351062775\n",
      "Step: 317, Loss: 0.08620347082614899\n",
      "Step: 318, Loss: 0.08596986532211304\n",
      "Step: 319, Loss: 0.0862824097275734\n",
      "Step: 320, Loss: 0.08580513298511505\n",
      "Step: 321, Loss: 0.08605844527482986\n",
      "Step: 322, Loss: 0.08586744964122772\n",
      "Step: 323, Loss: 0.08607989549636841\n",
      "Step: 324, Loss: 0.08627700060606003\n",
      "Step: 325, Loss: 0.08596104383468628\n",
      "Step: 326, Loss: 0.08588506281375885\n",
      "Step: 327, Loss: 0.08598686009645462\n",
      "Step: 328, Loss: 0.0859902873635292\n",
      "Step: 329, Loss: 0.08587560802698135\n",
      "Step: 330, Loss: 0.08606918156147003\n",
      "Step: 331, Loss: 0.08619207888841629\n",
      "Step: 332, Loss: 0.08578602224588394\n",
      "Step: 333, Loss: 0.08605923503637314\n",
      "Step: 334, Loss: 0.08587243407964706\n",
      "Step: 335, Loss: 0.08608721941709518\n",
      "Step: 336, Loss: 0.08597221970558167\n",
      "Step: 337, Loss: 0.08631670475006104\n",
      "Step: 338, Loss: 0.08627942949533463\n",
      "Step: 339, Loss: 0.0860271081328392\n",
      "Step: 340, Loss: 0.08590622246265411\n",
      "Step: 341, Loss: 0.08615189045667648\n",
      "Step: 342, Loss: 0.08624705672264099\n",
      "Step: 343, Loss: 0.08588604629039764\n",
      "Step: 344, Loss: 0.08630726486444473\n",
      "Step: 345, Loss: 0.08602747321128845\n",
      "Step: 346, Loss: 0.08622311800718307\n",
      "Step: 347, Loss: 0.08603315055370331\n",
      "Step: 348, Loss: 0.08609135448932648\n",
      "Step: 349, Loss: 0.08592702448368073\n",
      "Step: 350, Loss: 0.08582836389541626\n",
      "Step: 351, Loss: 0.08598636090755463\n",
      "Step: 352, Loss: 0.08611275255680084\n",
      "Step: 353, Loss: 0.0863533541560173\n",
      "Step: 354, Loss: 0.08611811697483063\n",
      "Step: 355, Loss: 0.08599170297384262\n",
      "Step: 356, Loss: 0.08628370612859726\n",
      "Step: 357, Loss: 0.0861537903547287\n",
      "Step: 358, Loss: 0.08613789081573486\n",
      "Step: 359, Loss: 0.0859624370932579\n",
      "Step: 360, Loss: 0.08587449043989182\n",
      "Step: 361, Loss: 0.08597726374864578\n",
      "Step: 362, Loss: 0.08604889363050461\n",
      "Step: 363, Loss: 0.0859326720237732\n",
      "Step: 364, Loss: 0.08596694469451904\n",
      "Step: 365, Loss: 0.08607003837823868\n",
      "Step: 366, Loss: 0.08587829768657684\n",
      "Step: 367, Loss: 0.08600540459156036\n",
      "Step: 368, Loss: 0.0859941765666008\n",
      "Step: 369, Loss: 0.08607685565948486\n",
      "Step: 370, Loss: 0.08599278330802917\n",
      "Step: 371, Loss: 0.08635535836219788\n",
      "Step: 372, Loss: 0.08598321676254272\n",
      "Step: 373, Loss: 0.08599423617124557\n",
      "Step: 374, Loss: 0.0860382467508316\n",
      "Step: 375, Loss: 0.08594053238630295\n",
      "Step: 376, Loss: 0.08572017401456833\n",
      "Step: 377, Loss: 0.08594192564487457\n",
      "Step: 378, Loss: 0.0857367068529129\n",
      "Step: 379, Loss: 0.08588763326406479\n",
      "Step: 380, Loss: 0.08620218932628632\n",
      "Step: 381, Loss: 0.08596033602952957\n",
      "Step: 382, Loss: 0.08590251207351685\n",
      "Step: 383, Loss: 0.08595270663499832\n",
      "Step: 384, Loss: 0.08632685989141464\n",
      "Step: 385, Loss: 0.08606667816638947\n",
      "Step: 386, Loss: 0.08608192205429077\n",
      "Step: 387, Loss: 0.08625578880310059\n",
      "Step: 388, Loss: 0.08601219952106476\n",
      "Step: 389, Loss: 0.08586742728948593\n",
      "Step: 390, Loss: 0.08590921014547348\n",
      "Step: 391, Loss: 0.08562722057104111\n",
      "Step: 392, Loss: 0.08619827032089233\n",
      "Step: 393, Loss: 0.08564276993274689\n",
      "Step: 394, Loss: 0.08618859201669693\n",
      "Step: 395, Loss: 0.0861019566655159\n",
      "Step: 396, Loss: 0.08618131279945374\n",
      "Step: 397, Loss: 0.08601067215204239\n",
      "Step: 398, Loss: 0.08599147945642471\n",
      "Step: 399, Loss: 0.08583378791809082\n",
      "Step: 400, Loss: 0.0858316719532013\n",
      "Step: 401, Loss: 0.08570298552513123\n",
      "Step: 402, Loss: 0.08621162176132202\n",
      "Step: 403, Loss: 0.08607614040374756\n",
      "Step: 404, Loss: 0.0857342928647995\n",
      "Step: 405, Loss: 0.08625245839357376\n",
      "Step: 406, Loss: 0.08593712747097015\n",
      "Step: 407, Loss: 0.08617345988750458\n",
      "Step: 408, Loss: 0.08587390184402466\n",
      "Step: 409, Loss: 0.08590241521596909\n",
      "Step: 410, Loss: 0.08603561669588089\n",
      "Step: 411, Loss: 0.0862780436873436\n",
      "Step: 412, Loss: 0.08582009375095367\n",
      "Step: 413, Loss: 0.08629421144723892\n",
      "Step: 414, Loss: 0.08604637533426285\n",
      "Step: 415, Loss: 0.08614300936460495\n",
      "Step: 416, Loss: 0.08610697835683823\n",
      "Step: 417, Loss: 0.08607558161020279\n",
      "Step: 418, Loss: 0.0859566330909729\n",
      "Step: 419, Loss: 0.08594612032175064\n",
      "Step: 420, Loss: 0.08613862842321396\n",
      "Step: 421, Loss: 0.08621295541524887\n",
      "Step: 422, Loss: 0.0858599916100502\n",
      "Step: 423, Loss: 0.08576025813817978\n",
      "Step: 424, Loss: 0.08597932755947113\n",
      "Step: 425, Loss: 0.08604764938354492\n",
      "Step: 426, Loss: 0.08594272285699844\n",
      "Step: 427, Loss: 0.08605843782424927\n",
      "Step: 428, Loss: 0.0860985666513443\n",
      "Step: 429, Loss: 0.08596902340650558\n",
      "Step: 430, Loss: 0.08601053059101105\n",
      "Step: 431, Loss: 0.08608980476856232\n",
      "Step: 432, Loss: 0.08594011515378952\n",
      "Step: 433, Loss: 0.08627872169017792\n",
      "Step: 434, Loss: 0.08593033254146576\n",
      "Step: 435, Loss: 0.08603419363498688\n",
      "Step: 436, Loss: 0.08617298305034637\n",
      "Step: 437, Loss: 0.08629579097032547\n",
      "Step: 438, Loss: 0.08663101494312286\n",
      "Step: 439, Loss: 0.08615216612815857\n",
      "Step: 440, Loss: 0.08630286902189255\n",
      "Step: 441, Loss: 0.08601786196231842\n",
      "Step: 442, Loss: 0.0861961841583252\n",
      "Step: 443, Loss: 0.0862840861082077\n",
      "Step: 444, Loss: 0.08623619377613068\n",
      "Step: 445, Loss: 0.08635158836841583\n",
      "Step: 446, Loss: 0.0865187719464302\n",
      "Step: 447, Loss: 0.08634519577026367\n",
      "Step: 448, Loss: 0.08627690374851227\n",
      "Step: 449, Loss: 0.0861436277627945\n",
      "Step: 450, Loss: 0.08595725893974304\n",
      "Step: 451, Loss: 0.08588670939207077\n",
      "Step: 452, Loss: 0.08625556528568268\n",
      "Step: 453, Loss: 0.08605688065290451\n",
      "Step: 454, Loss: 0.08612518012523651\n",
      "Step: 455, Loss: 0.08597882091999054\n",
      "Step: 456, Loss: 0.08595848828554153\n",
      "Step: 457, Loss: 0.08615659177303314\n",
      "Step: 458, Loss: 0.0859881192445755\n",
      "Step: 459, Loss: 0.08617774397134781\n",
      "Step: 460, Loss: 0.08594095706939697\n",
      "Step: 461, Loss: 0.08629917353391647\n",
      "Step: 462, Loss: 0.0863618329167366\n",
      "Step: 463, Loss: 0.08630654215812683\n",
      "Step: 464, Loss: 0.08631619065999985\n",
      "Step: 465, Loss: 0.08632386475801468\n",
      "Step: 466, Loss: 0.08609811216592789\n",
      "Step: 467, Loss: 0.08618714660406113\n",
      "Step: 468, Loss: 0.08605918288230896\n",
      "Step: 469, Loss: 0.08626683801412582\n",
      "Step: 470, Loss: 0.0863415077328682\n",
      "Step: 471, Loss: 0.08632093667984009\n",
      "Step: 472, Loss: 0.08627660572528839\n",
      "Step: 473, Loss: 0.08625674992799759\n",
      "Step: 474, Loss: 0.08621539920568466\n",
      "Step: 475, Loss: 0.08623706549406052\n",
      "Step: 476, Loss: 0.08631555736064911\n",
      "Step: 477, Loss: 0.08647041767835617\n",
      "Step: 478, Loss: 0.08639980852603912\n",
      "Step: 479, Loss: 0.08617997914552689\n",
      "Step: 480, Loss: 0.08643826842308044\n",
      "Step: 481, Loss: 0.08658479154109955\n",
      "Step: 482, Loss: 0.08642930537462234\n",
      "Step: 483, Loss: 0.08650761097669601\n",
      "Step: 484, Loss: 0.0865769237279892\n",
      "Step: 485, Loss: 0.08596552908420563\n",
      "Step: 486, Loss: 0.08640413731336594\n",
      "Step: 487, Loss: 0.08657409995794296\n",
      "Step: 488, Loss: 0.0865560919046402\n",
      "Step: 489, Loss: 0.08634129911661148\n",
      "Step: 490, Loss: 0.08671445399522781\n",
      "Step: 491, Loss: 0.08647122979164124\n",
      "Step: 492, Loss: 0.08648521453142166\n",
      "Step: 493, Loss: 0.08632048219442368\n",
      "Step: 494, Loss: 0.08637775480747223\n",
      "Step: 495, Loss: 0.08674610406160355\n",
      "Step: 496, Loss: 0.08679629117250443\n",
      "Step: 497, Loss: 0.08638124912977219\n",
      "Step: 498, Loss: 0.08625941723585129\n",
      "Step: 499, Loss: 0.08612240850925446\n",
      "Step: 500, Loss: 0.08630342781543732\n",
      "Step: 501, Loss: 0.0863112360239029\n",
      "Step: 502, Loss: 0.08641424030065536\n",
      "Step: 503, Loss: 0.08622652292251587\n",
      "Step: 504, Loss: 0.08617556095123291\n",
      "Step: 505, Loss: 0.08635735511779785\n",
      "Step: 506, Loss: 0.08612595498561859\n",
      "Step: 507, Loss: 0.08639070391654968\n",
      "Step: 508, Loss: 0.08613622188568115\n",
      "Step: 509, Loss: 0.08612649887800217\n",
      "Step: 510, Loss: 0.08625815808773041\n",
      "Step: 511, Loss: 0.08588805794715881\n",
      "Step: 512, Loss: 0.08618216961622238\n",
      "Step: 513, Loss: 0.0860113576054573\n",
      "Step: 514, Loss: 0.0860956609249115\n",
      "Step: 515, Loss: 0.08609239757061005\n",
      "Step: 516, Loss: 0.08624634146690369\n",
      "Step: 517, Loss: 0.08629825711250305\n",
      "Step: 518, Loss: 0.08597157895565033\n",
      "Step: 519, Loss: 0.08599650114774704\n",
      "Step: 520, Loss: 0.08621573448181152\n",
      "Step: 521, Loss: 0.08616059273481369\n",
      "Step: 522, Loss: 0.08618522435426712\n",
      "Step: 523, Loss: 0.08601607382297516\n",
      "Step: 524, Loss: 0.08616717159748077\n",
      "Step: 525, Loss: 0.08608131110668182\n",
      "Step: 526, Loss: 0.08623646944761276\n",
      "Step: 527, Loss: 0.08587419986724854\n",
      "Step: 528, Loss: 0.08617798984050751\n",
      "Step: 529, Loss: 0.08624815195798874\n",
      "Step: 530, Loss: 0.08624023199081421\n",
      "Step: 531, Loss: 0.08605161309242249\n",
      "Step: 532, Loss: 0.0861763060092926\n",
      "Step: 533, Loss: 0.08618103712797165\n",
      "Step: 534, Loss: 0.08614513278007507\n",
      "Step: 535, Loss: 0.08606068789958954\n",
      "Step: 536, Loss: 0.08630727231502533\n",
      "Step: 537, Loss: 0.08610454201698303\n",
      "Step: 538, Loss: 0.08609191328287125\n",
      "Step: 539, Loss: 0.08603992313146591\n",
      "Step: 540, Loss: 0.08556323498487473\n",
      "Step: 541, Loss: 0.08600317686796188\n",
      "Step: 542, Loss: 0.08608105033636093\n",
      "Step: 543, Loss: 0.08583320677280426\n",
      "Step: 544, Loss: 0.08623091876506805\n",
      "Step: 545, Loss: 0.08620944619178772\n",
      "Step: 546, Loss: 0.0862266942858696\n",
      "Step: 547, Loss: 0.08603815734386444\n",
      "Step: 548, Loss: 0.08615229278802872\n",
      "Step: 549, Loss: 0.08597937971353531\n",
      "Step: 550, Loss: 0.08612484484910965\n",
      "Step: 551, Loss: 0.086088627576828\n",
      "Step: 552, Loss: 0.08591235429048538\n",
      "Step: 553, Loss: 0.08588437736034393\n",
      "Step: 554, Loss: 0.08559216558933258\n",
      "Step: 555, Loss: 0.08572861552238464\n",
      "Step: 556, Loss: 0.08592754602432251\n",
      "Step: 557, Loss: 0.0857735201716423\n",
      "Step: 558, Loss: 0.08571897447109222\n",
      "Step: 559, Loss: 0.08593621850013733\n",
      "Step: 560, Loss: 0.08632947504520416\n",
      "Step: 561, Loss: 0.08610522747039795\n",
      "Step: 562, Loss: 0.0859566330909729\n",
      "Step: 563, Loss: 0.08612766861915588\n",
      "Step: 564, Loss: 0.08597031235694885\n",
      "Step: 565, Loss: 0.08595772087574005\n",
      "Step: 566, Loss: 0.08603326231241226\n",
      "Step: 567, Loss: 0.0859493836760521\n",
      "Step: 568, Loss: 0.08631489425897598\n",
      "Step: 569, Loss: 0.08593530207872391\n",
      "Step: 570, Loss: 0.08598688989877701\n",
      "Step: 571, Loss: 0.08592775464057922\n",
      "Step: 572, Loss: 0.08601553738117218\n",
      "Step: 573, Loss: 0.08585808426141739\n",
      "Step: 574, Loss: 0.08589743077754974\n",
      "Step: 575, Loss: 0.08633534610271454\n",
      "Step: 576, Loss: 0.08618354797363281\n",
      "Step: 577, Loss: 0.08613349497318268\n",
      "Step: 578, Loss: 0.08605020493268967\n",
      "Step: 579, Loss: 0.08625283092260361\n",
      "Step: 580, Loss: 0.08594463765621185\n",
      "Step: 581, Loss: 0.08596739172935486\n",
      "Step: 582, Loss: 0.08594578504562378\n",
      "Step: 583, Loss: 0.08590710908174515\n",
      "Step: 584, Loss: 0.08606917411088943\n",
      "Step: 585, Loss: 0.08594060689210892\n",
      "Step: 586, Loss: 0.08621670305728912\n",
      "Step: 587, Loss: 0.0859866812825203\n",
      "Step: 588, Loss: 0.08574363589286804\n",
      "Step: 589, Loss: 0.08591855317354202\n",
      "Step: 590, Loss: 0.08589237928390503\n",
      "Step: 591, Loss: 0.08579414337873459\n",
      "Step: 592, Loss: 0.08602190762758255\n",
      "Step: 593, Loss: 0.08604958653450012\n",
      "Step: 594, Loss: 0.08612123131752014\n",
      "Step: 595, Loss: 0.08622991293668747\n",
      "Step: 596, Loss: 0.08579987287521362\n",
      "Step: 597, Loss: 0.08608748018741608\n",
      "Step: 598, Loss: 0.08610335737466812\n",
      "Step: 599, Loss: 0.0859784483909607\n",
      "Final loss:  0.08597845\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/mixturelinear_tn_gev_twcrps_mean13.0_std4.0_epochs600.pkl\n"
     ]
    }
   ],
   "source": [
    "model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    parameter_names,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_constant = chain_function_constant,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOS Model Information:\n",
      "Loss function: loss_twCRPS_sample (Samples: 200)\n",
      "Forecast distribution: distr_mixture\n",
      "Distribution 1: distr_trunc_normal\n",
      "Distribution 2: distr_gev\n",
      "Mixture weight: [0.6714824]\n",
      "Parameters:\n",
      "  weight: [0.6714824]\n",
      "  a_tn: [0.8587427]\n",
      "  b_tn: [ 0.9033643  -0.8743947  -0.10445029 -0.27649736  0.85969   ]\n",
      "  c_tn: [1.8735183]\n",
      "  d_tn: [0.7428804]\n",
      "  a_gev: [-0.4236689]\n",
      "  b_gev: [ 0.9333376   0.5409048  -0.3650169   0.3564078   0.61552596]\n",
      "  c_gev: [1.4305779]\n",
      "  d_gev: [ 0.09890767 -0.5423227   0.16434298  0.31506124 -0.69037396]\n",
      "  e_gev: [-0.31247506]\n",
      "Features: wind_speed, press, kinetic, humid, geopot\n",
      "Number of features: 5\n",
      "Neighbourhood size: 11\n",
      "Chaining function: chain_function_normal_cdf (Mean: 13.0, Std: 4.0)\n",
      "Optimizer: Adam\n",
      "Learning rate: 0.009999999776482582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
