{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 08:54:10.832182: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-28 08:54:10.859837: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-28 08:54:10.859862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-28 08:54:10.860594: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-28 08:54:10.864800: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-28 08:54:10.865085: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-28 08:54:21.505693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.training.training import train_model, train_and_save, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_distribution = 'distr_trunc_normal_features'\n",
    "distribution_1 = 'distr_trunc_normal'\n",
    "distribution_2 = 'distr_gev_spatial'\n",
    "\n",
    "loss = 'loss_twCRPS_sample' # options: loss_CRPS_sample, loss_twCRPS_sample, loss_log_likelihood\n",
    "\n",
    "chain_function = 'chain_function_normal_cdf_plus_constant' # options: chain_function_normal_cdf, chain_function_indicator, chain_function_normal_cdf_plus_constant\n",
    "chain_function_mean = 11\n",
    "chain_function_std = 2\n",
    "chain_function_threshold = 15 # 12 / 15\n",
    "chain_function_constant = 0.02\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.03\n",
    "folds = [2,3]\n",
    "parameter_names = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "neighbourhood_size = 11\n",
    "ignore = ['229', '285', '323']\n",
    "epochs = 600\n",
    "\n",
    "samples = 100\n",
    "printing = True\n",
    "pretrained = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution with features\n",
      "Step: 0, Loss: 0.21167680621147156\n",
      "Step: 1, Loss: 0.19554102420806885\n",
      "Step: 2, Loss: 0.1827104389667511\n",
      "Step: 3, Loss: 0.17259614169597626\n",
      "Step: 4, Loss: 0.1657819151878357\n",
      "Step: 5, Loss: 0.16002097725868225\n",
      "Step: 6, Loss: 0.15680503845214844\n",
      "Step: 7, Loss: 0.15383225679397583\n",
      "Step: 8, Loss: 0.1506287157535553\n",
      "Step: 9, Loss: 0.14865481853485107\n",
      "Step: 10, Loss: 0.146095409989357\n",
      "Step: 11, Loss: 0.1437155306339264\n",
      "Step: 12, Loss: 0.1413622498512268\n",
      "Step: 13, Loss: 0.13864295184612274\n",
      "Step: 14, Loss: 0.13573303818702698\n",
      "Step: 15, Loss: 0.13259148597717285\n",
      "Step: 16, Loss: 0.1304941028356552\n",
      "Step: 17, Loss: 0.12802904844284058\n",
      "Step: 18, Loss: 0.1261807680130005\n",
      "Step: 19, Loss: 0.12429492175579071\n",
      "Step: 20, Loss: 0.12265383452177048\n",
      "Step: 21, Loss: 0.12172526121139526\n",
      "Step: 22, Loss: 0.12024581432342529\n",
      "Step: 23, Loss: 0.12026594579219818\n",
      "Step: 24, Loss: 0.11931362748146057\n",
      "Step: 25, Loss: 0.11861518025398254\n",
      "Step: 26, Loss: 0.1172071099281311\n",
      "Step: 27, Loss: 0.11589492857456207\n",
      "Step: 28, Loss: 0.11576247215270996\n",
      "Step: 29, Loss: 0.11453481018543243\n",
      "Step: 30, Loss: 0.11393241584300995\n",
      "Step: 31, Loss: 0.11362060904502869\n",
      "Step: 32, Loss: 0.11172990500926971\n",
      "Step: 33, Loss: 0.11206085979938507\n",
      "Step: 34, Loss: 0.11190252006053925\n",
      "Step: 35, Loss: 0.11171774566173553\n",
      "Step: 36, Loss: 0.11109386384487152\n",
      "Step: 37, Loss: 0.11157987266778946\n",
      "Step: 38, Loss: 0.11067058891057968\n",
      "Step: 39, Loss: 0.10994356125593185\n",
      "Step: 40, Loss: 0.11027709394693375\n",
      "Step: 41, Loss: 0.11019787192344666\n",
      "Step: 42, Loss: 0.10893487930297852\n",
      "Step: 43, Loss: 0.1095554530620575\n",
      "Step: 44, Loss: 0.10851067304611206\n",
      "Step: 45, Loss: 0.10885407030582428\n",
      "Step: 46, Loss: 0.10967451333999634\n",
      "Step: 47, Loss: 0.10843564569950104\n",
      "Step: 48, Loss: 0.10871653258800507\n",
      "Step: 49, Loss: 0.10829883813858032\n",
      "Step: 50, Loss: 0.10859259963035583\n",
      "Step: 51, Loss: 0.10851188004016876\n",
      "Step: 52, Loss: 0.10761460661888123\n",
      "Step: 53, Loss: 0.10828375816345215\n",
      "Step: 54, Loss: 0.1077439934015274\n",
      "Step: 55, Loss: 0.1072721928358078\n",
      "Step: 56, Loss: 0.10763228684663773\n",
      "Step: 57, Loss: 0.10789258778095245\n",
      "Step: 58, Loss: 0.10710889101028442\n",
      "Step: 59, Loss: 0.10729508846998215\n",
      "Step: 60, Loss: 0.10744411498308182\n",
      "Step: 61, Loss: 0.10597725212574005\n",
      "Step: 62, Loss: 0.10715216398239136\n",
      "Step: 63, Loss: 0.10666988790035248\n",
      "Step: 64, Loss: 0.10725755989551544\n",
      "Step: 65, Loss: 0.10639872401952744\n",
      "Step: 66, Loss: 0.10639382898807526\n",
      "Step: 67, Loss: 0.10702093690633774\n",
      "Step: 68, Loss: 0.10676297545433044\n",
      "Step: 69, Loss: 0.10659495741128922\n",
      "Step: 70, Loss: 0.10636989027261734\n",
      "Step: 71, Loss: 0.10664045065641403\n",
      "Step: 72, Loss: 0.10675571113824844\n",
      "Step: 73, Loss: 0.10623657703399658\n",
      "Step: 74, Loss: 0.1061871200799942\n",
      "Step: 75, Loss: 0.10698224604129791\n",
      "Step: 76, Loss: 0.10683392733335495\n",
      "Step: 77, Loss: 0.10638561844825745\n",
      "Step: 78, Loss: 0.106157585978508\n",
      "Step: 79, Loss: 0.10596886277198792\n",
      "Step: 80, Loss: 0.10644467920064926\n",
      "Step: 81, Loss: 0.1066993921995163\n",
      "Step: 82, Loss: 0.10614261776208878\n",
      "Step: 83, Loss: 0.10581490397453308\n",
      "Step: 84, Loss: 0.10608974099159241\n",
      "Step: 85, Loss: 0.10603770613670349\n",
      "Step: 86, Loss: 0.10609593987464905\n",
      "Step: 87, Loss: 0.10615803301334381\n",
      "Step: 88, Loss: 0.10656037926673889\n",
      "Step: 89, Loss: 0.1063915267586708\n",
      "Step: 90, Loss: 0.10561786592006683\n",
      "Step: 91, Loss: 0.10626636445522308\n",
      "Step: 92, Loss: 0.10646828263998032\n",
      "Step: 93, Loss: 0.10613783448934555\n",
      "Step: 94, Loss: 0.10659213364124298\n",
      "Step: 95, Loss: 0.10562964528799057\n",
      "Step: 96, Loss: 0.10704783350229263\n",
      "Step: 97, Loss: 0.10623884946107864\n",
      "Step: 98, Loss: 0.10542101413011551\n",
      "Step: 99, Loss: 0.10609672218561172\n",
      "Step: 100, Loss: 0.10593865811824799\n",
      "Step: 101, Loss: 0.10599218308925629\n",
      "Step: 102, Loss: 0.10535793006420135\n",
      "Step: 103, Loss: 0.10553181916475296\n",
      "Step: 104, Loss: 0.10581380128860474\n",
      "Step: 105, Loss: 0.105608731508255\n",
      "Step: 106, Loss: 0.10607095062732697\n",
      "Step: 107, Loss: 0.10583402961492538\n",
      "Step: 108, Loss: 0.1058604046702385\n",
      "Step: 109, Loss: 0.10592079907655716\n",
      "Step: 110, Loss: 0.10584971308708191\n",
      "Step: 111, Loss: 0.10621292889118195\n",
      "Step: 112, Loss: 0.10631108283996582\n",
      "Step: 113, Loss: 0.10618260502815247\n",
      "Step: 114, Loss: 0.1059197187423706\n",
      "Step: 115, Loss: 0.10653799772262573\n",
      "Step: 116, Loss: 0.10586579144001007\n",
      "Step: 117, Loss: 0.10575570166110992\n",
      "Step: 118, Loss: 0.10615634173154831\n",
      "Step: 119, Loss: 0.10614819079637527\n",
      "Step: 120, Loss: 0.10571666806936264\n",
      "Step: 121, Loss: 0.10612305998802185\n",
      "Step: 122, Loss: 0.10629831254482269\n",
      "Step: 123, Loss: 0.10591939836740494\n",
      "Step: 124, Loss: 0.1058863177895546\n",
      "Step: 125, Loss: 0.10622890293598175\n",
      "Step: 126, Loss: 0.10612660646438599\n",
      "Step: 127, Loss: 0.10640387237071991\n",
      "Step: 128, Loss: 0.10605587065219879\n",
      "Step: 129, Loss: 0.10540878772735596\n",
      "Step: 130, Loss: 0.10592014342546463\n",
      "Step: 131, Loss: 0.10575660318136215\n",
      "Step: 132, Loss: 0.10590258985757828\n",
      "Step: 133, Loss: 0.10588710010051727\n",
      "Step: 134, Loss: 0.10594414919614792\n",
      "Step: 135, Loss: 0.10545443743467331\n",
      "Step: 136, Loss: 0.10533309727907181\n",
      "Step: 137, Loss: 0.10569330304861069\n",
      "Step: 138, Loss: 0.10574783384799957\n",
      "Step: 139, Loss: 0.10612079501152039\n",
      "Step: 140, Loss: 0.10558389127254486\n",
      "Step: 141, Loss: 0.1058020070195198\n",
      "Step: 142, Loss: 0.10636113584041595\n",
      "Step: 143, Loss: 0.10610230267047882\n",
      "Step: 144, Loss: 0.10565539449453354\n",
      "Step: 145, Loss: 0.10661385208368301\n",
      "Step: 146, Loss: 0.1061360239982605\n",
      "Step: 147, Loss: 0.10603545606136322\n",
      "Step: 148, Loss: 0.10553152859210968\n",
      "Step: 149, Loss: 0.10603545606136322\n",
      "Step: 150, Loss: 0.10626073181629181\n",
      "Step: 151, Loss: 0.10591947287321091\n",
      "Step: 152, Loss: 0.10601668059825897\n",
      "Step: 153, Loss: 0.10572098940610886\n",
      "Step: 154, Loss: 0.10583993047475815\n",
      "Step: 155, Loss: 0.10588453710079193\n",
      "Step: 156, Loss: 0.10584568977355957\n",
      "Step: 157, Loss: 0.10574518144130707\n",
      "Step: 158, Loss: 0.10605251789093018\n",
      "Step: 159, Loss: 0.1057642474770546\n",
      "Step: 160, Loss: 0.10604967176914215\n",
      "Step: 161, Loss: 0.10595881193876266\n",
      "Step: 162, Loss: 0.10571279376745224\n",
      "Step: 163, Loss: 0.10600557923316956\n",
      "Step: 164, Loss: 0.105595164000988\n",
      "Step: 165, Loss: 0.10565994679927826\n",
      "Step: 166, Loss: 0.10601841658353806\n",
      "Step: 167, Loss: 0.1054372787475586\n",
      "Step: 168, Loss: 0.1054818257689476\n",
      "Step: 169, Loss: 0.10626892000436783\n",
      "Step: 170, Loss: 0.10555853694677353\n",
      "Step: 171, Loss: 0.1058601513504982\n",
      "Step: 172, Loss: 0.10580919682979584\n",
      "Step: 173, Loss: 0.10632499307394028\n",
      "Step: 174, Loss: 0.10634315758943558\n",
      "Step: 175, Loss: 0.10635603964328766\n",
      "Step: 176, Loss: 0.10566432774066925\n",
      "Step: 177, Loss: 0.10602037608623505\n",
      "Step: 178, Loss: 0.1058899536728859\n",
      "Step: 179, Loss: 0.10575242340564728\n",
      "Step: 180, Loss: 0.1054854616522789\n",
      "Step: 181, Loss: 0.10582371801137924\n",
      "Step: 182, Loss: 0.10546229034662247\n",
      "Step: 183, Loss: 0.10570693761110306\n",
      "Step: 184, Loss: 0.10598257929086685\n",
      "Step: 185, Loss: 0.10587190836668015\n",
      "Step: 186, Loss: 0.10606666654348373\n",
      "Step: 187, Loss: 0.10552429407835007\n",
      "Step: 188, Loss: 0.10614687204360962\n",
      "Step: 189, Loss: 0.10547397285699844\n",
      "Step: 190, Loss: 0.10578104853630066\n",
      "Step: 191, Loss: 0.105951689183712\n",
      "Step: 192, Loss: 0.10603411495685577\n",
      "Step: 193, Loss: 0.10629133135080338\n",
      "Step: 194, Loss: 0.10585671663284302\n",
      "Step: 195, Loss: 0.10640577226877213\n",
      "Step: 196, Loss: 0.10618602484464645\n",
      "Step: 197, Loss: 0.10601828992366791\n",
      "Step: 198, Loss: 0.1058686226606369\n",
      "Step: 199, Loss: 0.10561679303646088\n",
      "Step: 200, Loss: 0.10645787417888641\n",
      "Step: 201, Loss: 0.1056002750992775\n",
      "Step: 202, Loss: 0.10583612322807312\n",
      "Step: 203, Loss: 0.10566303879022598\n",
      "Step: 204, Loss: 0.10586001724004745\n",
      "Step: 205, Loss: 0.10610316693782806\n",
      "Step: 206, Loss: 0.10540065169334412\n",
      "Step: 207, Loss: 0.10635677725076675\n",
      "Step: 208, Loss: 0.10595086216926575\n",
      "Step: 209, Loss: 0.10611274838447571\n",
      "Step: 210, Loss: 0.105647973716259\n",
      "Step: 211, Loss: 0.10593201965093613\n",
      "Step: 212, Loss: 0.10594820976257324\n",
      "Step: 213, Loss: 0.10605663806200027\n",
      "Step: 214, Loss: 0.1057763546705246\n",
      "Step: 215, Loss: 0.10614276677370071\n",
      "Step: 216, Loss: 0.1062324270606041\n",
      "Step: 217, Loss: 0.10580477863550186\n",
      "Step: 218, Loss: 0.10572817176580429\n",
      "Step: 219, Loss: 0.10570617020130157\n",
      "Step: 220, Loss: 0.105721116065979\n",
      "Step: 221, Loss: 0.1055499017238617\n",
      "Step: 222, Loss: 0.10557836294174194\n",
      "Step: 223, Loss: 0.10586606711149216\n",
      "Step: 224, Loss: 0.10576866567134857\n",
      "Step: 225, Loss: 0.10610536485910416\n",
      "Step: 226, Loss: 0.1050911545753479\n",
      "Step: 227, Loss: 0.10616213828325272\n",
      "Step: 228, Loss: 0.10571790486574173\n",
      "Step: 229, Loss: 0.10533314943313599\n",
      "Step: 230, Loss: 0.10580118000507355\n",
      "Step: 231, Loss: 0.10610654205083847\n",
      "Step: 232, Loss: 0.10588061809539795\n",
      "Step: 233, Loss: 0.1062929704785347\n",
      "Step: 234, Loss: 0.10524336993694305\n",
      "Step: 235, Loss: 0.10584373027086258\n",
      "Step: 236, Loss: 0.10536724328994751\n",
      "Step: 237, Loss: 0.1056659072637558\n",
      "Step: 238, Loss: 0.10546232759952545\n",
      "Step: 239, Loss: 0.10538804531097412\n",
      "Step: 240, Loss: 0.10622468590736389\n",
      "Step: 241, Loss: 0.1061653271317482\n",
      "Step: 242, Loss: 0.10577069967985153\n",
      "Step: 243, Loss: 0.10621178895235062\n",
      "Step: 244, Loss: 0.10565346479415894\n",
      "Step: 245, Loss: 0.10606750845909119\n",
      "Step: 246, Loss: 0.10584373772144318\n",
      "Step: 247, Loss: 0.10582937300205231\n",
      "Step: 248, Loss: 0.10576269030570984\n",
      "Step: 249, Loss: 0.10591326653957367\n",
      "Step: 250, Loss: 0.10556306689977646\n",
      "Step: 251, Loss: 0.10606586188077927\n",
      "Step: 252, Loss: 0.10557445883750916\n",
      "Step: 253, Loss: 0.10573039203882217\n",
      "Step: 254, Loss: 0.10592103004455566\n",
      "Step: 255, Loss: 0.10596298426389694\n",
      "Step: 256, Loss: 0.10594702512025833\n",
      "Step: 257, Loss: 0.10564486682415009\n",
      "Step: 258, Loss: 0.1056983545422554\n",
      "Step: 259, Loss: 0.10567539930343628\n",
      "Step: 260, Loss: 0.10564298927783966\n",
      "Step: 261, Loss: 0.10608509927988052\n",
      "Step: 262, Loss: 0.10647118091583252\n",
      "Step: 263, Loss: 0.10623661428689957\n",
      "Step: 264, Loss: 0.10586535930633545\n",
      "Step: 265, Loss: 0.10604600608348846\n",
      "Step: 266, Loss: 0.10631340742111206\n",
      "Step: 267, Loss: 0.10585417598485947\n",
      "Step: 268, Loss: 0.10608478635549545\n",
      "Step: 269, Loss: 0.10579143464565277\n",
      "Step: 270, Loss: 0.10601819306612015\n",
      "Step: 271, Loss: 0.10575434565544128\n",
      "Step: 272, Loss: 0.10583419352769852\n",
      "Step: 273, Loss: 0.10618428885936737\n",
      "Step: 274, Loss: 0.1061118096113205\n",
      "Step: 275, Loss: 0.10622246563434601\n",
      "Step: 276, Loss: 0.10562764108181\n",
      "Step: 277, Loss: 0.10550498217344284\n",
      "Step: 278, Loss: 0.10569270700216293\n",
      "Step: 279, Loss: 0.10561923682689667\n",
      "Step: 280, Loss: 0.10563664883375168\n",
      "Step: 281, Loss: 0.1058344691991806\n",
      "Step: 282, Loss: 0.10575472563505173\n",
      "Step: 283, Loss: 0.10540357232093811\n",
      "Step: 284, Loss: 0.10597490519285202\n",
      "Step: 285, Loss: 0.10577677935361862\n",
      "Step: 286, Loss: 0.105979785323143\n",
      "Step: 287, Loss: 0.1057136282324791\n",
      "Step: 288, Loss: 0.10649228096008301\n",
      "Step: 289, Loss: 0.105975441634655\n",
      "Step: 290, Loss: 0.10568977892398834\n",
      "Step: 291, Loss: 0.10606879740953445\n",
      "Step: 292, Loss: 0.10589390248060226\n",
      "Step: 293, Loss: 0.10537730157375336\n",
      "Step: 294, Loss: 0.10601426661014557\n",
      "Step: 295, Loss: 0.10566028207540512\n",
      "Step: 296, Loss: 0.10648442059755325\n",
      "Step: 297, Loss: 0.10547313094139099\n",
      "Step: 298, Loss: 0.10576625168323517\n",
      "Step: 299, Loss: 0.10561565309762955\n",
      "Step: 300, Loss: 0.10551703721284866\n",
      "Step: 301, Loss: 0.10603129863739014\n",
      "Step: 302, Loss: 0.10639118403196335\n",
      "Step: 303, Loss: 0.10579069703817368\n",
      "Step: 304, Loss: 0.10592462122440338\n",
      "Step: 305, Loss: 0.10637348145246506\n",
      "Step: 306, Loss: 0.10561032593250275\n",
      "Step: 307, Loss: 0.10618915408849716\n",
      "Step: 308, Loss: 0.10568282008171082\n",
      "Step: 309, Loss: 0.10596642643213272\n",
      "Step: 310, Loss: 0.10623233765363693\n",
      "Step: 311, Loss: 0.10548412799835205\n",
      "Step: 312, Loss: 0.1059197410941124\n",
      "Step: 313, Loss: 0.10562451183795929\n",
      "Step: 314, Loss: 0.10569580644369125\n",
      "Step: 315, Loss: 0.10618216544389725\n",
      "Step: 316, Loss: 0.1056312620639801\n",
      "Step: 317, Loss: 0.10622598230838776\n",
      "Step: 318, Loss: 0.10626190900802612\n",
      "Step: 319, Loss: 0.10532302409410477\n",
      "Step: 320, Loss: 0.10584412515163422\n",
      "Step: 321, Loss: 0.1058005541563034\n",
      "Step: 322, Loss: 0.10596989095211029\n",
      "Step: 323, Loss: 0.1062099039554596\n",
      "Step: 324, Loss: 0.10543835163116455\n",
      "Step: 325, Loss: 0.1057351678609848\n",
      "Step: 326, Loss: 0.10590886324644089\n",
      "Step: 327, Loss: 0.10591018944978714\n",
      "Step: 328, Loss: 0.10603415966033936\n",
      "Step: 329, Loss: 0.10563568770885468\n",
      "Step: 330, Loss: 0.10558059811592102\n",
      "Step: 331, Loss: 0.10587189346551895\n",
      "Step: 332, Loss: 0.10585951805114746\n",
      "Step: 333, Loss: 0.10570593923330307\n",
      "Step: 334, Loss: 0.10607020556926727\n",
      "Step: 335, Loss: 0.1061912402510643\n",
      "Step: 336, Loss: 0.10575517266988754\n",
      "Step: 337, Loss: 0.10594737529754639\n",
      "Step: 338, Loss: 0.10640514642000198\n",
      "Step: 339, Loss: 0.10556446015834808\n",
      "Step: 340, Loss: 0.10557125508785248\n",
      "Step: 341, Loss: 0.10605373233556747\n",
      "Step: 342, Loss: 0.105318583548069\n",
      "Step: 343, Loss: 0.1057262048125267\n",
      "Step: 344, Loss: 0.10574070364236832\n",
      "Step: 345, Loss: 0.10603690147399902\n",
      "Step: 346, Loss: 0.10622896999120712\n",
      "Step: 347, Loss: 0.10608824342489243\n",
      "Step: 348, Loss: 0.10597962886095047\n",
      "Step: 349, Loss: 0.105524942278862\n",
      "Step: 350, Loss: 0.10569269210100174\n",
      "Step: 351, Loss: 0.10591962933540344\n",
      "Step: 352, Loss: 0.10569941997528076\n",
      "Step: 353, Loss: 0.10584798455238342\n",
      "Step: 354, Loss: 0.10640081018209457\n",
      "Step: 355, Loss: 0.10614024102687836\n",
      "Step: 356, Loss: 0.106082484126091\n",
      "Step: 357, Loss: 0.10562672466039658\n",
      "Step: 358, Loss: 0.10575392842292786\n",
      "Step: 359, Loss: 0.10576452314853668\n",
      "Step: 360, Loss: 0.10564906150102615\n",
      "Step: 361, Loss: 0.10615894198417664\n",
      "Step: 362, Loss: 0.10597977787256241\n",
      "Step: 363, Loss: 0.10651450604200363\n",
      "Step: 364, Loss: 0.10589571297168732\n",
      "Step: 365, Loss: 0.10619193315505981\n",
      "Step: 366, Loss: 0.10570782423019409\n",
      "Step: 367, Loss: 0.10574633628129959\n",
      "Step: 368, Loss: 0.10616142302751541\n",
      "Step: 369, Loss: 0.10617578774690628\n",
      "Step: 370, Loss: 0.10588566213846207\n",
      "Step: 371, Loss: 0.10591859370470047\n",
      "Step: 372, Loss: 0.10532088577747345\n",
      "Step: 373, Loss: 0.10651928186416626\n",
      "Step: 374, Loss: 0.10578323155641556\n",
      "Step: 375, Loss: 0.1052708700299263\n",
      "Step: 376, Loss: 0.10579405725002289\n",
      "Step: 377, Loss: 0.10582886636257172\n",
      "Step: 378, Loss: 0.10580258071422577\n",
      "Step: 379, Loss: 0.1060531735420227\n",
      "Step: 380, Loss: 0.10611450672149658\n",
      "Step: 381, Loss: 0.10563724488019943\n",
      "Step: 382, Loss: 0.10612595081329346\n",
      "Step: 383, Loss: 0.10591104626655579\n",
      "Step: 384, Loss: 0.10600604116916656\n",
      "Step: 385, Loss: 0.10608575493097305\n",
      "Step: 386, Loss: 0.10618065297603607\n",
      "Step: 387, Loss: 0.10569296032190323\n",
      "Step: 388, Loss: 0.10627391934394836\n",
      "Step: 389, Loss: 0.10600399971008301\n",
      "Step: 390, Loss: 0.10615542531013489\n",
      "Step: 391, Loss: 0.1054450273513794\n",
      "Step: 392, Loss: 0.10587283223867416\n",
      "Step: 393, Loss: 0.10614797472953796\n",
      "Step: 394, Loss: 0.10590964555740356\n",
      "Step: 395, Loss: 0.10633067786693573\n",
      "Step: 396, Loss: 0.10547717660665512\n",
      "Step: 397, Loss: 0.1056155413389206\n",
      "Step: 398, Loss: 0.10585883259773254\n",
      "Step: 399, Loss: 0.10555992275476456\n",
      "Step: 400, Loss: 0.10576657205820084\n",
      "Step: 401, Loss: 0.1060304343700409\n",
      "Step: 402, Loss: 0.10598940402269363\n",
      "Step: 403, Loss: 0.10597246140241623\n",
      "Step: 404, Loss: 0.1060713529586792\n",
      "Step: 405, Loss: 0.1055339053273201\n",
      "Step: 406, Loss: 0.10555677860975266\n",
      "Step: 407, Loss: 0.10619013756513596\n",
      "Step: 408, Loss: 0.1057218387722969\n",
      "Step: 409, Loss: 0.10545557737350464\n",
      "Step: 410, Loss: 0.10620104521512985\n",
      "Step: 411, Loss: 0.10589249432086945\n",
      "Step: 412, Loss: 0.10596751421689987\n",
      "Step: 413, Loss: 0.10594799369573593\n",
      "Step: 414, Loss: 0.10592715442180634\n",
      "Step: 415, Loss: 0.1057301014661789\n",
      "Step: 416, Loss: 0.10562224686145782\n",
      "Step: 417, Loss: 0.10605086386203766\n",
      "Step: 418, Loss: 0.10590092837810516\n",
      "Step: 419, Loss: 0.10555686801671982\n",
      "Step: 420, Loss: 0.10591662675142288\n",
      "Step: 421, Loss: 0.10627775639295578\n",
      "Step: 422, Loss: 0.10547267645597458\n",
      "Step: 423, Loss: 0.10606853663921356\n",
      "Step: 424, Loss: 0.10589076578617096\n",
      "Step: 425, Loss: 0.10568150877952576\n",
      "Step: 426, Loss: 0.1055736094713211\n",
      "Step: 427, Loss: 0.10580705851316452\n",
      "Step: 428, Loss: 0.10593989491462708\n",
      "Step: 429, Loss: 0.10600123554468155\n",
      "Step: 430, Loss: 0.1057540699839592\n",
      "Step: 431, Loss: 0.10584671050310135\n",
      "Step: 432, Loss: 0.10571914166212082\n",
      "Step: 433, Loss: 0.1054830551147461\n",
      "Step: 434, Loss: 0.10622115433216095\n",
      "Step: 435, Loss: 0.10592374950647354\n",
      "Step: 436, Loss: 0.10560039430856705\n",
      "Step: 437, Loss: 0.10600129514932632\n",
      "Step: 438, Loss: 0.1060231626033783\n",
      "Step: 439, Loss: 0.10609849542379379\n",
      "Step: 440, Loss: 0.10620773583650589\n",
      "Step: 441, Loss: 0.10568803548812866\n",
      "Step: 442, Loss: 0.10586025565862656\n",
      "Step: 443, Loss: 0.10615714639425278\n",
      "Step: 444, Loss: 0.10559744387865067\n",
      "Step: 445, Loss: 0.10538362711668015\n",
      "Step: 446, Loss: 0.10600057989358902\n",
      "Step: 447, Loss: 0.1061023399233818\n",
      "Step: 448, Loss: 0.10583588480949402\n",
      "Step: 449, Loss: 0.10579365491867065\n",
      "Step: 450, Loss: 0.10630572587251663\n",
      "Step: 451, Loss: 0.10578998178243637\n",
      "Step: 452, Loss: 0.10573897510766983\n",
      "Step: 453, Loss: 0.10556570440530777\n",
      "Step: 454, Loss: 0.10518864542245865\n",
      "Step: 455, Loss: 0.10636534541845322\n",
      "Step: 456, Loss: 0.10610520094633102\n",
      "Step: 457, Loss: 0.10604844987392426\n",
      "Step: 458, Loss: 0.10582630336284637\n",
      "Step: 459, Loss: 0.10585329681634903\n",
      "Step: 460, Loss: 0.10619626194238663\n",
      "Step: 461, Loss: 0.1060766726732254\n",
      "Step: 462, Loss: 0.1060314029455185\n",
      "Step: 463, Loss: 0.10510960221290588\n",
      "Step: 464, Loss: 0.10539772361516953\n",
      "Step: 465, Loss: 0.10526750236749649\n",
      "Step: 466, Loss: 0.10525849461555481\n",
      "Step: 467, Loss: 0.10615083575248718\n",
      "Step: 468, Loss: 0.10566326975822449\n",
      "Step: 469, Loss: 0.10591766238212585\n",
      "Step: 470, Loss: 0.10630857199430466\n",
      "Step: 471, Loss: 0.10610446333885193\n",
      "Step: 472, Loss: 0.10546956956386566\n",
      "Step: 473, Loss: 0.10597596317529678\n",
      "Step: 474, Loss: 0.10565410554409027\n",
      "Step: 475, Loss: 0.10597512125968933\n",
      "Step: 476, Loss: 0.10570717602968216\n",
      "Step: 477, Loss: 0.10542726516723633\n",
      "Step: 478, Loss: 0.10589227825403214\n",
      "Step: 479, Loss: 0.10562436282634735\n",
      "Step: 480, Loss: 0.1058872863650322\n",
      "Step: 481, Loss: 0.10601522028446198\n",
      "Step: 482, Loss: 0.10617144405841827\n",
      "Step: 483, Loss: 0.10593409091234207\n",
      "Step: 484, Loss: 0.10520603507757187\n",
      "Step: 485, Loss: 0.1060136929154396\n",
      "Step: 486, Loss: 0.10570335388183594\n",
      "Step: 487, Loss: 0.10582048445940018\n",
      "Step: 488, Loss: 0.10521691292524338\n",
      "Step: 489, Loss: 0.10576754063367844\n",
      "Step: 490, Loss: 0.10582485049962997\n",
      "Step: 491, Loss: 0.10620655119419098\n",
      "Step: 492, Loss: 0.10609521716833115\n",
      "Step: 493, Loss: 0.10562928020954132\n",
      "Step: 494, Loss: 0.1057293638586998\n",
      "Step: 495, Loss: 0.10584703087806702\n",
      "Step: 496, Loss: 0.10552127659320831\n",
      "Step: 497, Loss: 0.10579900443553925\n",
      "Step: 498, Loss: 0.10577361285686493\n",
      "Step: 499, Loss: 0.10575755685567856\n",
      "Step: 500, Loss: 0.1056632325053215\n",
      "Step: 501, Loss: 0.10594161599874496\n",
      "Step: 502, Loss: 0.10586672276258469\n",
      "Step: 503, Loss: 0.10554146766662598\n",
      "Step: 504, Loss: 0.10609928518533707\n",
      "Step: 505, Loss: 0.10570955276489258\n",
      "Step: 506, Loss: 0.10595902800559998\n",
      "Step: 507, Loss: 0.1056891456246376\n",
      "Step: 508, Loss: 0.1058470606803894\n",
      "Step: 509, Loss: 0.10557714849710464\n",
      "Step: 510, Loss: 0.10589898377656937\n",
      "Step: 511, Loss: 0.10562679916620255\n",
      "Step: 512, Loss: 0.10552575439214706\n",
      "Step: 513, Loss: 0.10627526789903641\n",
      "Step: 514, Loss: 0.10575389117002487\n",
      "Step: 515, Loss: 0.10554078221321106\n",
      "Step: 516, Loss: 0.1060517281293869\n",
      "Step: 517, Loss: 0.10611210763454437\n",
      "Step: 518, Loss: 0.10517313331365585\n",
      "Step: 519, Loss: 0.10558987408876419\n",
      "Step: 520, Loss: 0.1064421534538269\n",
      "Step: 521, Loss: 0.10543440282344818\n",
      "Step: 522, Loss: 0.10554327815771103\n",
      "Step: 523, Loss: 0.1061200350522995\n",
      "Step: 524, Loss: 0.10582214593887329\n",
      "Step: 525, Loss: 0.10560858994722366\n",
      "Step: 526, Loss: 0.10566266626119614\n",
      "Step: 527, Loss: 0.10572180896997452\n",
      "Step: 528, Loss: 0.10550851374864578\n",
      "Step: 529, Loss: 0.10544588416814804\n",
      "Step: 530, Loss: 0.1057472750544548\n",
      "Step: 531, Loss: 0.10511564463376999\n",
      "Step: 532, Loss: 0.1054723933339119\n",
      "Step: 533, Loss: 0.1052960678935051\n",
      "Step: 534, Loss: 0.10575930774211884\n",
      "Step: 535, Loss: 0.10560562461614609\n",
      "Step: 536, Loss: 0.10593826323747635\n",
      "Step: 537, Loss: 0.10570833832025528\n",
      "Step: 538, Loss: 0.10613001883029938\n",
      "Step: 539, Loss: 0.10532858967781067\n",
      "Step: 540, Loss: 0.10601021349430084\n",
      "Step: 541, Loss: 0.10594595223665237\n",
      "Step: 542, Loss: 0.10535939037799835\n",
      "Step: 543, Loss: 0.10646598786115646\n",
      "Step: 544, Loss: 0.10544021427631378\n",
      "Step: 545, Loss: 0.10518006980419159\n",
      "Step: 546, Loss: 0.10593071579933167\n",
      "Step: 547, Loss: 0.10633864253759384\n",
      "Step: 548, Loss: 0.10561444610357285\n",
      "Step: 549, Loss: 0.10513089597225189\n",
      "Step: 550, Loss: 0.1059785708785057\n",
      "Step: 551, Loss: 0.10577487200498581\n",
      "Step: 552, Loss: 0.10595610737800598\n",
      "Step: 553, Loss: 0.1055983155965805\n",
      "Step: 554, Loss: 0.10511177033185959\n",
      "Step: 555, Loss: 0.10531239211559296\n",
      "Step: 556, Loss: 0.10597114264965057\n",
      "Step: 557, Loss: 0.10548105835914612\n",
      "Step: 558, Loss: 0.10520437359809875\n",
      "Step: 559, Loss: 0.10603982955217361\n",
      "Step: 560, Loss: 0.10619478672742844\n",
      "Step: 561, Loss: 0.10561273992061615\n",
      "Step: 562, Loss: 0.10551630705595016\n",
      "Step: 563, Loss: 0.10599500685930252\n",
      "Step: 564, Loss: 0.10589524358510971\n",
      "Step: 565, Loss: 0.10590478777885437\n",
      "Step: 566, Loss: 0.10565220564603806\n",
      "Step: 567, Loss: 0.10608360171318054\n",
      "Step: 568, Loss: 0.10530762374401093\n",
      "Step: 569, Loss: 0.10534662753343582\n",
      "Step: 570, Loss: 0.10547424852848053\n",
      "Step: 571, Loss: 0.10532361268997192\n",
      "Step: 572, Loss: 0.10580507665872574\n",
      "Step: 573, Loss: 0.106132872402668\n",
      "Step: 574, Loss: 0.1056477278470993\n",
      "Step: 575, Loss: 0.10610576719045639\n",
      "Step: 576, Loss: 0.10615681856870651\n",
      "Step: 577, Loss: 0.10582172125577927\n",
      "Step: 578, Loss: 0.1059127002954483\n",
      "Step: 579, Loss: 0.1058809757232666\n",
      "Step: 580, Loss: 0.10674452036619186\n",
      "Step: 581, Loss: 0.10567915439605713\n",
      "Step: 582, Loss: 0.10585092008113861\n",
      "Step: 583, Loss: 0.10534555464982986\n",
      "Step: 584, Loss: 0.10577557981014252\n",
      "Step: 585, Loss: 0.10540898889303207\n",
      "Step: 586, Loss: 0.10568509995937347\n",
      "Step: 587, Loss: 0.10629294067621231\n",
      "Step: 588, Loss: 0.10596217960119247\n",
      "Step: 589, Loss: 0.10577565431594849\n",
      "Step: 590, Loss: 0.10585420578718185\n",
      "Step: 591, Loss: 0.10561078041791916\n",
      "Step: 592, Loss: 0.10607614368200302\n",
      "Step: 593, Loss: 0.10658426582813263\n",
      "Step: 594, Loss: 0.10573487728834152\n",
      "Step: 595, Loss: 0.10549721121788025\n",
      "Step: 596, Loss: 0.10596752911806107\n",
      "Step: 597, Loss: 0.10557006299495697\n",
      "Step: 598, Loss: 0.10578235238790512\n",
      "Step: 599, Loss: 0.10615944117307663\n",
      "Final loss:  0.10615944\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/trunc_normal_features/tnf_twcrps_mean11.0_std2.0_constant0.019999999552965164_epochs600_folds_2_3.pkl\n"
     ]
    }
   ],
   "source": [
    "model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    parameter_names,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_constant = chain_function_constant,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Final loss:  0.10335002\n",
      "Final loss:  0.101744905\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using given parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 0.10101192444562912\n",
      "Step: 1, Loss: 0.1045975387096405\n",
      "Step: 2, Loss: 0.10153059661388397\n",
      "Step: 3, Loss: 0.10202322900295258\n",
      "Step: 4, Loss: 0.1033298522233963\n",
      "Step: 5, Loss: 0.10229266434907913\n",
      "Step: 6, Loss: 0.1007765606045723\n",
      "Step: 7, Loss: 0.10148461908102036\n",
      "Step: 8, Loss: 0.10205074399709702\n",
      "Step: 9, Loss: 0.10181190073490143\n",
      "Step: 10, Loss: 0.10107484459877014\n",
      "Step: 11, Loss: 0.10078710317611694\n",
      "Step: 12, Loss: 0.1010366752743721\n",
      "Step: 13, Loss: 0.10091517120599747\n",
      "Step: 14, Loss: 0.1011858657002449\n",
      "Step: 15, Loss: 0.10120128095149994\n",
      "Step: 16, Loss: 0.10065849870443344\n",
      "Step: 17, Loss: 0.10093311220407486\n",
      "Step: 18, Loss: 0.10144681483507156\n",
      "Step: 19, Loss: 0.10081422328948975\n",
      "Step: 20, Loss: 0.100248321890831\n",
      "Step: 21, Loss: 0.10068947076797485\n",
      "Step: 22, Loss: 0.10042747110128403\n",
      "Step: 23, Loss: 0.10088712722063065\n",
      "Step: 24, Loss: 0.1005655974149704\n",
      "Step: 25, Loss: 0.10123177617788315\n",
      "Step: 26, Loss: 0.10017429292201996\n",
      "Step: 27, Loss: 0.10028838366270065\n",
      "Step: 28, Loss: 0.10079685598611832\n",
      "Step: 29, Loss: 0.1003103256225586\n",
      "Step: 30, Loss: 0.10113375633955002\n",
      "Step: 31, Loss: 0.10008753091096878\n",
      "Step: 32, Loss: 0.10060267895460129\n",
      "Step: 33, Loss: 0.09993112087249756\n",
      "Step: 34, Loss: 0.10006285458803177\n",
      "Step: 35, Loss: 0.10051048547029495\n",
      "Step: 36, Loss: 0.10016465932130814\n",
      "Step: 37, Loss: 0.1008554995059967\n",
      "Step: 38, Loss: 0.10040871053934097\n",
      "Step: 39, Loss: 0.10027137398719788\n",
      "Step: 40, Loss: 0.10040444135665894\n",
      "Step: 41, Loss: 0.10068489611148834\n",
      "Step: 42, Loss: 0.10042519867420197\n",
      "Step: 43, Loss: 0.1005309671163559\n",
      "Step: 44, Loss: 0.100456103682518\n",
      "Step: 45, Loss: 0.10045760124921799\n",
      "Step: 46, Loss: 0.1009252518415451\n",
      "Step: 47, Loss: 0.10039106011390686\n",
      "Step: 48, Loss: 0.10039254277944565\n",
      "Step: 49, Loss: 0.10032002627849579\n",
      "Step: 50, Loss: 0.10016867518424988\n",
      "Step: 51, Loss: 0.10017027705907822\n",
      "Step: 52, Loss: 0.10000988841056824\n",
      "Step: 53, Loss: 0.10024469345808029\n",
      "Step: 54, Loss: 0.09989137947559357\n",
      "Step: 55, Loss: 0.10024677962064743\n",
      "Step: 56, Loss: 0.0998087152838707\n",
      "Step: 57, Loss: 0.10010010749101639\n",
      "Step: 58, Loss: 0.10027453303337097\n",
      "Step: 59, Loss: 0.10038748383522034\n",
      "Step: 60, Loss: 0.09995193034410477\n",
      "Step: 61, Loss: 0.10020797699689865\n",
      "Step: 62, Loss: 0.10054070502519608\n",
      "Step: 63, Loss: 0.09990429133176804\n",
      "Step: 64, Loss: 0.10013564676046371\n",
      "Step: 65, Loss: 0.10048423707485199\n",
      "Step: 66, Loss: 0.10039737820625305\n",
      "Step: 67, Loss: 0.10024860501289368\n",
      "Step: 68, Loss: 0.10020972788333893\n",
      "Step: 69, Loss: 0.1000327616930008\n",
      "Step: 70, Loss: 0.10040455311536789\n",
      "Step: 71, Loss: 0.10010264068841934\n",
      "Step: 72, Loss: 0.100120410323143\n",
      "Step: 73, Loss: 0.10007312148809433\n",
      "Step: 74, Loss: 0.09965398907661438\n",
      "Step: 75, Loss: 0.10011614859104156\n",
      "Step: 76, Loss: 0.1004413589835167\n",
      "Step: 77, Loss: 0.10010948032140732\n",
      "Step: 78, Loss: 0.10064411908388138\n",
      "Step: 79, Loss: 0.10035866498947144\n",
      "Step: 80, Loss: 0.10028905421495438\n",
      "Step: 81, Loss: 0.10032825917005539\n",
      "Step: 82, Loss: 0.09956841170787811\n",
      "Step: 83, Loss: 0.1001385971903801\n",
      "Step: 84, Loss: 0.10013581067323685\n",
      "Step: 85, Loss: 0.10000420361757278\n",
      "Step: 86, Loss: 0.09996264427900314\n",
      "Step: 87, Loss: 0.10030550509691238\n",
      "Step: 88, Loss: 0.09974067658185959\n",
      "Step: 89, Loss: 0.10065500438213348\n",
      "Step: 90, Loss: 0.10021546483039856\n",
      "Step: 91, Loss: 0.10012366622686386\n",
      "Step: 92, Loss: 0.1002422347664833\n",
      "Step: 93, Loss: 0.09985672682523727\n",
      "Step: 94, Loss: 0.10030867159366608\n",
      "Step: 95, Loss: 0.09985712915658951\n",
      "Step: 96, Loss: 0.10033926367759705\n",
      "Step: 97, Loss: 0.10037868469953537\n",
      "Step: 98, Loss: 0.099961057305336\n",
      "Step: 99, Loss: 0.10020636022090912\n",
      "Step: 100, Loss: 0.09966293722391129\n",
      "Step: 101, Loss: 0.10012321919202805\n",
      "Step: 102, Loss: 0.10028219223022461\n",
      "Step: 103, Loss: 0.10012897104024887\n",
      "Step: 104, Loss: 0.10037192702293396\n",
      "Step: 105, Loss: 0.10018285363912582\n",
      "Step: 106, Loss: 0.10033261775970459\n",
      "Step: 107, Loss: 0.10020512342453003\n",
      "Step: 108, Loss: 0.09996239095926285\n",
      "Step: 109, Loss: 0.09985598176717758\n",
      "Step: 110, Loss: 0.10007429122924805\n",
      "Step: 111, Loss: 0.10003470629453659\n",
      "Step: 112, Loss: 0.10045289248228073\n",
      "Step: 113, Loss: 0.1001242995262146\n",
      "Step: 114, Loss: 0.1000967025756836\n",
      "Step: 115, Loss: 0.10052905231714249\n",
      "Step: 116, Loss: 0.10022064298391342\n",
      "Step: 117, Loss: 0.10005631297826767\n",
      "Step: 118, Loss: 0.1000601276755333\n",
      "Step: 119, Loss: 0.09992896765470505\n",
      "Step: 120, Loss: 0.10031336545944214\n",
      "Step: 121, Loss: 0.10004186630249023\n",
      "Step: 122, Loss: 0.09985711425542831\n",
      "Step: 123, Loss: 0.09995575249195099\n",
      "Step: 124, Loss: 0.0998339056968689\n",
      "Step: 125, Loss: 0.10030561685562134\n",
      "Step: 126, Loss: 0.10000582784414291\n",
      "Step: 127, Loss: 0.1001092940568924\n",
      "Step: 128, Loss: 0.09972014278173447\n",
      "Step: 129, Loss: 0.09994105994701385\n",
      "Step: 130, Loss: 0.1002492755651474\n",
      "Step: 131, Loss: 0.0996551588177681\n",
      "Step: 132, Loss: 0.10011706501245499\n",
      "Step: 133, Loss: 0.10062992572784424\n",
      "Step: 134, Loss: 0.10021651536226273\n",
      "Step: 135, Loss: 0.10009622573852539\n",
      "Step: 136, Loss: 0.09982169419527054\n",
      "Step: 137, Loss: 0.1003158912062645\n",
      "Step: 138, Loss: 0.10016825795173645\n",
      "Step: 139, Loss: 0.10030484199523926\n",
      "Step: 140, Loss: 0.09976093471050262\n",
      "Step: 141, Loss: 0.10006352514028549\n",
      "Step: 142, Loss: 0.1001281887292862\n",
      "Step: 143, Loss: 0.0998583659529686\n",
      "Step: 144, Loss: 0.09987346082925797\n",
      "Step: 145, Loss: 0.09980994462966919\n",
      "Step: 146, Loss: 0.0999317392706871\n",
      "Step: 147, Loss: 0.09982917457818985\n",
      "Step: 148, Loss: 0.09982779622077942\n",
      "Step: 149, Loss: 0.10061391443014145\n",
      "Step: 150, Loss: 0.09979795664548874\n",
      "Step: 151, Loss: 0.09970931708812714\n",
      "Step: 152, Loss: 0.09963594377040863\n",
      "Step: 153, Loss: 0.09963522106409073\n",
      "Step: 154, Loss: 0.10000116378068924\n",
      "Step: 155, Loss: 0.09996750950813293\n",
      "Step: 156, Loss: 0.09971670806407928\n",
      "Step: 157, Loss: 0.10017547011375427\n",
      "Step: 158, Loss: 0.10012545436620712\n",
      "Step: 159, Loss: 0.1003195270895958\n",
      "Step: 160, Loss: 0.09978775680065155\n",
      "Step: 161, Loss: 0.0996885895729065\n",
      "Step: 162, Loss: 0.09987661987543106\n",
      "Step: 163, Loss: 0.09971199184656143\n",
      "Step: 164, Loss: 0.09981833398342133\n",
      "Step: 165, Loss: 0.09961768984794617\n",
      "Step: 166, Loss: 0.099788136780262\n",
      "Step: 167, Loss: 0.09958240389823914\n",
      "Step: 168, Loss: 0.0994737520813942\n",
      "Step: 169, Loss: 0.09954477846622467\n",
      "Step: 170, Loss: 0.09974025934934616\n",
      "Step: 171, Loss: 0.09960589557886124\n",
      "Step: 172, Loss: 0.09965706616640091\n",
      "Step: 173, Loss: 0.10000422596931458\n",
      "Step: 174, Loss: 0.09968196600675583\n",
      "Step: 175, Loss: 0.09961492568254471\n",
      "Step: 176, Loss: 0.09962505102157593\n",
      "Step: 177, Loss: 0.09971467405557632\n",
      "Step: 178, Loss: 0.09962061792612076\n",
      "Step: 179, Loss: 0.10008540749549866\n",
      "Step: 180, Loss: 0.09984629601240158\n",
      "Step: 181, Loss: 0.09999776631593704\n",
      "Step: 182, Loss: 0.09971991181373596\n",
      "Step: 183, Loss: 0.09985431283712387\n",
      "Step: 184, Loss: 0.09943965077400208\n",
      "Step: 185, Loss: 0.09947750717401505\n",
      "Step: 186, Loss: 0.09945524483919144\n",
      "Step: 187, Loss: 0.09984668344259262\n",
      "Step: 188, Loss: 0.09993193298578262\n",
      "Step: 189, Loss: 0.10006702691316605\n",
      "Step: 190, Loss: 0.09995385259389877\n",
      "Step: 191, Loss: 0.09992042928934097\n",
      "Step: 192, Loss: 0.09999304264783859\n",
      "Step: 193, Loss: 0.09988008439540863\n",
      "Step: 194, Loss: 0.10006505995988846\n",
      "Step: 195, Loss: 0.0996040552854538\n",
      "Step: 196, Loss: 0.09975820779800415\n",
      "Step: 197, Loss: 0.0998237207531929\n",
      "Step: 198, Loss: 0.10023599117994308\n",
      "Step: 199, Loss: 0.09960661828517914\n",
      "Step: 200, Loss: 0.10003604739904404\n",
      "Step: 201, Loss: 0.09946393966674805\n",
      "Step: 202, Loss: 0.100131094455719\n",
      "Step: 203, Loss: 0.09991443157196045\n",
      "Step: 204, Loss: 0.09985866397619247\n",
      "Step: 205, Loss: 0.09977446496486664\n",
      "Step: 206, Loss: 0.09983466565608978\n",
      "Step: 207, Loss: 0.09991797804832458\n",
      "Step: 208, Loss: 0.10013588517904282\n",
      "Step: 209, Loss: 0.09971050173044205\n",
      "Step: 210, Loss: 0.09985184669494629\n",
      "Step: 211, Loss: 0.10012660920619965\n",
      "Step: 212, Loss: 0.10011982917785645\n",
      "Step: 213, Loss: 0.10008106380701065\n",
      "Step: 214, Loss: 0.0996464341878891\n",
      "Step: 215, Loss: 0.09952999651432037\n",
      "Step: 216, Loss: 0.09975336492061615\n",
      "Step: 217, Loss: 0.10034859925508499\n",
      "Step: 218, Loss: 0.10001468658447266\n",
      "Step: 219, Loss: 0.09998709708452225\n",
      "Step: 220, Loss: 0.09959638863801956\n",
      "Step: 221, Loss: 0.10007799416780472\n",
      "Step: 222, Loss: 0.09981153905391693\n",
      "Step: 223, Loss: 0.09976130723953247\n",
      "Step: 224, Loss: 0.09986881166696548\n",
      "Step: 225, Loss: 0.1004132404923439\n",
      "Step: 226, Loss: 0.10018006712198257\n",
      "Step: 227, Loss: 0.09992454946041107\n",
      "Step: 228, Loss: 0.09985758364200592\n",
      "Step: 229, Loss: 0.10003688186407089\n",
      "Step: 230, Loss: 0.09983201324939728\n",
      "Step: 231, Loss: 0.09984076023101807\n",
      "Step: 232, Loss: 0.09968853741884232\n",
      "Step: 233, Loss: 0.10021143406629562\n",
      "Step: 234, Loss: 0.10005252063274384\n",
      "Step: 235, Loss: 0.09953856468200684\n",
      "Step: 236, Loss: 0.09988480806350708\n",
      "Step: 237, Loss: 0.09968262910842896\n",
      "Step: 238, Loss: 0.09969595074653625\n",
      "Step: 239, Loss: 0.09975699335336685\n",
      "Step: 240, Loss: 0.09970810264348984\n",
      "Step: 241, Loss: 0.09993459284305573\n",
      "Step: 242, Loss: 0.10000336170196533\n",
      "Step: 243, Loss: 0.10025124996900558\n",
      "Step: 244, Loss: 0.09998960793018341\n",
      "Step: 245, Loss: 0.09986984729766846\n",
      "Step: 246, Loss: 0.09957825392484665\n",
      "Step: 247, Loss: 0.09991302341222763\n",
      "Step: 248, Loss: 0.09960884600877762\n",
      "Step: 249, Loss: 0.10005608946084976\n",
      "Step: 250, Loss: 0.10001975297927856\n",
      "Step: 251, Loss: 0.10007541626691818\n",
      "Step: 252, Loss: 0.09981565922498703\n",
      "Step: 253, Loss: 0.0997411459684372\n",
      "Step: 254, Loss: 0.09985959529876709\n",
      "Step: 255, Loss: 0.09977702796459198\n",
      "Step: 256, Loss: 0.09994734078645706\n",
      "Step: 257, Loss: 0.09973761439323425\n",
      "Step: 258, Loss: 0.09974502772092819\n",
      "Step: 259, Loss: 0.09975292533636093\n",
      "Step: 260, Loss: 0.0995725467801094\n",
      "Step: 261, Loss: 0.09971857070922852\n",
      "Step: 262, Loss: 0.09960460662841797\n",
      "Step: 263, Loss: 0.09983714669942856\n",
      "Step: 264, Loss: 0.0994969978928566\n",
      "Step: 265, Loss: 0.09993026405572891\n",
      "Step: 266, Loss: 0.10000336170196533\n",
      "Step: 267, Loss: 0.0997394397854805\n",
      "Step: 268, Loss: 0.0997757539153099\n",
      "Step: 269, Loss: 0.10010257363319397\n",
      "Step: 270, Loss: 0.10022033751010895\n",
      "Step: 271, Loss: 0.09982910007238388\n",
      "Step: 272, Loss: 0.1000540629029274\n",
      "Step: 273, Loss: 0.09944810718297958\n",
      "Step: 274, Loss: 0.09984477609395981\n",
      "Step: 275, Loss: 0.0992569848895073\n",
      "Step: 276, Loss: 0.09955143183469772\n",
      "Step: 277, Loss: 0.09981421381235123\n",
      "Step: 278, Loss: 0.09991719573736191\n",
      "Step: 279, Loss: 0.0994381308555603\n",
      "Step: 280, Loss: 0.09965036064386368\n",
      "Step: 281, Loss: 0.09970209002494812\n",
      "Step: 282, Loss: 0.10018698871135712\n",
      "Step: 283, Loss: 0.09987757354974747\n",
      "Step: 284, Loss: 0.0996500626206398\n",
      "Step: 285, Loss: 0.100091852247715\n",
      "Step: 286, Loss: 0.09989342093467712\n",
      "Step: 287, Loss: 0.10021798312664032\n",
      "Step: 288, Loss: 0.0999566838145256\n",
      "Step: 289, Loss: 0.0995660126209259\n",
      "Step: 290, Loss: 0.09934667497873306\n",
      "Step: 291, Loss: 0.0996696874499321\n",
      "Step: 292, Loss: 0.09985460340976715\n",
      "Step: 293, Loss: 0.09978452324867249\n",
      "Step: 294, Loss: 0.09970322996377945\n",
      "Step: 295, Loss: 0.09984125941991806\n",
      "Step: 296, Loss: 0.09951096773147583\n",
      "Step: 297, Loss: 0.09961052238941193\n",
      "Step: 298, Loss: 0.09987451136112213\n",
      "Step: 299, Loss: 0.09990452975034714\n",
      "Step: 300, Loss: 0.10009047389030457\n",
      "Step: 301, Loss: 0.09997782856225967\n",
      "Step: 302, Loss: 0.09924758970737457\n",
      "Step: 303, Loss: 0.09946586191654205\n",
      "Step: 304, Loss: 0.0998670756816864\n",
      "Step: 305, Loss: 0.10005007684230804\n",
      "Step: 306, Loss: 0.10001236945390701\n",
      "Step: 307, Loss: 0.09990290552377701\n",
      "Step: 308, Loss: 0.09959187358617783\n",
      "Step: 309, Loss: 0.1002177894115448\n",
      "Step: 310, Loss: 0.09970739483833313\n",
      "Step: 311, Loss: 0.09943041950464249\n",
      "Step: 312, Loss: 0.0995108038187027\n",
      "Step: 313, Loss: 0.09992336481809616\n",
      "Step: 314, Loss: 0.09930036962032318\n",
      "Step: 315, Loss: 0.09970066696405411\n",
      "Step: 316, Loss: 0.09972327202558517\n",
      "Step: 317, Loss: 0.09958726167678833\n",
      "Step: 318, Loss: 0.09994520246982574\n",
      "Step: 319, Loss: 0.09952209144830704\n",
      "Step: 320, Loss: 0.09948418289422989\n",
      "Step: 321, Loss: 0.09951978921890259\n",
      "Step: 322, Loss: 0.09943463653326035\n",
      "Step: 323, Loss: 0.09984159469604492\n",
      "Step: 324, Loss: 0.09962151199579239\n",
      "Step: 325, Loss: 0.0996653139591217\n",
      "Step: 326, Loss: 0.09927761554718018\n",
      "Step: 327, Loss: 0.09962236881256104\n",
      "Step: 328, Loss: 0.0997888371348381\n",
      "Step: 329, Loss: 0.09921903908252716\n",
      "Step: 330, Loss: 0.10006149858236313\n",
      "Step: 331, Loss: 0.0994364321231842\n",
      "Step: 332, Loss: 0.10026399791240692\n",
      "Step: 333, Loss: 0.09971509873867035\n",
      "Step: 334, Loss: 0.10039117187261581\n",
      "Step: 335, Loss: 0.09976818412542343\n",
      "Step: 336, Loss: 0.09957128018140793\n",
      "Step: 337, Loss: 0.10015113651752472\n",
      "Step: 338, Loss: 0.10032059252262115\n",
      "Step: 339, Loss: 0.09994091838598251\n",
      "Step: 340, Loss: 0.10010998696088791\n",
      "Step: 341, Loss: 0.09978180378675461\n",
      "Step: 342, Loss: 0.09978150576353073\n",
      "Step: 343, Loss: 0.10012223571538925\n",
      "Step: 344, Loss: 0.09989391267299652\n",
      "Step: 345, Loss: 0.09989049285650253\n",
      "Step: 346, Loss: 0.10021966695785522\n",
      "Step: 347, Loss: 0.10012882947921753\n",
      "Step: 348, Loss: 0.09990854561328888\n",
      "Step: 349, Loss: 0.09975525736808777\n",
      "Step: 350, Loss: 0.10012611001729965\n",
      "Step: 351, Loss: 0.09984101355075836\n",
      "Step: 352, Loss: 0.09986074268817902\n",
      "Step: 353, Loss: 0.09975353628396988\n",
      "Step: 354, Loss: 0.09982657432556152\n",
      "Step: 355, Loss: 0.09984082728624344\n",
      "Step: 356, Loss: 0.10024143010377884\n",
      "Step: 357, Loss: 0.10007154941558838\n",
      "Step: 358, Loss: 0.10003611445426941\n",
      "Step: 359, Loss: 0.10013946890830994\n",
      "Step: 360, Loss: 0.10002493113279343\n",
      "Step: 361, Loss: 0.10018520057201385\n",
      "Step: 362, Loss: 0.09982316195964813\n",
      "Step: 363, Loss: 0.1001284196972847\n",
      "Step: 364, Loss: 0.10002555698156357\n",
      "Step: 365, Loss: 0.09970288723707199\n",
      "Step: 366, Loss: 0.09989278018474579\n",
      "Step: 367, Loss: 0.1003192737698555\n",
      "Step: 368, Loss: 0.10013553500175476\n",
      "Step: 369, Loss: 0.09986501932144165\n",
      "Step: 370, Loss: 0.10005918145179749\n",
      "Step: 371, Loss: 0.10015542060136795\n",
      "Step: 372, Loss: 0.09939827024936676\n",
      "Step: 373, Loss: 0.10003519803285599\n",
      "Step: 374, Loss: 0.0998372957110405\n",
      "Step: 375, Loss: 0.10035349428653717\n",
      "Step: 376, Loss: 0.09981882572174072\n",
      "Step: 377, Loss: 0.09971339255571365\n",
      "Step: 378, Loss: 0.09950613230466843\n",
      "Step: 379, Loss: 0.0999230444431305\n",
      "Step: 380, Loss: 0.10002359747886658\n",
      "Step: 381, Loss: 0.10012052953243256\n",
      "Step: 382, Loss: 0.10015081614255905\n",
      "Step: 383, Loss: 0.09993571788072586\n",
      "Step: 384, Loss: 0.10028791427612305\n",
      "Step: 385, Loss: 0.09959328919649124\n",
      "Step: 386, Loss: 0.09993324428796768\n",
      "Step: 387, Loss: 0.09984131157398224\n",
      "Step: 388, Loss: 0.09991424530744553\n",
      "Step: 389, Loss: 0.10016492754220963\n",
      "Step: 390, Loss: 0.09998331218957901\n",
      "Step: 391, Loss: 0.09986256062984467\n",
      "Step: 392, Loss: 0.09996448457241058\n",
      "Step: 393, Loss: 0.09943445771932602\n",
      "Step: 394, Loss: 0.09974827617406845\n",
      "Step: 395, Loss: 0.10029754787683487\n",
      "Step: 396, Loss: 0.10010799765586853\n",
      "Step: 397, Loss: 0.09959585964679718\n",
      "Step: 398, Loss: 0.09976475685834885\n",
      "Step: 399, Loss: 0.09986741095781326\n",
      "Step: 400, Loss: 0.09970524162054062\n",
      "Step: 401, Loss: 0.09987582266330719\n",
      "Step: 402, Loss: 0.09949187934398651\n",
      "Step: 403, Loss: 0.09993467479944229\n",
      "Step: 404, Loss: 0.10013909637928009\n",
      "Step: 405, Loss: 0.09980069845914841\n",
      "Step: 406, Loss: 0.10038300603628159\n",
      "Step: 407, Loss: 0.09998966008424759\n",
      "Step: 408, Loss: 0.09987952560186386\n",
      "Step: 409, Loss: 0.0997140109539032\n",
      "Step: 410, Loss: 0.09966640919446945\n",
      "Step: 411, Loss: 0.09958469867706299\n",
      "Step: 412, Loss: 0.10002169013023376\n",
      "Step: 413, Loss: 0.09982061386108398\n",
      "Step: 414, Loss: 0.09994416683912277\n",
      "Step: 415, Loss: 0.09970349818468094\n",
      "Step: 416, Loss: 0.10004322975873947\n",
      "Step: 417, Loss: 0.0996941328048706\n",
      "Step: 418, Loss: 0.09992799162864685\n",
      "Step: 419, Loss: 0.10003471374511719\n",
      "Step: 420, Loss: 0.09996713697910309\n",
      "Step: 421, Loss: 0.09973790496587753\n",
      "Step: 422, Loss: 0.09977637976408005\n",
      "Step: 423, Loss: 0.10000914335250854\n",
      "Step: 424, Loss: 0.10004518181085587\n",
      "Step: 425, Loss: 0.10023046284914017\n",
      "Step: 426, Loss: 0.09995432198047638\n",
      "Step: 427, Loss: 0.09925954788923264\n",
      "Step: 428, Loss: 0.09991728514432907\n",
      "Step: 429, Loss: 0.09946966916322708\n",
      "Step: 430, Loss: 0.09996330738067627\n",
      "Step: 431, Loss: 0.09970986098051071\n",
      "Step: 432, Loss: 0.09970537573099136\n",
      "Step: 433, Loss: 0.10017500817775726\n",
      "Step: 434, Loss: 0.0995701476931572\n",
      "Step: 435, Loss: 0.09996020793914795\n",
      "Step: 436, Loss: 0.09986601769924164\n",
      "Step: 437, Loss: 0.09947854280471802\n",
      "Step: 438, Loss: 0.09946966916322708\n",
      "Step: 439, Loss: 0.09941795468330383\n",
      "Step: 440, Loss: 0.0998176634311676\n",
      "Step: 441, Loss: 0.0996798649430275\n",
      "Step: 442, Loss: 0.0996241644024849\n",
      "Step: 443, Loss: 0.09985533356666565\n",
      "Step: 444, Loss: 0.09977034479379654\n",
      "Step: 445, Loss: 0.09954078495502472\n",
      "Step: 446, Loss: 0.09973353892564774\n",
      "Step: 447, Loss: 0.09921430796384811\n",
      "Step: 448, Loss: 0.09962459653615952\n",
      "Step: 449, Loss: 0.09963539242744446\n",
      "Step: 450, Loss: 0.09927268326282501\n",
      "Step: 451, Loss: 0.09973824769258499\n",
      "Step: 452, Loss: 0.10009041428565979\n",
      "Step: 453, Loss: 0.09966740012168884\n",
      "Step: 454, Loss: 0.10032093524932861\n",
      "Step: 455, Loss: 0.10001173615455627\n",
      "Step: 456, Loss: 0.09977711737155914\n",
      "Step: 457, Loss: 0.09990888833999634\n",
      "Step: 458, Loss: 0.10095333307981491\n",
      "Step: 459, Loss: 0.10029976814985275\n",
      "Step: 460, Loss: 0.10055486112833023\n",
      "Step: 461, Loss: 0.100510373711586\n",
      "Step: 462, Loss: 0.1001894623041153\n",
      "Step: 463, Loss: 0.1000116840004921\n",
      "Step: 464, Loss: 0.10017205774784088\n",
      "Step: 465, Loss: 0.09987734258174896\n",
      "Step: 466, Loss: 0.09993784874677658\n",
      "Step: 467, Loss: 0.10050167143344879\n",
      "Step: 468, Loss: 0.10080045461654663\n",
      "Step: 469, Loss: 0.10109151899814606\n",
      "Step: 470, Loss: 0.10154446959495544\n",
      "Step: 471, Loss: 0.101442851126194\n",
      "Step: 472, Loss: 0.10067947208881378\n",
      "Step: 473, Loss: 0.10112825036048889\n",
      "Step: 474, Loss: 0.10031715780496597\n",
      "Step: 475, Loss: 0.10064329952001572\n",
      "Step: 476, Loss: 0.1008189469575882\n",
      "Step: 477, Loss: 0.1007021814584732\n",
      "Step: 478, Loss: 0.10026750713586807\n",
      "Step: 479, Loss: 0.10025402158498764\n",
      "Step: 480, Loss: 0.10001331567764282\n",
      "Step: 481, Loss: 0.10018754005432129\n",
      "Step: 482, Loss: 0.10039862245321274\n",
      "Step: 483, Loss: 0.10031452029943466\n",
      "Step: 484, Loss: 0.10024953633546829\n",
      "Step: 485, Loss: 0.10004188120365143\n",
      "Step: 486, Loss: 0.09993920475244522\n",
      "Step: 487, Loss: 0.10009193420410156\n",
      "Step: 488, Loss: 0.09955208003520966\n",
      "Step: 489, Loss: 0.09983590990304947\n",
      "Step: 490, Loss: 0.10010526329278946\n",
      "Step: 491, Loss: 0.09993577003479004\n",
      "Step: 492, Loss: 0.10001025348901749\n",
      "Step: 493, Loss: 0.09999268501996994\n",
      "Step: 494, Loss: 0.1001076027750969\n",
      "Step: 495, Loss: 0.09974785894155502\n",
      "Step: 496, Loss: 0.10024365037679672\n",
      "Step: 497, Loss: 0.09986631572246552\n",
      "Step: 498, Loss: 0.09976784884929657\n",
      "Step: 499, Loss: 0.10009763389825821\n",
      "Step: 500, Loss: 0.0998651534318924\n",
      "Step: 501, Loss: 0.0996825098991394\n",
      "Step: 502, Loss: 0.0994672030210495\n",
      "Step: 503, Loss: 0.09935992956161499\n",
      "Step: 504, Loss: 0.09979315102100372\n",
      "Step: 505, Loss: 0.09958283603191376\n",
      "Step: 506, Loss: 0.09996163845062256\n",
      "Step: 507, Loss: 0.09993566572666168\n",
      "Step: 508, Loss: 0.1003762036561966\n",
      "Step: 509, Loss: 0.10008284449577332\n",
      "Step: 510, Loss: 0.09933522343635559\n",
      "Step: 511, Loss: 0.0995444506406784\n",
      "Step: 512, Loss: 0.10031702369451523\n",
      "Step: 513, Loss: 0.09976620972156525\n",
      "Step: 514, Loss: 0.09976449608802795\n",
      "Step: 515, Loss: 0.0998372808098793\n",
      "Step: 516, Loss: 0.0995604619383812\n",
      "Step: 517, Loss: 0.09973584115505219\n",
      "Step: 518, Loss: 0.09963072091341019\n",
      "Step: 519, Loss: 0.09962606430053711\n",
      "Step: 520, Loss: 0.09978165477514267\n",
      "Step: 521, Loss: 0.0993596613407135\n",
      "Step: 522, Loss: 0.09998148679733276\n",
      "Step: 523, Loss: 0.1000947579741478\n",
      "Step: 524, Loss: 0.09979057312011719\n",
      "Step: 525, Loss: 0.1000363752245903\n",
      "Step: 526, Loss: 0.099674291908741\n",
      "Step: 527, Loss: 0.09972982108592987\n",
      "Step: 528, Loss: 0.09998524188995361\n",
      "Step: 529, Loss: 0.09960104525089264\n",
      "Step: 530, Loss: 0.09983497858047485\n",
      "Step: 531, Loss: 0.09992524981498718\n",
      "Step: 532, Loss: 0.09966707229614258\n",
      "Step: 533, Loss: 0.09958945959806442\n",
      "Step: 534, Loss: 0.09980438649654388\n",
      "Step: 535, Loss: 0.0994650349020958\n",
      "Step: 536, Loss: 0.09983722120523453\n",
      "Step: 537, Loss: 0.09963468462228775\n",
      "Step: 538, Loss: 0.09971854090690613\n",
      "Step: 539, Loss: 0.09986277669668198\n",
      "Step: 540, Loss: 0.09945572912693024\n",
      "Step: 541, Loss: 0.10004832595586777\n",
      "Step: 542, Loss: 0.09977001696825027\n",
      "Step: 543, Loss: 0.09960256516933441\n",
      "Step: 544, Loss: 0.09927938133478165\n",
      "Step: 545, Loss: 0.09926994144916534\n",
      "Step: 546, Loss: 0.09978073835372925\n",
      "Step: 547, Loss: 0.09961432218551636\n",
      "Step: 548, Loss: 0.09936332702636719\n",
      "Step: 549, Loss: 0.09980623424053192\n",
      "Step: 550, Loss: 0.09983431547880173\n",
      "Step: 551, Loss: 0.09938172250986099\n",
      "Step: 552, Loss: 0.09943331778049469\n",
      "Step: 553, Loss: 0.09966979175806046\n",
      "Step: 554, Loss: 0.09966657310724258\n",
      "Step: 555, Loss: 0.09943845868110657\n",
      "Step: 556, Loss: 0.09954114258289337\n",
      "Step: 557, Loss: 0.09956977516412735\n",
      "Step: 558, Loss: 0.09957275539636612\n",
      "Step: 559, Loss: 0.09975379705429077\n",
      "Step: 560, Loss: 0.09955392777919769\n",
      "Step: 561, Loss: 0.09936036169528961\n",
      "Step: 562, Loss: 0.09943887591362\n",
      "Step: 563, Loss: 0.09995297342538834\n",
      "Step: 564, Loss: 0.0995626151561737\n",
      "Step: 565, Loss: 0.09926672279834747\n",
      "Step: 566, Loss: 0.09947508573532104\n",
      "Step: 567, Loss: 0.09905603528022766\n",
      "Step: 568, Loss: 0.09952205419540405\n",
      "Step: 569, Loss: 0.09928470104932785\n",
      "Step: 570, Loss: 0.09992332756519318\n",
      "Step: 571, Loss: 0.09943296760320663\n",
      "Step: 572, Loss: 0.09956399351358414\n",
      "Step: 573, Loss: 0.09931079298257828\n",
      "Step: 574, Loss: 0.09973365068435669\n",
      "Step: 575, Loss: 0.0994490459561348\n",
      "Step: 576, Loss: 0.09972744435071945\n",
      "Step: 577, Loss: 0.09966512769460678\n",
      "Step: 578, Loss: 0.09981945157051086\n",
      "Step: 579, Loss: 0.0996147096157074\n",
      "Step: 580, Loss: 0.09937168657779694\n",
      "Step: 581, Loss: 0.09961981326341629\n",
      "Step: 582, Loss: 0.09938687086105347\n",
      "Step: 583, Loss: 0.09917938709259033\n",
      "Step: 584, Loss: 0.09949074685573578\n",
      "Step: 585, Loss: 0.09972810000181198\n",
      "Step: 586, Loss: 0.09952537715435028\n",
      "Step: 587, Loss: 0.09971737116575241\n",
      "Step: 588, Loss: 0.09962929040193558\n",
      "Step: 589, Loss: 0.09988339990377426\n",
      "Step: 590, Loss: 0.09978655725717545\n",
      "Step: 591, Loss: 0.09918398410081863\n",
      "Step: 592, Loss: 0.09979411959648132\n",
      "Step: 593, Loss: 0.09905647486448288\n",
      "Step: 594, Loss: 0.09952230006456375\n",
      "Step: 595, Loss: 0.09919173270463943\n",
      "Step: 596, Loss: 0.09927141666412354\n",
      "Step: 597, Loss: 0.09943622350692749\n",
      "Step: 598, Loss: 0.09921099990606308\n",
      "Step: 599, Loss: 0.09971991926431656\n",
      "Final loss:  0.09971992\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/mixturelinear_tn_gev_twcrps_mean13.0_std4.0_epochs600_folds_1_3.pkl\n"
     ]
    }
   ],
   "source": [
    "chain_function_std = 4\n",
    "\n",
    "model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    parameter_names,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_constant = chain_function_constant,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
