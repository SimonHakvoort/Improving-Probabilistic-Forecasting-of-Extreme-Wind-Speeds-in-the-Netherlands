{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.training import train_model, train_and_save, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_distribution = 'distr_mixture_linear'\n",
    "distribution_1 = 'distr_trunc_normal'\n",
    "distribution_2 = 'distr_gev'\n",
    "\n",
    "loss = 'loss_twCRPS_sample' # options: loss_CRPS_sample, loss_twCRPS_sample, loss_log_likelihood\n",
    "\n",
    "chain_function = 'chain_function_normal_cdf_plus_constant' # options: chain_function_normal_cdf, chain_function_indicator, chain_function_normal_cdf_plus_constant\n",
    "chain_function_mean = 13\n",
    "chain_function_std = 1.5\n",
    "chain_function_threshold = 15 # 12 / 15\n",
    "chain_function_constant = 0.03\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.03\n",
    "folds = [1,2]\n",
    "parameter_names = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "neighbourhood_size = 11\n",
    "ignore = ['229', '285', '323']\n",
    "epochs = 600\n",
    "\n",
    "samples = 200\n",
    "printing = True\n",
    "pretrained = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Final loss:  0.07355957\n",
      "Final loss:  0.07442123\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using given parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 0.07308967411518097\n",
      "Step: 1, Loss: 0.07332175970077515\n",
      "Step: 2, Loss: 0.07289049029350281\n",
      "Step: 3, Loss: 0.07267902791500092\n",
      "Step: 4, Loss: 0.0722259134054184\n",
      "Step: 5, Loss: 0.07247698307037354\n",
      "Step: 6, Loss: 0.07255276292562485\n",
      "Step: 7, Loss: 0.07246234267950058\n",
      "Step: 8, Loss: 0.07212276011705399\n",
      "Step: 9, Loss: 0.0725584477186203\n",
      "Step: 10, Loss: 0.07225164771080017\n",
      "Step: 11, Loss: 0.07201602309942245\n",
      "Step: 12, Loss: 0.07240552455186844\n",
      "Step: 13, Loss: 0.07212428748607635\n",
      "Step: 14, Loss: 0.07216006517410278\n",
      "Step: 15, Loss: 0.07223380357027054\n",
      "Step: 16, Loss: 0.07244416326284409\n",
      "Step: 17, Loss: 0.07207071781158447\n",
      "Step: 18, Loss: 0.07177704572677612\n",
      "Step: 19, Loss: 0.07201306521892548\n",
      "Step: 20, Loss: 0.0723009705543518\n",
      "Step: 21, Loss: 0.07225554436445236\n",
      "Step: 22, Loss: 0.07209757715463638\n",
      "Step: 23, Loss: 0.07221953570842743\n",
      "Step: 24, Loss: 0.07214444875717163\n",
      "Step: 25, Loss: 0.07217322289943695\n",
      "Step: 26, Loss: 0.07195961475372314\n",
      "Step: 27, Loss: 0.07203836739063263\n",
      "Step: 28, Loss: 0.07202889770269394\n",
      "Step: 29, Loss: 0.07232224941253662\n",
      "Step: 30, Loss: 0.07189299911260605\n",
      "Step: 31, Loss: 0.07197776436805725\n",
      "Step: 32, Loss: 0.07212936878204346\n",
      "Step: 33, Loss: 0.07188871502876282\n",
      "Step: 34, Loss: 0.07188403606414795\n",
      "Step: 35, Loss: 0.07192962616682053\n",
      "Step: 36, Loss: 0.07183542847633362\n",
      "Step: 37, Loss: 0.07186184078454971\n",
      "Step: 38, Loss: 0.07172518968582153\n",
      "Step: 39, Loss: 0.07189237326383591\n",
      "Step: 40, Loss: 0.07158602774143219\n",
      "Step: 41, Loss: 0.07178505510091782\n",
      "Step: 42, Loss: 0.0719524472951889\n",
      "Step: 43, Loss: 0.07197404652833939\n",
      "Step: 44, Loss: 0.071809783577919\n",
      "Step: 45, Loss: 0.07200121879577637\n",
      "Step: 46, Loss: 0.07201175391674042\n",
      "Step: 47, Loss: 0.0721956267952919\n",
      "Step: 48, Loss: 0.07186148315668106\n",
      "Step: 49, Loss: 0.07203890383243561\n",
      "Step: 50, Loss: 0.07207465171813965\n",
      "Step: 51, Loss: 0.07175219058990479\n",
      "Step: 52, Loss: 0.07189774513244629\n",
      "Step: 53, Loss: 0.07190116494894028\n",
      "Step: 54, Loss: 0.07186421751976013\n",
      "Step: 55, Loss: 0.0717523917555809\n",
      "Step: 56, Loss: 0.0719815343618393\n",
      "Step: 57, Loss: 0.07207581400871277\n",
      "Step: 58, Loss: 0.0717105120420456\n",
      "Step: 59, Loss: 0.07158654928207397\n",
      "Step: 60, Loss: 0.07168193906545639\n",
      "Step: 61, Loss: 0.07197267562150955\n",
      "Step: 62, Loss: 0.07173743098974228\n",
      "Step: 63, Loss: 0.07193799316883087\n",
      "Step: 64, Loss: 0.07188274711370468\n",
      "Step: 65, Loss: 0.07155351340770721\n",
      "Step: 66, Loss: 0.0716608390212059\n",
      "Step: 67, Loss: 0.07160526514053345\n",
      "Step: 68, Loss: 0.07169193774461746\n",
      "Step: 69, Loss: 0.0715772956609726\n",
      "Step: 70, Loss: 0.07187449932098389\n",
      "Step: 71, Loss: 0.07169967144727707\n",
      "Step: 72, Loss: 0.07161789387464523\n",
      "Step: 73, Loss: 0.0714121088385582\n",
      "Step: 74, Loss: 0.07142377644777298\n",
      "Step: 75, Loss: 0.07173963636159897\n",
      "Step: 76, Loss: 0.07155351340770721\n",
      "Step: 77, Loss: 0.07172392308712006\n",
      "Step: 78, Loss: 0.07180046290159225\n",
      "Step: 79, Loss: 0.07165250182151794\n",
      "Step: 80, Loss: 0.07122332602739334\n",
      "Step: 81, Loss: 0.07148962467908859\n",
      "Step: 82, Loss: 0.07135377079248428\n",
      "Step: 83, Loss: 0.07166726142168045\n",
      "Step: 84, Loss: 0.07152605801820755\n",
      "Step: 85, Loss: 0.07154562324285507\n",
      "Step: 86, Loss: 0.07148827612400055\n",
      "Step: 87, Loss: 0.071661576628685\n",
      "Step: 88, Loss: 0.07149051129817963\n",
      "Step: 89, Loss: 0.07127819210290909\n",
      "Step: 90, Loss: 0.07157433032989502\n",
      "Step: 91, Loss: 0.07163961976766586\n",
      "Step: 92, Loss: 0.07160037755966187\n",
      "Step: 93, Loss: 0.07145405560731888\n",
      "Step: 94, Loss: 0.07144860923290253\n",
      "Step: 95, Loss: 0.0715126320719719\n",
      "Step: 96, Loss: 0.07141051441431046\n",
      "Step: 97, Loss: 0.07133788615465164\n",
      "Step: 98, Loss: 0.07149888575077057\n",
      "Step: 99, Loss: 0.07158932089805603\n",
      "Step: 100, Loss: 0.0716017633676529\n",
      "Step: 101, Loss: 0.07147140800952911\n",
      "Step: 102, Loss: 0.07136567682027817\n",
      "Step: 103, Loss: 0.07174380123615265\n",
      "Step: 104, Loss: 0.07138809561729431\n",
      "Step: 105, Loss: 0.07150475680828094\n",
      "Step: 106, Loss: 0.07144487649202347\n",
      "Step: 107, Loss: 0.07151452451944351\n",
      "Step: 108, Loss: 0.07173288613557816\n",
      "Step: 109, Loss: 0.07151567935943604\n",
      "Step: 110, Loss: 0.07165691256523132\n",
      "Step: 111, Loss: 0.07153394818305969\n",
      "Step: 112, Loss: 0.07172813266515732\n",
      "Step: 113, Loss: 0.0718027725815773\n",
      "Step: 114, Loss: 0.0715167298913002\n",
      "Step: 115, Loss: 0.07183917611837387\n",
      "Step: 116, Loss: 0.07150524109601974\n",
      "Step: 117, Loss: 0.07187485694885254\n",
      "Step: 118, Loss: 0.07149580121040344\n",
      "Step: 119, Loss: 0.07160605490207672\n",
      "Step: 120, Loss: 0.07170993089675903\n",
      "Step: 121, Loss: 0.07138095796108246\n",
      "Step: 122, Loss: 0.07161431759595871\n",
      "Step: 123, Loss: 0.07170876115560532\n",
      "Step: 124, Loss: 0.07159429043531418\n",
      "Step: 125, Loss: 0.07156315445899963\n",
      "Step: 126, Loss: 0.07134038954973221\n",
      "Step: 127, Loss: 0.07153967767953873\n",
      "Step: 128, Loss: 0.07182031869888306\n",
      "Step: 129, Loss: 0.07152260839939117\n",
      "Step: 130, Loss: 0.07166104763746262\n",
      "Step: 131, Loss: 0.07160232961177826\n",
      "Step: 132, Loss: 0.07164710760116577\n",
      "Step: 133, Loss: 0.07151558250188828\n",
      "Step: 134, Loss: 0.07146552205085754\n",
      "Step: 135, Loss: 0.07147173583507538\n",
      "Step: 136, Loss: 0.07175644487142563\n",
      "Step: 137, Loss: 0.0714496374130249\n",
      "Step: 138, Loss: 0.07164207845926285\n",
      "Step: 139, Loss: 0.07176125794649124\n",
      "Step: 140, Loss: 0.07159645110368729\n",
      "Step: 141, Loss: 0.07168158888816833\n",
      "Step: 142, Loss: 0.07194184511899948\n",
      "Step: 143, Loss: 0.07143925875425339\n",
      "Step: 144, Loss: 0.07133615016937256\n",
      "Step: 145, Loss: 0.07144103944301605\n",
      "Step: 146, Loss: 0.07128050923347473\n",
      "Step: 147, Loss: 0.07156527042388916\n",
      "Step: 148, Loss: 0.07145146280527115\n",
      "Step: 149, Loss: 0.07143989205360413\n",
      "Step: 150, Loss: 0.07140503823757172\n",
      "Step: 151, Loss: 0.07149321585893631\n",
      "Step: 152, Loss: 0.071431003510952\n",
      "Step: 153, Loss: 0.0714796781539917\n",
      "Step: 154, Loss: 0.07165736705064774\n",
      "Step: 155, Loss: 0.07124695926904678\n",
      "Step: 156, Loss: 0.07146801799535751\n",
      "Step: 157, Loss: 0.07153447717428207\n",
      "Step: 158, Loss: 0.07134123891592026\n",
      "Step: 159, Loss: 0.07141706347465515\n",
      "Step: 160, Loss: 0.07126681506633759\n",
      "Step: 161, Loss: 0.07138360291719437\n",
      "Step: 162, Loss: 0.07160022109746933\n",
      "Step: 163, Loss: 0.07151676714420319\n",
      "Step: 164, Loss: 0.07142706215381622\n",
      "Step: 165, Loss: 0.07116887718439102\n",
      "Step: 166, Loss: 0.07102302461862564\n",
      "Step: 167, Loss: 0.07143794745206833\n",
      "Step: 168, Loss: 0.0713299959897995\n",
      "Step: 169, Loss: 0.07127346843481064\n",
      "Step: 170, Loss: 0.07117576897144318\n",
      "Step: 171, Loss: 0.07127315551042557\n",
      "Step: 172, Loss: 0.07114040106534958\n",
      "Step: 173, Loss: 0.07102563232183456\n",
      "Step: 174, Loss: 0.07124842703342438\n",
      "Step: 175, Loss: 0.07149647921323776\n",
      "Step: 176, Loss: 0.07099700719118118\n",
      "Step: 177, Loss: 0.07121478021144867\n",
      "Step: 178, Loss: 0.0713566243648529\n",
      "Step: 179, Loss: 0.07147986441850662\n",
      "Step: 180, Loss: 0.07118096947669983\n",
      "Step: 181, Loss: 0.07126215845346451\n",
      "Step: 182, Loss: 0.0713648572564125\n",
      "Step: 183, Loss: 0.0714496448636055\n",
      "Step: 184, Loss: 0.07120942324399948\n",
      "Step: 185, Loss: 0.07131502777338028\n",
      "Step: 186, Loss: 0.0713592916727066\n",
      "Step: 187, Loss: 0.07123401761054993\n",
      "Step: 188, Loss: 0.07112744450569153\n",
      "Step: 189, Loss: 0.07119742035865784\n",
      "Step: 190, Loss: 0.07112635672092438\n",
      "Step: 191, Loss: 0.07098381966352463\n",
      "Step: 192, Loss: 0.0713752955198288\n",
      "Step: 193, Loss: 0.07138384133577347\n",
      "Step: 194, Loss: 0.07112851738929749\n",
      "Step: 195, Loss: 0.07119157165288925\n",
      "Step: 196, Loss: 0.07118336856365204\n",
      "Step: 197, Loss: 0.07125771790742874\n",
      "Step: 198, Loss: 0.07114413380622864\n",
      "Step: 199, Loss: 0.07096092402935028\n",
      "Step: 200, Loss: 0.07136714458465576\n",
      "Step: 201, Loss: 0.07121658325195312\n",
      "Step: 202, Loss: 0.07102812081575394\n",
      "Step: 203, Loss: 0.07111784815788269\n",
      "Step: 204, Loss: 0.07102296501398087\n",
      "Step: 205, Loss: 0.071238674223423\n",
      "Step: 206, Loss: 0.07109346985816956\n",
      "Step: 207, Loss: 0.07088347524404526\n",
      "Step: 208, Loss: 0.0713387131690979\n",
      "Step: 209, Loss: 0.07137130200862885\n",
      "Step: 210, Loss: 0.0710979476571083\n",
      "Step: 211, Loss: 0.0710124522447586\n",
      "Step: 212, Loss: 0.07129612565040588\n",
      "Step: 213, Loss: 0.07095669209957123\n",
      "Step: 214, Loss: 0.07088959962129593\n",
      "Step: 215, Loss: 0.07106834650039673\n",
      "Step: 216, Loss: 0.07123836874961853\n",
      "Step: 217, Loss: 0.07116618752479553\n",
      "Step: 218, Loss: 0.07120535522699356\n",
      "Step: 219, Loss: 0.07106707990169525\n",
      "Step: 220, Loss: 0.07095254212617874\n",
      "Step: 221, Loss: 0.0710892528295517\n",
      "Step: 222, Loss: 0.07113807648420334\n",
      "Step: 223, Loss: 0.07071017473936081\n",
      "Step: 224, Loss: 0.07117144763469696\n",
      "Step: 225, Loss: 0.07085680216550827\n",
      "Step: 226, Loss: 0.07091356813907623\n",
      "Step: 227, Loss: 0.07108455896377563\n",
      "Step: 228, Loss: 0.07094844430685043\n",
      "Step: 229, Loss: 0.07113997638225555\n",
      "Step: 230, Loss: 0.07111277431249619\n",
      "Step: 231, Loss: 0.07109757512807846\n",
      "Step: 232, Loss: 0.07105106860399246\n",
      "Step: 233, Loss: 0.07106807082891464\n",
      "Step: 234, Loss: 0.07123330980539322\n",
      "Step: 235, Loss: 0.0709589347243309\n",
      "Step: 236, Loss: 0.07097093015909195\n",
      "Step: 237, Loss: 0.07103198021650314\n",
      "Step: 238, Loss: 0.07086718827486038\n",
      "Step: 239, Loss: 0.07105068117380142\n",
      "Step: 240, Loss: 0.07102019339799881\n",
      "Step: 241, Loss: 0.07095135003328323\n",
      "Step: 242, Loss: 0.07119099050760269\n",
      "Step: 243, Loss: 0.07115234434604645\n",
      "Step: 244, Loss: 0.07124409079551697\n",
      "Step: 245, Loss: 0.07114548981189728\n",
      "Step: 246, Loss: 0.07097143679857254\n",
      "Step: 247, Loss: 0.0707806721329689\n",
      "Step: 248, Loss: 0.07097280770540237\n",
      "Step: 249, Loss: 0.07088015973567963\n",
      "Step: 250, Loss: 0.07085976749658585\n",
      "Step: 251, Loss: 0.07132786512374878\n",
      "Step: 252, Loss: 0.07100380957126617\n",
      "Step: 253, Loss: 0.07077298313379288\n",
      "Step: 254, Loss: 0.0712919533252716\n",
      "Step: 255, Loss: 0.0709943175315857\n",
      "Step: 256, Loss: 0.07103215157985687\n",
      "Step: 257, Loss: 0.07114197313785553\n",
      "Step: 258, Loss: 0.07123281061649323\n",
      "Step: 259, Loss: 0.07135550677776337\n",
      "Step: 260, Loss: 0.07134133577346802\n",
      "Step: 261, Loss: 0.07132959365844727\n",
      "Step: 262, Loss: 0.07145904004573822\n",
      "Step: 263, Loss: 0.07152626663446426\n",
      "Step: 264, Loss: 0.07160792499780655\n",
      "Step: 265, Loss: 0.07139673084020615\n",
      "Step: 266, Loss: 0.0712592825293541\n",
      "Step: 267, Loss: 0.07136103510856628\n",
      "Step: 268, Loss: 0.07176900655031204\n",
      "Step: 269, Loss: 0.07132019847631454\n",
      "Step: 270, Loss: 0.07144864648580551\n",
      "Step: 271, Loss: 0.07133261114358902\n",
      "Step: 272, Loss: 0.07150504738092422\n",
      "Step: 273, Loss: 0.0714062750339508\n",
      "Step: 274, Loss: 0.07121715694665909\n",
      "Step: 275, Loss: 0.07141502946615219\n",
      "Step: 276, Loss: 0.07135019451379776\n",
      "Step: 277, Loss: 0.07140941172838211\n",
      "Step: 278, Loss: 0.07136208564043045\n",
      "Step: 279, Loss: 0.07137870043516159\n",
      "Step: 280, Loss: 0.07120435684919357\n",
      "Step: 281, Loss: 0.07166625559329987\n",
      "Step: 282, Loss: 0.07140924036502838\n",
      "Step: 283, Loss: 0.07130765169858932\n",
      "Step: 284, Loss: 0.07113392651081085\n",
      "Step: 285, Loss: 0.07141273468732834\n",
      "Step: 286, Loss: 0.07153648883104324\n",
      "Step: 287, Loss: 0.07143808156251907\n",
      "Step: 288, Loss: 0.0714263916015625\n",
      "Step: 289, Loss: 0.0713082030415535\n",
      "Step: 290, Loss: 0.07090027630329132\n",
      "Step: 291, Loss: 0.07126791030168533\n",
      "Step: 292, Loss: 0.07119812816381454\n",
      "Step: 293, Loss: 0.07109048962593079\n",
      "Step: 294, Loss: 0.07112546265125275\n",
      "Step: 295, Loss: 0.07115817815065384\n",
      "Step: 296, Loss: 0.07098608464002609\n",
      "Step: 297, Loss: 0.07128086686134338\n",
      "Step: 298, Loss: 0.07112938165664673\n",
      "Step: 299, Loss: 0.07076579332351685\n",
      "Step: 300, Loss: 0.07092510908842087\n",
      "Step: 301, Loss: 0.07086467742919922\n",
      "Step: 302, Loss: 0.07104908674955368\n",
      "Step: 303, Loss: 0.07122331857681274\n",
      "Step: 304, Loss: 0.07096469402313232\n",
      "Step: 305, Loss: 0.07086748629808426\n",
      "Step: 306, Loss: 0.07134484499692917\n",
      "Step: 307, Loss: 0.07078755646944046\n",
      "Step: 308, Loss: 0.07098982483148575\n",
      "Step: 309, Loss: 0.07094572484493256\n",
      "Step: 310, Loss: 0.0713006854057312\n",
      "Step: 311, Loss: 0.07093411684036255\n",
      "Step: 312, Loss: 0.07091156393289566\n",
      "Step: 313, Loss: 0.0712440013885498\n",
      "Step: 314, Loss: 0.07119036465883255\n",
      "Step: 315, Loss: 0.07082664966583252\n",
      "Step: 316, Loss: 0.07081158459186554\n",
      "Step: 317, Loss: 0.07077735662460327\n",
      "Step: 318, Loss: 0.0710945799946785\n",
      "Step: 319, Loss: 0.07105845212936401\n",
      "Step: 320, Loss: 0.07096674293279648\n",
      "Step: 321, Loss: 0.07088867574930191\n",
      "Step: 322, Loss: 0.07076303660869598\n",
      "Step: 323, Loss: 0.07094778120517731\n",
      "Step: 324, Loss: 0.07094712555408478\n",
      "Step: 325, Loss: 0.07087798416614532\n",
      "Step: 326, Loss: 0.0712030827999115\n",
      "Step: 327, Loss: 0.07138455659151077\n",
      "Step: 328, Loss: 0.0712921991944313\n",
      "Step: 329, Loss: 0.07157765328884125\n",
      "Step: 330, Loss: 0.07152879983186722\n",
      "Step: 331, Loss: 0.07175217568874359\n",
      "Step: 332, Loss: 0.07175532728433609\n",
      "Step: 333, Loss: 0.0719185322523117\n",
      "Step: 334, Loss: 0.0718088299036026\n",
      "Step: 335, Loss: 0.07156164199113846\n",
      "Step: 336, Loss: 0.07173197716474533\n",
      "Step: 337, Loss: 0.07170360535383224\n",
      "Step: 338, Loss: 0.07152797281742096\n",
      "Step: 339, Loss: 0.07163340598344803\n",
      "Step: 340, Loss: 0.07150126248598099\n",
      "Step: 341, Loss: 0.07199929654598236\n",
      "Step: 342, Loss: 0.07159589231014252\n",
      "Step: 343, Loss: 0.0716644674539566\n",
      "Step: 344, Loss: 0.07174067199230194\n",
      "Step: 345, Loss: 0.07172410190105438\n",
      "Step: 346, Loss: 0.07183459401130676\n",
      "Step: 347, Loss: 0.0717889666557312\n",
      "Step: 348, Loss: 0.07205839455127716\n",
      "Step: 349, Loss: 0.0714821144938469\n",
      "Step: 350, Loss: 0.0721157044172287\n",
      "Step: 351, Loss: 0.07141170650720596\n",
      "Step: 352, Loss: 0.07191765308380127\n",
      "Step: 353, Loss: 0.07168672978878021\n",
      "Step: 354, Loss: 0.07200079411268234\n",
      "Step: 355, Loss: 0.0717543289065361\n",
      "Step: 356, Loss: 0.07202611118555069\n",
      "Step: 357, Loss: 0.07187968492507935\n",
      "Step: 358, Loss: 0.07180828601121902\n",
      "Step: 359, Loss: 0.07181005924940109\n",
      "Step: 360, Loss: 0.07160448282957077\n",
      "Step: 361, Loss: 0.07146994769573212\n",
      "Step: 362, Loss: 0.07139614224433899\n",
      "Step: 363, Loss: 0.07162254303693771\n",
      "Step: 364, Loss: 0.07161107659339905\n",
      "Step: 365, Loss: 0.07173395156860352\n",
      "Step: 366, Loss: 0.07121328264474869\n",
      "Step: 367, Loss: 0.07197018712759018\n",
      "Step: 368, Loss: 0.07147472351789474\n",
      "Step: 369, Loss: 0.07190325111150742\n",
      "Step: 370, Loss: 0.07192596793174744\n",
      "Step: 371, Loss: 0.07194392383098602\n",
      "Step: 372, Loss: 0.07193928956985474\n",
      "Step: 373, Loss: 0.07181788235902786\n",
      "Step: 374, Loss: 0.07177484780550003\n",
      "Step: 375, Loss: 0.0718517005443573\n",
      "Step: 376, Loss: 0.07168363034725189\n",
      "Step: 377, Loss: 0.07163939625024796\n",
      "Step: 378, Loss: 0.07157448679208755\n",
      "Step: 379, Loss: 0.07152949273586273\n",
      "Step: 380, Loss: 0.07168320566415787\n",
      "Step: 381, Loss: 0.07182969897985458\n",
      "Step: 382, Loss: 0.07172971218824387\n",
      "Step: 383, Loss: 0.07178807258605957\n",
      "Step: 384, Loss: 0.07175510376691818\n",
      "Step: 385, Loss: 0.07175114750862122\n",
      "Step: 386, Loss: 0.07164701074361801\n",
      "Step: 387, Loss: 0.07184553146362305\n",
      "Step: 388, Loss: 0.07197358459234238\n",
      "Step: 389, Loss: 0.07177533209323883\n",
      "Step: 390, Loss: 0.07156349718570709\n",
      "Step: 391, Loss: 0.07170719653367996\n",
      "Step: 392, Loss: 0.07173838466405869\n",
      "Step: 393, Loss: 0.0718427300453186\n",
      "Step: 394, Loss: 0.07166454195976257\n",
      "Step: 395, Loss: 0.07156511396169662\n",
      "Step: 396, Loss: 0.07159122824668884\n",
      "Step: 397, Loss: 0.07118625938892365\n",
      "Step: 398, Loss: 0.07137738168239594\n",
      "Step: 399, Loss: 0.07113853096961975\n",
      "Step: 400, Loss: 0.07129965722560883\n",
      "Step: 401, Loss: 0.07125863432884216\n",
      "Step: 402, Loss: 0.0712113156914711\n",
      "Step: 403, Loss: 0.07127184420824051\n",
      "Step: 404, Loss: 0.07089558243751526\n",
      "Step: 405, Loss: 0.07101994007825851\n",
      "Step: 406, Loss: 0.07142122834920883\n",
      "Step: 407, Loss: 0.07123637944459915\n",
      "Step: 408, Loss: 0.07139880955219269\n",
      "Step: 409, Loss: 0.0713137537240982\n",
      "Step: 410, Loss: 0.07125122845172882\n",
      "Step: 411, Loss: 0.07119607925415039\n",
      "Step: 412, Loss: 0.07119955122470856\n",
      "Step: 413, Loss: 0.07127642631530762\n",
      "Step: 414, Loss: 0.0712132453918457\n",
      "Step: 415, Loss: 0.07126396894454956\n",
      "Step: 416, Loss: 0.07083813101053238\n",
      "Step: 417, Loss: 0.0713515356183052\n",
      "Step: 418, Loss: 0.07133052498102188\n",
      "Step: 419, Loss: 0.07116204500198364\n",
      "Step: 420, Loss: 0.07119881361722946\n",
      "Step: 421, Loss: 0.0712442621588707\n",
      "Step: 422, Loss: 0.07100951671600342\n",
      "Step: 423, Loss: 0.0709906741976738\n",
      "Step: 424, Loss: 0.07102638483047485\n",
      "Step: 425, Loss: 0.07108248770236969\n",
      "Step: 426, Loss: 0.07111208140850067\n",
      "Step: 427, Loss: 0.07111729681491852\n",
      "Step: 428, Loss: 0.07114620506763458\n",
      "Step: 429, Loss: 0.07121842354536057\n",
      "Step: 430, Loss: 0.07102295756340027\n",
      "Step: 431, Loss: 0.07127193361520767\n",
      "Step: 432, Loss: 0.07108239084482193\n",
      "Step: 433, Loss: 0.07113797217607498\n",
      "Step: 434, Loss: 0.07102710753679276\n",
      "Step: 435, Loss: 0.0708671286702156\n",
      "Step: 436, Loss: 0.07104277610778809\n",
      "Step: 437, Loss: 0.07085682451725006\n",
      "Step: 438, Loss: 0.07098984718322754\n",
      "Step: 439, Loss: 0.07104948908090591\n",
      "Step: 440, Loss: 0.07083052396774292\n",
      "Step: 441, Loss: 0.07062193006277084\n",
      "Step: 442, Loss: 0.07049503177404404\n",
      "Step: 443, Loss: 0.07109104841947556\n",
      "Step: 444, Loss: 0.07089566439390182\n",
      "Step: 445, Loss: 0.07079318910837173\n",
      "Step: 446, Loss: 0.07068514078855515\n",
      "Step: 447, Loss: 0.07104633003473282\n",
      "Step: 448, Loss: 0.07115743309259415\n",
      "Step: 449, Loss: 0.07055332511663437\n",
      "Step: 450, Loss: 0.07072622328996658\n",
      "Step: 451, Loss: 0.07082687318325043\n",
      "Step: 452, Loss: 0.07072083652019501\n",
      "Step: 453, Loss: 0.07081558555364609\n",
      "Step: 454, Loss: 0.07092846184968948\n",
      "Step: 455, Loss: 0.07086227834224701\n",
      "Step: 456, Loss: 0.07066813856363297\n",
      "Step: 457, Loss: 0.07094774395227432\n",
      "Step: 458, Loss: 0.07086169719696045\n",
      "Step: 459, Loss: 0.07065719366073608\n",
      "Step: 460, Loss: 0.07068225741386414\n",
      "Step: 461, Loss: 0.07063844054937363\n",
      "Step: 462, Loss: 0.07087588310241699\n",
      "Step: 463, Loss: 0.07086075097322464\n",
      "Step: 464, Loss: 0.07055806368589401\n",
      "Step: 465, Loss: 0.07056125998497009\n",
      "Step: 466, Loss: 0.07093502581119537\n",
      "Step: 467, Loss: 0.07061616331338882\n",
      "Step: 468, Loss: 0.07065968960523605\n",
      "Step: 469, Loss: 0.07054425030946732\n",
      "Step: 470, Loss: 0.0711103305220604\n",
      "Step: 471, Loss: 0.07078158855438232\n",
      "Step: 472, Loss: 0.0709553211927414\n",
      "Step: 473, Loss: 0.07098612934350967\n",
      "Step: 474, Loss: 0.07113415002822876\n",
      "Step: 475, Loss: 0.0715561956167221\n",
      "Step: 476, Loss: 0.07121307402849197\n",
      "Step: 477, Loss: 0.07167806476354599\n",
      "Step: 478, Loss: 0.07201153039932251\n",
      "Step: 479, Loss: 0.07188279926776886\n",
      "Step: 480, Loss: 0.07226929068565369\n",
      "Step: 481, Loss: 0.07200323790311813\n",
      "Step: 482, Loss: 0.07183521240949631\n",
      "Step: 483, Loss: 0.07188209146261215\n",
      "Step: 484, Loss: 0.0719970166683197\n",
      "Step: 485, Loss: 0.07172276079654694\n",
      "Step: 486, Loss: 0.07162171602249146\n",
      "Step: 487, Loss: 0.0715557336807251\n",
      "Step: 488, Loss: 0.07142504304647446\n",
      "Step: 489, Loss: 0.07119031250476837\n",
      "Step: 490, Loss: 0.0711820125579834\n",
      "Step: 491, Loss: 0.07117796689271927\n",
      "Step: 492, Loss: 0.07122129946947098\n",
      "Step: 493, Loss: 0.07129795849323273\n",
      "Step: 494, Loss: 0.07125206291675568\n",
      "Step: 495, Loss: 0.07148168236017227\n",
      "Step: 496, Loss: 0.07179661840200424\n",
      "Step: 497, Loss: 0.07154827564954758\n",
      "Step: 498, Loss: 0.0714205875992775\n",
      "Step: 499, Loss: 0.07160059362649918\n",
      "Step: 500, Loss: 0.07182732969522476\n",
      "Step: 501, Loss: 0.07178404182195663\n",
      "Step: 502, Loss: 0.07192791253328323\n",
      "Step: 503, Loss: 0.07180973142385483\n",
      "Step: 504, Loss: 0.07203412055969238\n",
      "Step: 505, Loss: 0.07193700969219208\n",
      "Step: 506, Loss: 0.0719597190618515\n",
      "Step: 507, Loss: 0.0722227469086647\n",
      "Step: 508, Loss: 0.07199564576148987\n",
      "Step: 509, Loss: 0.07177607715129852\n",
      "Step: 510, Loss: 0.07167202979326248\n",
      "Step: 511, Loss: 0.07188775390386581\n",
      "Step: 512, Loss: 0.07212522625923157\n",
      "Step: 513, Loss: 0.07171551883220673\n",
      "Step: 514, Loss: 0.07163041085004807\n",
      "Step: 515, Loss: 0.07194551080465317\n",
      "Step: 516, Loss: 0.07199185341596603\n",
      "Step: 517, Loss: 0.07183060050010681\n",
      "Step: 518, Loss: 0.07155652344226837\n",
      "Step: 519, Loss: 0.07174813747406006\n",
      "Step: 520, Loss: 0.07194823026657104\n",
      "Step: 521, Loss: 0.07164772599935532\n",
      "Step: 522, Loss: 0.07202383875846863\n",
      "Step: 523, Loss: 0.07202505320310593\n",
      "Step: 524, Loss: 0.07191380858421326\n",
      "Step: 525, Loss: 0.07181176543235779\n",
      "Step: 526, Loss: 0.07202965021133423\n",
      "Step: 527, Loss: 0.0719558447599411\n",
      "Step: 528, Loss: 0.07167326658964157\n",
      "Step: 529, Loss: 0.07232868671417236\n",
      "Step: 530, Loss: 0.07202386111021042\n",
      "Step: 531, Loss: 0.07200247049331665\n",
      "Step: 532, Loss: 0.0720134973526001\n",
      "Step: 533, Loss: 0.07216107100248337\n",
      "Step: 534, Loss: 0.07205028086900711\n",
      "Step: 535, Loss: 0.07197588682174683\n",
      "Step: 536, Loss: 0.0719989538192749\n",
      "Step: 537, Loss: 0.07187309861183167\n",
      "Step: 538, Loss: 0.07183016091585159\n",
      "Step: 539, Loss: 0.07197089493274689\n",
      "Step: 540, Loss: 0.07203862071037292\n",
      "Step: 541, Loss: 0.07198891788721085\n",
      "Step: 542, Loss: 0.07212160527706146\n",
      "Step: 543, Loss: 0.07187185436487198\n",
      "Step: 544, Loss: 0.07193399220705032\n",
      "Step: 545, Loss: 0.0718424990773201\n",
      "Step: 546, Loss: 0.07170919328927994\n",
      "Step: 547, Loss: 0.0720343068242073\n",
      "Step: 548, Loss: 0.07179956138134003\n",
      "Step: 549, Loss: 0.07220163196325302\n",
      "Step: 550, Loss: 0.07214872539043427\n",
      "Step: 551, Loss: 0.07199915498495102\n",
      "Step: 552, Loss: 0.07214511185884476\n",
      "Step: 553, Loss: 0.07220347225666046\n",
      "Step: 554, Loss: 0.07198476791381836\n",
      "Step: 555, Loss: 0.07207922637462616\n",
      "Step: 556, Loss: 0.07222959399223328\n",
      "Step: 557, Loss: 0.07186825573444366\n",
      "Step: 558, Loss: 0.07225323468446732\n",
      "Step: 559, Loss: 0.07202092558145523\n",
      "Step: 560, Loss: 0.07200047373771667\n",
      "Step: 561, Loss: 0.07175212353467941\n",
      "Step: 562, Loss: 0.07209435850381851\n",
      "Step: 563, Loss: 0.07169203460216522\n",
      "Step: 564, Loss: 0.07197011262178421\n",
      "Step: 565, Loss: 0.07204493880271912\n",
      "Step: 566, Loss: 0.07198963314294815\n",
      "Step: 567, Loss: 0.07229526340961456\n",
      "Step: 568, Loss: 0.07180128246545792\n",
      "Step: 569, Loss: 0.0721081793308258\n",
      "Step: 570, Loss: 0.0718064159154892\n",
      "Step: 571, Loss: 0.0722515881061554\n",
      "Step: 572, Loss: 0.07182972878217697\n",
      "Step: 573, Loss: 0.07197275757789612\n",
      "Step: 574, Loss: 0.07196692377328873\n",
      "Step: 575, Loss: 0.07146767526865005\n",
      "Step: 576, Loss: 0.07192672789096832\n",
      "Step: 577, Loss: 0.07237807661294937\n",
      "Step: 578, Loss: 0.07183954864740372\n",
      "Step: 579, Loss: 0.07208014279603958\n",
      "Step: 580, Loss: 0.07165446132421494\n",
      "Step: 581, Loss: 0.07221884280443192\n",
      "Step: 582, Loss: 0.07198620587587357\n",
      "Step: 583, Loss: 0.07208395004272461\n",
      "Step: 584, Loss: 0.07205749303102493\n",
      "Step: 585, Loss: 0.07228227704763412\n",
      "Step: 586, Loss: 0.07168637961149216\n",
      "Step: 587, Loss: 0.07225413620471954\n",
      "Step: 588, Loss: 0.0721217468380928\n",
      "Step: 589, Loss: 0.07204234600067139\n",
      "Step: 590, Loss: 0.07189439982175827\n",
      "Step: 591, Loss: 0.07219758629798889\n",
      "Step: 592, Loss: 0.07212703675031662\n",
      "Step: 593, Loss: 0.07195982336997986\n",
      "Step: 594, Loss: 0.07193721830844879\n",
      "Step: 595, Loss: 0.07218139618635178\n",
      "Step: 596, Loss: 0.0718739852309227\n",
      "Step: 597, Loss: 0.07179490476846695\n",
      "Step: 598, Loss: 0.07223261147737503\n",
      "Step: 599, Loss: 0.07169781625270844\n",
      "Final loss:  0.071697816\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/mixturelinear_tn_gev_twcrps_mean13.0_std1.5_constant0.029999999329447746_epochs600.pkl\n"
     ]
    }
   ],
   "source": [
    "model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    parameter_names,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_constant = chain_function_constant,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOS Model Information:\n",
      "Loss function: loss_twCRPS_sample (Samples: 200)\n",
      "Forecast distribution: distr_mixture\n",
      "Distribution 1: distr_trunc_normal\n",
      "Distribution 2: distr_gev\n",
      "Mixture weight: [0.6714824]\n",
      "Parameters:\n",
      "  weight: [0.6714824]\n",
      "  a_tn: [0.8587427]\n",
      "  b_tn: [ 0.9033643  -0.8743947  -0.10445029 -0.27649736  0.85969   ]\n",
      "  c_tn: [1.8735183]\n",
      "  d_tn: [0.7428804]\n",
      "  a_gev: [-0.4236689]\n",
      "  b_gev: [ 0.9333376   0.5409048  -0.3650169   0.3564078   0.61552596]\n",
      "  c_gev: [1.4305779]\n",
      "  d_gev: [ 0.09890767 -0.5423227   0.16434298  0.31506124 -0.69037396]\n",
      "  e_gev: [-0.31247506]\n",
      "Features: wind_speed, press, kinetic, humid, geopot\n",
      "Number of features: 5\n",
      "Neighbourhood size: 11\n",
      "Chaining function: chain_function_normal_cdf (Mean: 13.0, Std: 4.0)\n",
      "Optimizer: Adam\n",
      "Learning rate: 0.009999999776482582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(setup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
