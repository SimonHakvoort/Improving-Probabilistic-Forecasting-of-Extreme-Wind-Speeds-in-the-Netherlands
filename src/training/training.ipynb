{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 11:39:00.702219: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-12 11:39:00.729434: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-12 11:39:00.729453: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-12 11:39:00.730133: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-12 11:39:00.734484: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-12 11:39:00.734960: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-12 11:39:06.241234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.training.training import train_model, train_and_save, load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_distribution = 'distr_mixture_linear'\n",
    "distribution_1 = 'distr_trunc_normal'\n",
    "distribution_2 = 'distr_gev'\n",
    "\n",
    "loss = 'loss_twCRPS_sample' # options: loss_CRPS_sample, loss_twCRPS_sample, loss_log_likelihood\n",
    "\n",
    "chain_function = 'chain_function_normal_cdf_plus_constant' # options: chain_function_normal_cdf, chain_function_indicator, chain_function_normal_cdf_plus_constant\n",
    "chain_function_mean = 12\n",
    "chain_function_std = 1\n",
    "chain_function_threshold = 10 # 12 / 15\n",
    "chain_function_constant = 0.4\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.03\n",
    "folds = [1,2]\n",
    "neighbourhood_size = 11\n",
    "ignore = ['229', '285', '323']\n",
    "epochs = 600\n",
    "\n",
    "samples = 100\n",
    "printing = True\n",
    "pretrained = True\n",
    "random_init = False\n",
    "\n",
    "#parameters = {'weight_a': tf.constant([1.6], dtype=tf.float32), 'weight_b': tf.constant([-0.2], dtype=tf.float32)}\n",
    "\n",
    "all_features = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "\n",
    "location_features = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "\n",
    "scale_features = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Final loss:  0.4238337\n",
      "Final loss:  0.4295263\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using given parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 0.4251396656036377\n",
      "Step: 1, Loss: 0.42739489674568176\n",
      "Step: 2, Loss: 0.4252251982688904\n",
      "Step: 3, Loss: 0.4263163208961487\n",
      "Step: 4, Loss: 0.42416417598724365\n",
      "Step: 5, Loss: 0.4242735505104065\n",
      "Step: 6, Loss: 0.4252910614013672\n",
      "Step: 7, Loss: 0.42416754364967346\n",
      "Step: 8, Loss: 0.42374613881111145\n",
      "Step: 9, Loss: 0.4250827431678772\n",
      "Step: 10, Loss: 0.4240795969963074\n",
      "Step: 11, Loss: 0.4236934781074524\n",
      "Step: 12, Loss: 0.4235028624534607\n",
      "Step: 13, Loss: 0.4243316054344177\n",
      "Step: 14, Loss: 0.4239499866962433\n",
      "Step: 15, Loss: 0.42319154739379883\n",
      "Step: 16, Loss: 0.42390984296798706\n",
      "Step: 17, Loss: 0.4230411946773529\n",
      "Step: 18, Loss: 0.42311033606529236\n",
      "Step: 19, Loss: 0.42288938164711\n",
      "Step: 20, Loss: 0.4244249165058136\n",
      "Step: 21, Loss: 0.4232160151004791\n",
      "Step: 22, Loss: 0.4233180582523346\n",
      "Step: 23, Loss: 0.42272308468818665\n",
      "Step: 24, Loss: 0.4232163429260254\n",
      "Step: 25, Loss: 0.423738032579422\n",
      "Step: 26, Loss: 0.4228622317314148\n",
      "Step: 27, Loss: 0.42325010895729065\n",
      "Step: 28, Loss: 0.42339757084846497\n",
      "Step: 29, Loss: 0.42360198497772217\n",
      "Step: 30, Loss: 0.42338451743125916\n",
      "Step: 31, Loss: 0.42365455627441406\n",
      "Step: 32, Loss: 0.4235813319683075\n",
      "Step: 33, Loss: 0.423424631357193\n",
      "Step: 34, Loss: 0.4240211844444275\n",
      "Step: 35, Loss: 0.4229535758495331\n",
      "Step: 36, Loss: 0.42376360297203064\n",
      "Step: 37, Loss: 0.42392081022262573\n",
      "Step: 38, Loss: 0.4234846234321594\n",
      "Step: 39, Loss: 0.4232325553894043\n",
      "Step: 40, Loss: 0.4229167699813843\n",
      "Step: 41, Loss: 0.4242711067199707\n",
      "Step: 42, Loss: 0.4231196343898773\n",
      "Step: 43, Loss: 0.422871857881546\n",
      "Step: 44, Loss: 0.4223460853099823\n",
      "Step: 45, Loss: 0.42281439900398254\n",
      "Step: 46, Loss: 0.4228346347808838\n",
      "Step: 47, Loss: 0.4230247139930725\n",
      "Step: 48, Loss: 0.4232019782066345\n",
      "Step: 49, Loss: 0.42283234000205994\n",
      "Step: 50, Loss: 0.4231559634208679\n",
      "Step: 51, Loss: 0.4228537380695343\n",
      "Step: 52, Loss: 0.4233584702014923\n",
      "Step: 53, Loss: 0.42302146553993225\n",
      "Step: 54, Loss: 0.42327073216438293\n",
      "Step: 55, Loss: 0.4228936433792114\n",
      "Step: 56, Loss: 0.4231361746788025\n",
      "Step: 57, Loss: 0.42321187257766724\n",
      "Step: 58, Loss: 0.4232672452926636\n",
      "Step: 59, Loss: 0.4230343997478485\n",
      "Step: 60, Loss: 0.42218756675720215\n",
      "Step: 61, Loss: 0.4226875603199005\n",
      "Step: 62, Loss: 0.4224728047847748\n",
      "Step: 63, Loss: 0.42227697372436523\n",
      "Step: 64, Loss: 0.42157208919525146\n",
      "Step: 65, Loss: 0.4214843511581421\n",
      "Step: 66, Loss: 0.4215017259120941\n",
      "Step: 67, Loss: 0.4214639663696289\n",
      "Step: 68, Loss: 0.4216993749141693\n",
      "Step: 69, Loss: 0.42168113589286804\n",
      "Step: 70, Loss: 0.421103835105896\n",
      "Step: 71, Loss: 0.42184504866600037\n",
      "Step: 72, Loss: 0.42188921570777893\n",
      "Step: 73, Loss: 0.42239710688591003\n",
      "Step: 74, Loss: 0.4209192097187042\n",
      "Step: 75, Loss: 0.42116332054138184\n",
      "Step: 76, Loss: 0.42153841257095337\n",
      "Step: 77, Loss: 0.4217742383480072\n",
      "Step: 78, Loss: 0.4221888780593872\n",
      "Step: 79, Loss: 0.42132508754730225\n",
      "Step: 80, Loss: 0.4221895933151245\n",
      "Step: 81, Loss: 0.42184847593307495\n",
      "Step: 82, Loss: 0.4221382141113281\n",
      "Step: 83, Loss: 0.42187079787254333\n",
      "Step: 84, Loss: 0.42156437039375305\n",
      "Step: 85, Loss: 0.42104867100715637\n",
      "Step: 86, Loss: 0.4209788143634796\n",
      "Step: 87, Loss: 0.4212551414966583\n",
      "Step: 88, Loss: 0.4213680028915405\n",
      "Step: 89, Loss: 0.4210711419582367\n",
      "Step: 90, Loss: 0.42133828997612\n",
      "Step: 91, Loss: 0.4206571877002716\n",
      "Step: 92, Loss: 0.421905517578125\n",
      "Step: 93, Loss: 0.4210924208164215\n",
      "Step: 94, Loss: 0.4208049774169922\n",
      "Step: 95, Loss: 0.42250651121139526\n",
      "Step: 96, Loss: 0.4209188222885132\n",
      "Step: 97, Loss: 0.4213894307613373\n",
      "Step: 98, Loss: 0.4215725064277649\n",
      "Step: 99, Loss: 0.42108675837516785\n",
      "Step: 100, Loss: 0.4220421016216278\n",
      "Step: 101, Loss: 0.4208592474460602\n",
      "Step: 102, Loss: 0.4210067093372345\n",
      "Step: 103, Loss: 0.42118915915489197\n",
      "Step: 104, Loss: 0.42121613025665283\n",
      "Step: 105, Loss: 0.42013534903526306\n",
      "Step: 106, Loss: 0.4227839410305023\n",
      "Step: 107, Loss: 0.42100200057029724\n",
      "Step: 108, Loss: 0.42020055651664734\n",
      "Step: 109, Loss: 0.4218679666519165\n",
      "Step: 110, Loss: 0.42219388484954834\n",
      "Step: 111, Loss: 0.4210440516471863\n",
      "Step: 112, Loss: 0.4211127758026123\n",
      "Step: 113, Loss: 0.42266905307769775\n",
      "Step: 114, Loss: 0.4219949245452881\n",
      "Step: 115, Loss: 0.4211015999317169\n",
      "Step: 116, Loss: 0.42031964659690857\n",
      "Step: 117, Loss: 0.42083296179771423\n",
      "Step: 118, Loss: 0.42161959409713745\n",
      "Step: 119, Loss: 0.42011427879333496\n",
      "Step: 120, Loss: 0.42091572284698486\n",
      "Step: 121, Loss: 0.4202502965927124\n",
      "Step: 122, Loss: 0.4206376373767853\n",
      "Step: 123, Loss: 0.42135345935821533\n",
      "Step: 124, Loss: 0.42142510414123535\n",
      "Step: 125, Loss: 0.42179492115974426\n",
      "Step: 126, Loss: 0.42105919122695923\n",
      "Step: 127, Loss: 0.42003265023231506\n",
      "Step: 128, Loss: 0.42126932740211487\n",
      "Step: 129, Loss: 0.4217017889022827\n",
      "Step: 130, Loss: 0.42164260149002075\n",
      "Step: 131, Loss: 0.4209713637828827\n",
      "Step: 132, Loss: 0.4205080270767212\n",
      "Step: 133, Loss: 0.42051491141319275\n",
      "Step: 134, Loss: 0.42263734340667725\n",
      "Step: 135, Loss: 0.4223366677761078\n",
      "Step: 136, Loss: 0.42175689339637756\n",
      "Step: 137, Loss: 0.42109763622283936\n",
      "Step: 138, Loss: 0.4217061400413513\n",
      "Step: 139, Loss: 0.42043745517730713\n",
      "Step: 140, Loss: 0.42101889848709106\n",
      "Step: 141, Loss: 0.4211580753326416\n",
      "Step: 142, Loss: 0.4215206801891327\n",
      "Step: 143, Loss: 0.42158499360084534\n",
      "Step: 144, Loss: 0.4214603006839752\n",
      "Step: 145, Loss: 0.42066705226898193\n",
      "Step: 146, Loss: 0.42195847630500793\n",
      "Step: 147, Loss: 0.4208242893218994\n",
      "Step: 148, Loss: 0.42107316851615906\n",
      "Step: 149, Loss: 0.420518159866333\n",
      "Step: 150, Loss: 0.4206327497959137\n",
      "Step: 151, Loss: 0.4209252595901489\n",
      "Step: 152, Loss: 0.42113012075424194\n",
      "Step: 153, Loss: 0.42015254497528076\n",
      "Step: 154, Loss: 0.42174094915390015\n",
      "Step: 155, Loss: 0.42047083377838135\n",
      "Step: 156, Loss: 0.4212728440761566\n",
      "Step: 157, Loss: 0.4205615818500519\n",
      "Step: 158, Loss: 0.42138129472732544\n",
      "Step: 159, Loss: 0.42034175992012024\n",
      "Step: 160, Loss: 0.4214601218700409\n",
      "Step: 161, Loss: 0.42160871624946594\n",
      "Step: 162, Loss: 0.4205445647239685\n",
      "Step: 163, Loss: 0.42063581943511963\n",
      "Step: 164, Loss: 0.42063334584236145\n",
      "Step: 165, Loss: 0.4199599027633667\n",
      "Step: 166, Loss: 0.4209509491920471\n",
      "Step: 167, Loss: 0.420878142118454\n",
      "Step: 168, Loss: 0.420827180147171\n",
      "Step: 169, Loss: 0.4200016260147095\n",
      "Step: 170, Loss: 0.4222119450569153\n",
      "Step: 171, Loss: 0.4211156368255615\n",
      "Step: 172, Loss: 0.4200339913368225\n",
      "Step: 173, Loss: 0.4208977222442627\n",
      "Step: 174, Loss: 0.4210071563720703\n",
      "Step: 175, Loss: 0.42184504866600037\n",
      "Step: 176, Loss: 0.42053768038749695\n",
      "Step: 177, Loss: 0.4207906424999237\n",
      "Step: 178, Loss: 0.4202008545398712\n",
      "Step: 179, Loss: 0.42047300934791565\n",
      "Step: 180, Loss: 0.4213743507862091\n",
      "Step: 181, Loss: 0.4209427535533905\n",
      "Step: 182, Loss: 0.4207669794559479\n",
      "Step: 183, Loss: 0.4204629063606262\n",
      "Step: 184, Loss: 0.42087322473526\n",
      "Step: 185, Loss: 0.4212506115436554\n",
      "Step: 186, Loss: 0.42087703943252563\n",
      "Step: 187, Loss: 0.42091310024261475\n",
      "Step: 188, Loss: 0.42104774713516235\n",
      "Step: 189, Loss: 0.42125359177589417\n",
      "Step: 190, Loss: 0.42125770449638367\n",
      "Step: 191, Loss: 0.42031043767929077\n",
      "Step: 192, Loss: 0.42084285616874695\n",
      "Step: 193, Loss: 0.41985681653022766\n",
      "Step: 194, Loss: 0.4222082495689392\n",
      "Step: 195, Loss: 0.41953569650650024\n",
      "Step: 196, Loss: 0.4201523959636688\n",
      "Step: 197, Loss: 0.4201609790325165\n",
      "Step: 198, Loss: 0.4208911955356598\n",
      "Step: 199, Loss: 0.42081761360168457\n",
      "Step: 200, Loss: 0.42088478803634644\n",
      "Step: 201, Loss: 0.4209394156932831\n",
      "Step: 202, Loss: 0.42065176367759705\n",
      "Step: 203, Loss: 0.4216121733188629\n",
      "Step: 204, Loss: 0.4216185510158539\n",
      "Step: 205, Loss: 0.419900506734848\n",
      "Step: 206, Loss: 0.42136046290397644\n",
      "Step: 207, Loss: 0.421421617269516\n",
      "Step: 208, Loss: 0.42136722803115845\n",
      "Step: 209, Loss: 0.4213942885398865\n",
      "Step: 210, Loss: 0.42055028676986694\n",
      "Step: 211, Loss: 0.42084139585494995\n",
      "Step: 212, Loss: 0.4214029908180237\n",
      "Step: 213, Loss: 0.4211711287498474\n",
      "Step: 214, Loss: 0.4207862317562103\n",
      "Step: 215, Loss: 0.42082640528678894\n",
      "Step: 216, Loss: 0.4203479290008545\n",
      "Step: 217, Loss: 0.4203484356403351\n",
      "Step: 218, Loss: 0.4208625257015228\n",
      "Step: 219, Loss: 0.4203709065914154\n",
      "Step: 220, Loss: 0.4209739863872528\n",
      "Step: 221, Loss: 0.4209269881248474\n",
      "Step: 222, Loss: 0.4203253388404846\n",
      "Step: 223, Loss: 0.4203544557094574\n",
      "Step: 224, Loss: 0.4203838109970093\n",
      "Step: 225, Loss: 0.42063280940055847\n",
      "Step: 226, Loss: 0.42069074511528015\n",
      "Step: 227, Loss: 0.4202238917350769\n",
      "Step: 228, Loss: 0.4208377003669739\n",
      "Step: 229, Loss: 0.4209105968475342\n",
      "Step: 230, Loss: 0.42037433385849\n",
      "Step: 231, Loss: 0.4203762710094452\n",
      "Step: 232, Loss: 0.42048874497413635\n",
      "Step: 233, Loss: 0.4217984080314636\n",
      "Step: 234, Loss: 0.4200669527053833\n",
      "Step: 235, Loss: 0.42043161392211914\n",
      "Step: 236, Loss: 0.41973933577537537\n",
      "Step: 237, Loss: 0.4212122857570648\n",
      "Step: 238, Loss: 0.420291543006897\n",
      "Step: 239, Loss: 0.42099523544311523\n",
      "Step: 240, Loss: 0.4206656217575073\n",
      "Step: 241, Loss: 0.4193607270717621\n",
      "Step: 242, Loss: 0.42082440853118896\n",
      "Step: 243, Loss: 0.4207444489002228\n",
      "Step: 244, Loss: 0.42135441303253174\n",
      "Step: 245, Loss: 0.4200538396835327\n",
      "Step: 246, Loss: 0.42064353823661804\n",
      "Step: 247, Loss: 0.4214257299900055\n",
      "Step: 248, Loss: 0.419220507144928\n",
      "Step: 249, Loss: 0.4198773503303528\n",
      "Step: 250, Loss: 0.42056334018707275\n",
      "Step: 251, Loss: 0.4200662672519684\n",
      "Step: 252, Loss: 0.42003604769706726\n",
      "Step: 253, Loss: 0.42032697796821594\n",
      "Step: 254, Loss: 0.42115455865859985\n",
      "Step: 255, Loss: 0.4213159680366516\n",
      "Step: 256, Loss: 0.42127472162246704\n",
      "Step: 257, Loss: 0.4211852252483368\n",
      "Step: 258, Loss: 0.4217023551464081\n",
      "Step: 259, Loss: 0.4198404550552368\n",
      "Step: 260, Loss: 0.42024660110473633\n",
      "Step: 261, Loss: 0.4204629063606262\n",
      "Step: 262, Loss: 0.4201706647872925\n",
      "Step: 263, Loss: 0.4209464192390442\n",
      "Step: 264, Loss: 0.41980716586112976\n",
      "Step: 265, Loss: 0.41923728585243225\n",
      "Step: 266, Loss: 0.42054447531700134\n",
      "Step: 267, Loss: 0.41946348547935486\n",
      "Step: 268, Loss: 0.42033442854881287\n",
      "Step: 269, Loss: 0.42055219411849976\n",
      "Step: 270, Loss: 0.42078423500061035\n",
      "Step: 271, Loss: 0.4200774133205414\n",
      "Step: 272, Loss: 0.42052289843559265\n",
      "Step: 273, Loss: 0.4208909273147583\n",
      "Step: 274, Loss: 0.4211733639240265\n",
      "Step: 275, Loss: 0.4197573661804199\n",
      "Step: 276, Loss: 0.4205849766731262\n",
      "Step: 277, Loss: 0.4193684160709381\n",
      "Step: 278, Loss: 0.42060452699661255\n",
      "Step: 279, Loss: 0.42016473412513733\n",
      "Step: 280, Loss: 0.4204866290092468\n",
      "Step: 281, Loss: 0.41972649097442627\n",
      "Step: 282, Loss: 0.42125436663627625\n",
      "Step: 283, Loss: 0.4204127788543701\n",
      "Step: 284, Loss: 0.42073854804039\n",
      "Step: 285, Loss: 0.42125117778778076\n",
      "Step: 286, Loss: 0.4203178584575653\n",
      "Step: 287, Loss: 0.42015495896339417\n",
      "Step: 288, Loss: 0.42044126987457275\n",
      "Step: 289, Loss: 0.41980159282684326\n",
      "Step: 290, Loss: 0.420370489358902\n",
      "Step: 291, Loss: 0.4203442633152008\n",
      "Step: 292, Loss: 0.41970857977867126\n",
      "Step: 293, Loss: 0.4206586182117462\n",
      "Step: 294, Loss: 0.420848548412323\n",
      "Step: 295, Loss: 0.4211079776287079\n",
      "Step: 296, Loss: 0.41978901624679565\n",
      "Step: 297, Loss: 0.4203251302242279\n",
      "Step: 298, Loss: 0.42009657621383667\n",
      "Step: 299, Loss: 0.4208410680294037\n",
      "Step: 300, Loss: 0.42063984274864197\n",
      "Step: 301, Loss: 0.4220205843448639\n",
      "Step: 302, Loss: 0.4204813539981842\n",
      "Step: 303, Loss: 0.4209230840206146\n",
      "Step: 304, Loss: 0.42099300026893616\n",
      "Step: 305, Loss: 0.42112892866134644\n",
      "Step: 306, Loss: 0.42017143964767456\n",
      "Step: 307, Loss: 0.4198766052722931\n",
      "Step: 308, Loss: 0.4205026924610138\n",
      "Step: 309, Loss: 0.42032352089881897\n",
      "Step: 310, Loss: 0.4208212196826935\n",
      "Step: 311, Loss: 0.42075315117836\n",
      "Step: 312, Loss: 0.41946670413017273\n",
      "Step: 313, Loss: 0.41976773738861084\n",
      "Step: 314, Loss: 0.42081308364868164\n",
      "Step: 315, Loss: 0.420151948928833\n",
      "Step: 316, Loss: 0.42067307233810425\n",
      "Step: 317, Loss: 0.4205966591835022\n",
      "Step: 318, Loss: 0.42058172821998596\n",
      "Step: 319, Loss: 0.42000555992126465\n",
      "Step: 320, Loss: 0.4205523133277893\n",
      "Step: 321, Loss: 0.4192206859588623\n",
      "Step: 322, Loss: 0.42080944776535034\n",
      "Step: 323, Loss: 0.4204092025756836\n",
      "Step: 324, Loss: 0.42131349444389343\n",
      "Step: 325, Loss: 0.4201928973197937\n",
      "Step: 326, Loss: 0.4205547869205475\n",
      "Step: 327, Loss: 0.42110422253608704\n",
      "Step: 328, Loss: 0.4201847016811371\n",
      "Step: 329, Loss: 0.4200260639190674\n",
      "Step: 330, Loss: 0.4208366572856903\n",
      "Step: 331, Loss: 0.4200160503387451\n",
      "Step: 332, Loss: 0.41908255219459534\n",
      "Step: 333, Loss: 0.42120060324668884\n",
      "Step: 334, Loss: 0.42107293009757996\n",
      "Step: 335, Loss: 0.42011651396751404\n",
      "Step: 336, Loss: 0.4202439486980438\n",
      "Step: 337, Loss: 0.4208860695362091\n",
      "Step: 338, Loss: 0.42068472504615784\n",
      "Step: 339, Loss: 0.4211999773979187\n",
      "Step: 340, Loss: 0.420602411031723\n",
      "Step: 341, Loss: 0.42045798897743225\n",
      "Step: 342, Loss: 0.42027902603149414\n",
      "Step: 343, Loss: 0.4197947382926941\n",
      "Step: 344, Loss: 0.4201910197734833\n",
      "Step: 345, Loss: 0.42078647017478943\n",
      "Step: 346, Loss: 0.4201370179653168\n",
      "Step: 347, Loss: 0.420883446931839\n",
      "Step: 348, Loss: 0.41998574137687683\n",
      "Step: 349, Loss: 0.42002248764038086\n",
      "Step: 350, Loss: 0.4208260178565979\n",
      "Step: 351, Loss: 0.4198281466960907\n",
      "Step: 352, Loss: 0.42038530111312866\n",
      "Step: 353, Loss: 0.42017367482185364\n",
      "Step: 354, Loss: 0.4211026728153229\n",
      "Step: 355, Loss: 0.42040055990219116\n",
      "Step: 356, Loss: 0.41981711983680725\n",
      "Step: 357, Loss: 0.42003536224365234\n",
      "Step: 358, Loss: 0.42120853066444397\n",
      "Step: 359, Loss: 0.4201895296573639\n",
      "Step: 360, Loss: 0.419793039560318\n",
      "Step: 361, Loss: 0.42088449001312256\n",
      "Step: 362, Loss: 0.4201202094554901\n",
      "Step: 363, Loss: 0.4211389124393463\n",
      "Step: 364, Loss: 0.4201545715332031\n",
      "Step: 365, Loss: 0.41956210136413574\n",
      "Step: 366, Loss: 0.41970160603523254\n",
      "Step: 367, Loss: 0.4196574091911316\n",
      "Step: 368, Loss: 0.42069128155708313\n",
      "Step: 369, Loss: 0.42056235671043396\n",
      "Step: 370, Loss: 0.420224130153656\n",
      "Step: 371, Loss: 0.42076680064201355\n",
      "Step: 372, Loss: 0.42039862275123596\n",
      "Step: 373, Loss: 0.42039623856544495\n",
      "Step: 374, Loss: 0.4196236729621887\n",
      "Step: 375, Loss: 0.4206399917602539\n",
      "Step: 376, Loss: 0.42082005739212036\n",
      "Step: 377, Loss: 0.42066192626953125\n",
      "Step: 378, Loss: 0.4201721251010895\n",
      "Step: 379, Loss: 0.4203248620033264\n",
      "Step: 380, Loss: 0.41992852091789246\n",
      "Step: 381, Loss: 0.4204140901565552\n",
      "Step: 382, Loss: 0.4204598367214203\n",
      "Step: 383, Loss: 0.4204937219619751\n",
      "Step: 384, Loss: 0.4195221960544586\n",
      "Step: 385, Loss: 0.42035919427871704\n",
      "Step: 386, Loss: 0.4197673499584198\n",
      "Step: 387, Loss: 0.41980862617492676\n",
      "Step: 388, Loss: 0.4212549030780792\n",
      "Step: 389, Loss: 0.41981759667396545\n",
      "Step: 390, Loss: 0.4203822612762451\n",
      "Step: 391, Loss: 0.41897496581077576\n",
      "Step: 392, Loss: 0.42056602239608765\n",
      "Step: 393, Loss: 0.41958358883857727\n",
      "Step: 394, Loss: 0.42045947909355164\n",
      "Step: 395, Loss: 0.420354425907135\n",
      "Step: 396, Loss: 0.42028936743736267\n",
      "Step: 397, Loss: 0.42083561420440674\n",
      "Step: 398, Loss: 0.4211787283420563\n",
      "Step: 399, Loss: 0.4197458326816559\n",
      "Step: 400, Loss: 0.42090415954589844\n",
      "Step: 401, Loss: 0.42077499628067017\n",
      "Step: 402, Loss: 0.4196633994579315\n",
      "Step: 403, Loss: 0.4199560880661011\n",
      "Step: 404, Loss: 0.4197438359260559\n",
      "Step: 405, Loss: 0.4198402762413025\n",
      "Step: 406, Loss: 0.42025306820869446\n",
      "Step: 407, Loss: 0.4206833839416504\n",
      "Step: 408, Loss: 0.42138057947158813\n",
      "Step: 409, Loss: 0.42022472620010376\n",
      "Step: 410, Loss: 0.420740008354187\n",
      "Step: 411, Loss: 0.41990354657173157\n",
      "Step: 412, Loss: 0.42066118121147156\n",
      "Step: 413, Loss: 0.4203757345676422\n",
      "Step: 414, Loss: 0.4197520613670349\n",
      "Step: 415, Loss: 0.42050597071647644\n",
      "Step: 416, Loss: 0.4202483892440796\n",
      "Step: 417, Loss: 0.42096564173698425\n",
      "Step: 418, Loss: 0.4201899766921997\n",
      "Step: 419, Loss: 0.42061397433280945\n",
      "Step: 420, Loss: 0.4204915463924408\n",
      "Step: 421, Loss: 0.41970905661582947\n",
      "Step: 422, Loss: 0.4199467599391937\n",
      "Step: 423, Loss: 0.420169472694397\n",
      "Step: 424, Loss: 0.42062869668006897\n",
      "Step: 425, Loss: 0.4201216697692871\n",
      "Step: 426, Loss: 0.4203512966632843\n",
      "Step: 427, Loss: 0.41988950967788696\n",
      "Step: 428, Loss: 0.4210047721862793\n",
      "Step: 429, Loss: 0.42044797539711\n",
      "Step: 430, Loss: 0.4205300509929657\n",
      "Step: 431, Loss: 0.4204888641834259\n",
      "Step: 432, Loss: 0.4196639657020569\n",
      "Step: 433, Loss: 0.4200686812400818\n",
      "Step: 434, Loss: 0.42045655846595764\n",
      "Step: 435, Loss: 0.42015907168388367\n",
      "Step: 436, Loss: 0.4200787842273712\n",
      "Step: 437, Loss: 0.4207041561603546\n",
      "Step: 438, Loss: 0.41973719000816345\n",
      "Step: 439, Loss: 0.4201579689979553\n",
      "Step: 440, Loss: 0.4202919602394104\n",
      "Step: 441, Loss: 0.41972464323043823\n",
      "Step: 442, Loss: 0.4197898507118225\n",
      "Step: 443, Loss: 0.42100197076797485\n",
      "Step: 444, Loss: 0.42067646980285645\n",
      "Step: 445, Loss: 0.41975390911102295\n",
      "Step: 446, Loss: 0.42060568928718567\n",
      "Step: 447, Loss: 0.4204517602920532\n",
      "Step: 448, Loss: 0.41988176107406616\n",
      "Step: 449, Loss: 0.42018112540245056\n",
      "Step: 450, Loss: 0.4203065037727356\n",
      "Step: 451, Loss: 0.42167502641677856\n",
      "Step: 452, Loss: 0.4198555648326874\n",
      "Step: 453, Loss: 0.4212479293346405\n",
      "Step: 454, Loss: 0.4200459122657776\n",
      "Step: 455, Loss: 0.41955143213272095\n",
      "Step: 456, Loss: 0.41962382197380066\n",
      "Step: 457, Loss: 0.42021021246910095\n",
      "Step: 458, Loss: 0.42074909806251526\n",
      "Step: 459, Loss: 0.41993024945259094\n",
      "Step: 460, Loss: 0.4200390577316284\n",
      "Step: 461, Loss: 0.4191323518753052\n",
      "Step: 462, Loss: 0.42030349373817444\n",
      "Step: 463, Loss: 0.4200252890586853\n",
      "Step: 464, Loss: 0.4194941818714142\n",
      "Step: 465, Loss: 0.42039310932159424\n",
      "Step: 466, Loss: 0.42110785841941833\n",
      "Step: 467, Loss: 0.4198957085609436\n",
      "Step: 468, Loss: 0.41998064517974854\n",
      "Step: 469, Loss: 0.4196334481239319\n",
      "Step: 470, Loss: 0.41907864809036255\n",
      "Step: 471, Loss: 0.4200632572174072\n",
      "Step: 472, Loss: 0.41963136196136475\n",
      "Step: 473, Loss: 0.42042219638824463\n",
      "Step: 474, Loss: 0.42124035954475403\n",
      "Step: 475, Loss: 0.42035508155822754\n",
      "Step: 476, Loss: 0.4205482602119446\n",
      "Step: 477, Loss: 0.42024365067481995\n",
      "Step: 478, Loss: 0.42107439041137695\n",
      "Step: 479, Loss: 0.4206490218639374\n",
      "Step: 480, Loss: 0.42059043049812317\n",
      "Step: 481, Loss: 0.41964787244796753\n",
      "Step: 482, Loss: 0.41955873370170593\n",
      "Step: 483, Loss: 0.4204355776309967\n",
      "Step: 484, Loss: 0.42003539204597473\n",
      "Step: 485, Loss: 0.42055046558380127\n",
      "Step: 486, Loss: 0.420457661151886\n",
      "Step: 487, Loss: 0.42076700925827026\n",
      "Step: 488, Loss: 0.42072591185569763\n",
      "Step: 489, Loss: 0.4207138121128082\n",
      "Step: 490, Loss: 0.42072615027427673\n",
      "Step: 491, Loss: 0.41999179124832153\n",
      "Step: 492, Loss: 0.4203672707080841\n",
      "Step: 493, Loss: 0.42005589604377747\n",
      "Step: 494, Loss: 0.4198201894760132\n",
      "Step: 495, Loss: 0.4200287461280823\n",
      "Step: 496, Loss: 0.41985395550727844\n",
      "Step: 497, Loss: 0.4199497103691101\n",
      "Step: 498, Loss: 0.41950520873069763\n",
      "Step: 499, Loss: 0.42013657093048096\n",
      "Step: 500, Loss: 0.4199453592300415\n",
      "Step: 501, Loss: 0.4191589057445526\n",
      "Step: 502, Loss: 0.41984879970550537\n",
      "Step: 503, Loss: 0.41996994614601135\n",
      "Step: 504, Loss: 0.4201323688030243\n",
      "Step: 505, Loss: 0.4198701083660126\n",
      "Step: 506, Loss: 0.42008262872695923\n",
      "Step: 507, Loss: 0.41953757405281067\n",
      "Step: 508, Loss: 0.42061227560043335\n",
      "Step: 509, Loss: 0.4204380512237549\n",
      "Step: 510, Loss: 0.419232040643692\n",
      "Step: 511, Loss: 0.4202613830566406\n",
      "Step: 512, Loss: 0.4206099510192871\n",
      "Step: 513, Loss: 0.4206976592540741\n",
      "Step: 514, Loss: 0.41964593529701233\n",
      "Step: 515, Loss: 0.4204956293106079\n",
      "Step: 516, Loss: 0.4203413128852844\n",
      "Step: 517, Loss: 0.42052987217903137\n",
      "Step: 518, Loss: 0.42025038599967957\n",
      "Step: 519, Loss: 0.42050400376319885\n",
      "Step: 520, Loss: 0.41970497369766235\n",
      "Step: 521, Loss: 0.4195585548877716\n",
      "Step: 522, Loss: 0.41979724168777466\n",
      "Step: 523, Loss: 0.4201761782169342\n",
      "Step: 524, Loss: 0.42032963037490845\n",
      "Step: 525, Loss: 0.41979309916496277\n",
      "Step: 526, Loss: 0.4202629625797272\n",
      "Step: 527, Loss: 0.4201485216617584\n",
      "Step: 528, Loss: 0.4194766879081726\n",
      "Step: 529, Loss: 0.4204427897930145\n",
      "Step: 530, Loss: 0.42015764117240906\n",
      "Step: 531, Loss: 0.41930103302001953\n",
      "Step: 532, Loss: 0.4199759066104889\n",
      "Step: 533, Loss: 0.41941124200820923\n",
      "Step: 534, Loss: 0.42040395736694336\n",
      "Step: 535, Loss: 0.4206879436969757\n",
      "Step: 536, Loss: 0.4198667109012604\n",
      "Step: 537, Loss: 0.4189811050891876\n",
      "Step: 538, Loss: 0.4211326837539673\n",
      "Step: 539, Loss: 0.4198239743709564\n",
      "Step: 540, Loss: 0.4192637503147125\n",
      "Step: 541, Loss: 0.42041972279548645\n",
      "Step: 542, Loss: 0.4198351204395294\n",
      "Step: 543, Loss: 0.42009398341178894\n",
      "Step: 544, Loss: 0.42067915201187134\n",
      "Step: 545, Loss: 0.41962486505508423\n",
      "Step: 546, Loss: 0.4189686179161072\n",
      "Step: 547, Loss: 0.4197208285331726\n",
      "Step: 548, Loss: 0.41966742277145386\n",
      "Step: 549, Loss: 0.42111334204673767\n",
      "Step: 550, Loss: 0.41835817694664\n",
      "Step: 551, Loss: 0.4198237359523773\n",
      "Step: 552, Loss: 0.4208177626132965\n",
      "Step: 553, Loss: 0.41971948742866516\n",
      "Step: 554, Loss: 0.4204491078853607\n",
      "Step: 555, Loss: 0.4196394383907318\n",
      "Step: 556, Loss: 0.4195311367511749\n",
      "Step: 557, Loss: 0.42025524377822876\n",
      "Step: 558, Loss: 0.4191487729549408\n",
      "Step: 559, Loss: 0.41930529475212097\n",
      "Step: 560, Loss: 0.4211016595363617\n",
      "Step: 561, Loss: 0.4208824336528778\n",
      "Step: 562, Loss: 0.4196069538593292\n",
      "Step: 563, Loss: 0.4205208718776703\n",
      "Step: 564, Loss: 0.4201869070529938\n",
      "Step: 565, Loss: 0.4213794469833374\n",
      "Step: 566, Loss: 0.4206577241420746\n",
      "Step: 567, Loss: 0.4203002452850342\n",
      "Step: 568, Loss: 0.4195637106895447\n",
      "Step: 569, Loss: 0.42004209756851196\n",
      "Step: 570, Loss: 0.4196585714817047\n",
      "Step: 571, Loss: 0.4194820523262024\n",
      "Step: 572, Loss: 0.42017531394958496\n",
      "Step: 573, Loss: 0.4195775091648102\n",
      "Step: 574, Loss: 0.42079657316207886\n",
      "Step: 575, Loss: 0.4201921820640564\n",
      "Step: 576, Loss: 0.4201129972934723\n",
      "Step: 577, Loss: 0.4203677177429199\n",
      "Step: 578, Loss: 0.4200596213340759\n",
      "Step: 579, Loss: 0.420192688703537\n",
      "Step: 580, Loss: 0.4204167425632477\n",
      "Step: 581, Loss: 0.41958945989608765\n",
      "Step: 582, Loss: 0.4202391505241394\n",
      "Step: 583, Loss: 0.41916975378990173\n",
      "Step: 584, Loss: 0.41974076628685\n",
      "Step: 585, Loss: 0.41982603073120117\n",
      "Step: 586, Loss: 0.4206005036830902\n",
      "Step: 587, Loss: 0.42003217339515686\n",
      "Step: 588, Loss: 0.41953128576278687\n",
      "Step: 589, Loss: 0.4202781319618225\n",
      "Step: 590, Loss: 0.4197225272655487\n",
      "Step: 591, Loss: 0.41982027888298035\n",
      "Step: 592, Loss: 0.4195793569087982\n",
      "Step: 593, Loss: 0.42061010003089905\n",
      "Step: 594, Loss: 0.4193331301212311\n",
      "Step: 595, Loss: 0.4193774163722992\n",
      "Step: 596, Loss: 0.420514851808548\n",
      "Step: 597, Loss: 0.41974177956581116\n",
      "Step: 598, Loss: 0.41910386085510254\n",
      "Step: 599, Loss: 0.4190845787525177\n",
      "Final loss:  0.41908458\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/ml_tn_gev_twcrps_mean12.0_std1.0_constant0.4000000059604645_epochs600_folds_1_2_mean0_1_2_3_4_std0_1_2_3_4_.pkl\n"
     ]
    }
   ],
   "source": [
    "model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    all_features,\n",
    "    location_features,\n",
    "    scale_features,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_constant = chain_function_constant,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained,\n",
    "    random_init = random_init,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Final loss:  0.10335002\n",
      "Final loss:  0.101744905\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using given parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 0.10101192444562912\n",
      "Step: 1, Loss: 0.1045975387096405\n",
      "Step: 2, Loss: 0.10153059661388397\n",
      "Step: 3, Loss: 0.10202322900295258\n",
      "Step: 4, Loss: 0.1033298522233963\n",
      "Step: 5, Loss: 0.10229266434907913\n",
      "Step: 6, Loss: 0.1007765606045723\n",
      "Step: 7, Loss: 0.10148461908102036\n",
      "Step: 8, Loss: 0.10205074399709702\n",
      "Step: 9, Loss: 0.10181190073490143\n",
      "Step: 10, Loss: 0.10107484459877014\n",
      "Step: 11, Loss: 0.10078710317611694\n",
      "Step: 12, Loss: 0.1010366752743721\n",
      "Step: 13, Loss: 0.10091517120599747\n",
      "Step: 14, Loss: 0.1011858657002449\n",
      "Step: 15, Loss: 0.10120128095149994\n",
      "Step: 16, Loss: 0.10065849870443344\n",
      "Step: 17, Loss: 0.10093311220407486\n",
      "Step: 18, Loss: 0.10144681483507156\n",
      "Step: 19, Loss: 0.10081422328948975\n",
      "Step: 20, Loss: 0.100248321890831\n",
      "Step: 21, Loss: 0.10068947076797485\n",
      "Step: 22, Loss: 0.10042747110128403\n",
      "Step: 23, Loss: 0.10088712722063065\n",
      "Step: 24, Loss: 0.1005655974149704\n",
      "Step: 25, Loss: 0.10123177617788315\n",
      "Step: 26, Loss: 0.10017429292201996\n",
      "Step: 27, Loss: 0.10028838366270065\n",
      "Step: 28, Loss: 0.10079685598611832\n",
      "Step: 29, Loss: 0.1003103256225586\n",
      "Step: 30, Loss: 0.10113375633955002\n",
      "Step: 31, Loss: 0.10008753091096878\n",
      "Step: 32, Loss: 0.10060267895460129\n",
      "Step: 33, Loss: 0.09993112087249756\n",
      "Step: 34, Loss: 0.10006285458803177\n",
      "Step: 35, Loss: 0.10051048547029495\n",
      "Step: 36, Loss: 0.10016465932130814\n",
      "Step: 37, Loss: 0.1008554995059967\n",
      "Step: 38, Loss: 0.10040871053934097\n",
      "Step: 39, Loss: 0.10027137398719788\n",
      "Step: 40, Loss: 0.10040444135665894\n",
      "Step: 41, Loss: 0.10068489611148834\n",
      "Step: 42, Loss: 0.10042519867420197\n",
      "Step: 43, Loss: 0.1005309671163559\n",
      "Step: 44, Loss: 0.100456103682518\n",
      "Step: 45, Loss: 0.10045760124921799\n",
      "Step: 46, Loss: 0.1009252518415451\n",
      "Step: 47, Loss: 0.10039106011390686\n",
      "Step: 48, Loss: 0.10039254277944565\n",
      "Step: 49, Loss: 0.10032002627849579\n",
      "Step: 50, Loss: 0.10016867518424988\n",
      "Step: 51, Loss: 0.10017027705907822\n",
      "Step: 52, Loss: 0.10000988841056824\n",
      "Step: 53, Loss: 0.10024469345808029\n",
      "Step: 54, Loss: 0.09989137947559357\n",
      "Step: 55, Loss: 0.10024677962064743\n",
      "Step: 56, Loss: 0.0998087152838707\n",
      "Step: 57, Loss: 0.10010010749101639\n",
      "Step: 58, Loss: 0.10027453303337097\n",
      "Step: 59, Loss: 0.10038748383522034\n",
      "Step: 60, Loss: 0.09995193034410477\n",
      "Step: 61, Loss: 0.10020797699689865\n",
      "Step: 62, Loss: 0.10054070502519608\n",
      "Step: 63, Loss: 0.09990429133176804\n",
      "Step: 64, Loss: 0.10013564676046371\n",
      "Step: 65, Loss: 0.10048423707485199\n",
      "Step: 66, Loss: 0.10039737820625305\n",
      "Step: 67, Loss: 0.10024860501289368\n",
      "Step: 68, Loss: 0.10020972788333893\n",
      "Step: 69, Loss: 0.1000327616930008\n",
      "Step: 70, Loss: 0.10040455311536789\n",
      "Step: 71, Loss: 0.10010264068841934\n",
      "Step: 72, Loss: 0.100120410323143\n",
      "Step: 73, Loss: 0.10007312148809433\n",
      "Step: 74, Loss: 0.09965398907661438\n",
      "Step: 75, Loss: 0.10011614859104156\n",
      "Step: 76, Loss: 0.1004413589835167\n",
      "Step: 77, Loss: 0.10010948032140732\n",
      "Step: 78, Loss: 0.10064411908388138\n",
      "Step: 79, Loss: 0.10035866498947144\n",
      "Step: 80, Loss: 0.10028905421495438\n",
      "Step: 81, Loss: 0.10032825917005539\n",
      "Step: 82, Loss: 0.09956841170787811\n",
      "Step: 83, Loss: 0.1001385971903801\n",
      "Step: 84, Loss: 0.10013581067323685\n",
      "Step: 85, Loss: 0.10000420361757278\n",
      "Step: 86, Loss: 0.09996264427900314\n",
      "Step: 87, Loss: 0.10030550509691238\n",
      "Step: 88, Loss: 0.09974067658185959\n",
      "Step: 89, Loss: 0.10065500438213348\n",
      "Step: 90, Loss: 0.10021546483039856\n",
      "Step: 91, Loss: 0.10012366622686386\n",
      "Step: 92, Loss: 0.1002422347664833\n",
      "Step: 93, Loss: 0.09985672682523727\n",
      "Step: 94, Loss: 0.10030867159366608\n",
      "Step: 95, Loss: 0.09985712915658951\n",
      "Step: 96, Loss: 0.10033926367759705\n",
      "Step: 97, Loss: 0.10037868469953537\n",
      "Step: 98, Loss: 0.099961057305336\n",
      "Step: 99, Loss: 0.10020636022090912\n",
      "Step: 100, Loss: 0.09966293722391129\n",
      "Step: 101, Loss: 0.10012321919202805\n",
      "Step: 102, Loss: 0.10028219223022461\n",
      "Step: 103, Loss: 0.10012897104024887\n",
      "Step: 104, Loss: 0.10037192702293396\n",
      "Step: 105, Loss: 0.10018285363912582\n",
      "Step: 106, Loss: 0.10033261775970459\n",
      "Step: 107, Loss: 0.10020512342453003\n",
      "Step: 108, Loss: 0.09996239095926285\n",
      "Step: 109, Loss: 0.09985598176717758\n",
      "Step: 110, Loss: 0.10007429122924805\n",
      "Step: 111, Loss: 0.10003470629453659\n",
      "Step: 112, Loss: 0.10045289248228073\n",
      "Step: 113, Loss: 0.1001242995262146\n",
      "Step: 114, Loss: 0.1000967025756836\n",
      "Step: 115, Loss: 0.10052905231714249\n",
      "Step: 116, Loss: 0.10022064298391342\n",
      "Step: 117, Loss: 0.10005631297826767\n",
      "Step: 118, Loss: 0.1000601276755333\n",
      "Step: 119, Loss: 0.09992896765470505\n",
      "Step: 120, Loss: 0.10031336545944214\n",
      "Step: 121, Loss: 0.10004186630249023\n",
      "Step: 122, Loss: 0.09985711425542831\n",
      "Step: 123, Loss: 0.09995575249195099\n",
      "Step: 124, Loss: 0.0998339056968689\n",
      "Step: 125, Loss: 0.10030561685562134\n",
      "Step: 126, Loss: 0.10000582784414291\n",
      "Step: 127, Loss: 0.1001092940568924\n",
      "Step: 128, Loss: 0.09972014278173447\n",
      "Step: 129, Loss: 0.09994105994701385\n",
      "Step: 130, Loss: 0.1002492755651474\n",
      "Step: 131, Loss: 0.0996551588177681\n",
      "Step: 132, Loss: 0.10011706501245499\n",
      "Step: 133, Loss: 0.10062992572784424\n",
      "Step: 134, Loss: 0.10021651536226273\n",
      "Step: 135, Loss: 0.10009622573852539\n",
      "Step: 136, Loss: 0.09982169419527054\n",
      "Step: 137, Loss: 0.1003158912062645\n",
      "Step: 138, Loss: 0.10016825795173645\n",
      "Step: 139, Loss: 0.10030484199523926\n",
      "Step: 140, Loss: 0.09976093471050262\n",
      "Step: 141, Loss: 0.10006352514028549\n",
      "Step: 142, Loss: 0.1001281887292862\n",
      "Step: 143, Loss: 0.0998583659529686\n",
      "Step: 144, Loss: 0.09987346082925797\n",
      "Step: 145, Loss: 0.09980994462966919\n",
      "Step: 146, Loss: 0.0999317392706871\n",
      "Step: 147, Loss: 0.09982917457818985\n",
      "Step: 148, Loss: 0.09982779622077942\n",
      "Step: 149, Loss: 0.10061391443014145\n",
      "Step: 150, Loss: 0.09979795664548874\n",
      "Step: 151, Loss: 0.09970931708812714\n",
      "Step: 152, Loss: 0.09963594377040863\n",
      "Step: 153, Loss: 0.09963522106409073\n",
      "Step: 154, Loss: 0.10000116378068924\n",
      "Step: 155, Loss: 0.09996750950813293\n",
      "Step: 156, Loss: 0.09971670806407928\n",
      "Step: 157, Loss: 0.10017547011375427\n",
      "Step: 158, Loss: 0.10012545436620712\n",
      "Step: 159, Loss: 0.1003195270895958\n",
      "Step: 160, Loss: 0.09978775680065155\n",
      "Step: 161, Loss: 0.0996885895729065\n",
      "Step: 162, Loss: 0.09987661987543106\n",
      "Step: 163, Loss: 0.09971199184656143\n",
      "Step: 164, Loss: 0.09981833398342133\n",
      "Step: 165, Loss: 0.09961768984794617\n",
      "Step: 166, Loss: 0.099788136780262\n",
      "Step: 167, Loss: 0.09958240389823914\n",
      "Step: 168, Loss: 0.0994737520813942\n",
      "Step: 169, Loss: 0.09954477846622467\n",
      "Step: 170, Loss: 0.09974025934934616\n",
      "Step: 171, Loss: 0.09960589557886124\n",
      "Step: 172, Loss: 0.09965706616640091\n",
      "Step: 173, Loss: 0.10000422596931458\n",
      "Step: 174, Loss: 0.09968196600675583\n",
      "Step: 175, Loss: 0.09961492568254471\n",
      "Step: 176, Loss: 0.09962505102157593\n",
      "Step: 177, Loss: 0.09971467405557632\n",
      "Step: 178, Loss: 0.09962061792612076\n",
      "Step: 179, Loss: 0.10008540749549866\n",
      "Step: 180, Loss: 0.09984629601240158\n",
      "Step: 181, Loss: 0.09999776631593704\n",
      "Step: 182, Loss: 0.09971991181373596\n",
      "Step: 183, Loss: 0.09985431283712387\n",
      "Step: 184, Loss: 0.09943965077400208\n",
      "Step: 185, Loss: 0.09947750717401505\n",
      "Step: 186, Loss: 0.09945524483919144\n",
      "Step: 187, Loss: 0.09984668344259262\n",
      "Step: 188, Loss: 0.09993193298578262\n",
      "Step: 189, Loss: 0.10006702691316605\n",
      "Step: 190, Loss: 0.09995385259389877\n",
      "Step: 191, Loss: 0.09992042928934097\n",
      "Step: 192, Loss: 0.09999304264783859\n",
      "Step: 193, Loss: 0.09988008439540863\n",
      "Step: 194, Loss: 0.10006505995988846\n",
      "Step: 195, Loss: 0.0996040552854538\n",
      "Step: 196, Loss: 0.09975820779800415\n",
      "Step: 197, Loss: 0.0998237207531929\n",
      "Step: 198, Loss: 0.10023599117994308\n",
      "Step: 199, Loss: 0.09960661828517914\n",
      "Step: 200, Loss: 0.10003604739904404\n",
      "Step: 201, Loss: 0.09946393966674805\n",
      "Step: 202, Loss: 0.100131094455719\n",
      "Step: 203, Loss: 0.09991443157196045\n",
      "Step: 204, Loss: 0.09985866397619247\n",
      "Step: 205, Loss: 0.09977446496486664\n",
      "Step: 206, Loss: 0.09983466565608978\n",
      "Step: 207, Loss: 0.09991797804832458\n",
      "Step: 208, Loss: 0.10013588517904282\n",
      "Step: 209, Loss: 0.09971050173044205\n",
      "Step: 210, Loss: 0.09985184669494629\n",
      "Step: 211, Loss: 0.10012660920619965\n",
      "Step: 212, Loss: 0.10011982917785645\n",
      "Step: 213, Loss: 0.10008106380701065\n",
      "Step: 214, Loss: 0.0996464341878891\n",
      "Step: 215, Loss: 0.09952999651432037\n",
      "Step: 216, Loss: 0.09975336492061615\n",
      "Step: 217, Loss: 0.10034859925508499\n",
      "Step: 218, Loss: 0.10001468658447266\n",
      "Step: 219, Loss: 0.09998709708452225\n",
      "Step: 220, Loss: 0.09959638863801956\n",
      "Step: 221, Loss: 0.10007799416780472\n",
      "Step: 222, Loss: 0.09981153905391693\n",
      "Step: 223, Loss: 0.09976130723953247\n",
      "Step: 224, Loss: 0.09986881166696548\n",
      "Step: 225, Loss: 0.1004132404923439\n",
      "Step: 226, Loss: 0.10018006712198257\n",
      "Step: 227, Loss: 0.09992454946041107\n",
      "Step: 228, Loss: 0.09985758364200592\n",
      "Step: 229, Loss: 0.10003688186407089\n",
      "Step: 230, Loss: 0.09983201324939728\n",
      "Step: 231, Loss: 0.09984076023101807\n",
      "Step: 232, Loss: 0.09968853741884232\n",
      "Step: 233, Loss: 0.10021143406629562\n",
      "Step: 234, Loss: 0.10005252063274384\n",
      "Step: 235, Loss: 0.09953856468200684\n",
      "Step: 236, Loss: 0.09988480806350708\n",
      "Step: 237, Loss: 0.09968262910842896\n",
      "Step: 238, Loss: 0.09969595074653625\n",
      "Step: 239, Loss: 0.09975699335336685\n",
      "Step: 240, Loss: 0.09970810264348984\n",
      "Step: 241, Loss: 0.09993459284305573\n",
      "Step: 242, Loss: 0.10000336170196533\n",
      "Step: 243, Loss: 0.10025124996900558\n",
      "Step: 244, Loss: 0.09998960793018341\n",
      "Step: 245, Loss: 0.09986984729766846\n",
      "Step: 246, Loss: 0.09957825392484665\n",
      "Step: 247, Loss: 0.09991302341222763\n",
      "Step: 248, Loss: 0.09960884600877762\n",
      "Step: 249, Loss: 0.10005608946084976\n",
      "Step: 250, Loss: 0.10001975297927856\n",
      "Step: 251, Loss: 0.10007541626691818\n",
      "Step: 252, Loss: 0.09981565922498703\n",
      "Step: 253, Loss: 0.0997411459684372\n",
      "Step: 254, Loss: 0.09985959529876709\n",
      "Step: 255, Loss: 0.09977702796459198\n",
      "Step: 256, Loss: 0.09994734078645706\n",
      "Step: 257, Loss: 0.09973761439323425\n",
      "Step: 258, Loss: 0.09974502772092819\n",
      "Step: 259, Loss: 0.09975292533636093\n",
      "Step: 260, Loss: 0.0995725467801094\n",
      "Step: 261, Loss: 0.09971857070922852\n",
      "Step: 262, Loss: 0.09960460662841797\n",
      "Step: 263, Loss: 0.09983714669942856\n",
      "Step: 264, Loss: 0.0994969978928566\n",
      "Step: 265, Loss: 0.09993026405572891\n",
      "Step: 266, Loss: 0.10000336170196533\n",
      "Step: 267, Loss: 0.0997394397854805\n",
      "Step: 268, Loss: 0.0997757539153099\n",
      "Step: 269, Loss: 0.10010257363319397\n",
      "Step: 270, Loss: 0.10022033751010895\n",
      "Step: 271, Loss: 0.09982910007238388\n",
      "Step: 272, Loss: 0.1000540629029274\n",
      "Step: 273, Loss: 0.09944810718297958\n",
      "Step: 274, Loss: 0.09984477609395981\n",
      "Step: 275, Loss: 0.0992569848895073\n",
      "Step: 276, Loss: 0.09955143183469772\n",
      "Step: 277, Loss: 0.09981421381235123\n",
      "Step: 278, Loss: 0.09991719573736191\n",
      "Step: 279, Loss: 0.0994381308555603\n",
      "Step: 280, Loss: 0.09965036064386368\n",
      "Step: 281, Loss: 0.09970209002494812\n",
      "Step: 282, Loss: 0.10018698871135712\n",
      "Step: 283, Loss: 0.09987757354974747\n",
      "Step: 284, Loss: 0.0996500626206398\n",
      "Step: 285, Loss: 0.100091852247715\n",
      "Step: 286, Loss: 0.09989342093467712\n",
      "Step: 287, Loss: 0.10021798312664032\n",
      "Step: 288, Loss: 0.0999566838145256\n",
      "Step: 289, Loss: 0.0995660126209259\n",
      "Step: 290, Loss: 0.09934667497873306\n",
      "Step: 291, Loss: 0.0996696874499321\n",
      "Step: 292, Loss: 0.09985460340976715\n",
      "Step: 293, Loss: 0.09978452324867249\n",
      "Step: 294, Loss: 0.09970322996377945\n",
      "Step: 295, Loss: 0.09984125941991806\n",
      "Step: 296, Loss: 0.09951096773147583\n",
      "Step: 297, Loss: 0.09961052238941193\n",
      "Step: 298, Loss: 0.09987451136112213\n",
      "Step: 299, Loss: 0.09990452975034714\n",
      "Step: 300, Loss: 0.10009047389030457\n",
      "Step: 301, Loss: 0.09997782856225967\n",
      "Step: 302, Loss: 0.09924758970737457\n",
      "Step: 303, Loss: 0.09946586191654205\n",
      "Step: 304, Loss: 0.0998670756816864\n",
      "Step: 305, Loss: 0.10005007684230804\n",
      "Step: 306, Loss: 0.10001236945390701\n",
      "Step: 307, Loss: 0.09990290552377701\n",
      "Step: 308, Loss: 0.09959187358617783\n",
      "Step: 309, Loss: 0.1002177894115448\n",
      "Step: 310, Loss: 0.09970739483833313\n",
      "Step: 311, Loss: 0.09943041950464249\n",
      "Step: 312, Loss: 0.0995108038187027\n",
      "Step: 313, Loss: 0.09992336481809616\n",
      "Step: 314, Loss: 0.09930036962032318\n",
      "Step: 315, Loss: 0.09970066696405411\n",
      "Step: 316, Loss: 0.09972327202558517\n",
      "Step: 317, Loss: 0.09958726167678833\n",
      "Step: 318, Loss: 0.09994520246982574\n",
      "Step: 319, Loss: 0.09952209144830704\n",
      "Step: 320, Loss: 0.09948418289422989\n",
      "Step: 321, Loss: 0.09951978921890259\n",
      "Step: 322, Loss: 0.09943463653326035\n",
      "Step: 323, Loss: 0.09984159469604492\n",
      "Step: 324, Loss: 0.09962151199579239\n",
      "Step: 325, Loss: 0.0996653139591217\n",
      "Step: 326, Loss: 0.09927761554718018\n",
      "Step: 327, Loss: 0.09962236881256104\n",
      "Step: 328, Loss: 0.0997888371348381\n",
      "Step: 329, Loss: 0.09921903908252716\n",
      "Step: 330, Loss: 0.10006149858236313\n",
      "Step: 331, Loss: 0.0994364321231842\n",
      "Step: 332, Loss: 0.10026399791240692\n",
      "Step: 333, Loss: 0.09971509873867035\n",
      "Step: 334, Loss: 0.10039117187261581\n",
      "Step: 335, Loss: 0.09976818412542343\n",
      "Step: 336, Loss: 0.09957128018140793\n",
      "Step: 337, Loss: 0.10015113651752472\n",
      "Step: 338, Loss: 0.10032059252262115\n",
      "Step: 339, Loss: 0.09994091838598251\n",
      "Step: 340, Loss: 0.10010998696088791\n",
      "Step: 341, Loss: 0.09978180378675461\n",
      "Step: 342, Loss: 0.09978150576353073\n",
      "Step: 343, Loss: 0.10012223571538925\n",
      "Step: 344, Loss: 0.09989391267299652\n",
      "Step: 345, Loss: 0.09989049285650253\n",
      "Step: 346, Loss: 0.10021966695785522\n",
      "Step: 347, Loss: 0.10012882947921753\n",
      "Step: 348, Loss: 0.09990854561328888\n",
      "Step: 349, Loss: 0.09975525736808777\n",
      "Step: 350, Loss: 0.10012611001729965\n",
      "Step: 351, Loss: 0.09984101355075836\n",
      "Step: 352, Loss: 0.09986074268817902\n",
      "Step: 353, Loss: 0.09975353628396988\n",
      "Step: 354, Loss: 0.09982657432556152\n",
      "Step: 355, Loss: 0.09984082728624344\n",
      "Step: 356, Loss: 0.10024143010377884\n",
      "Step: 357, Loss: 0.10007154941558838\n",
      "Step: 358, Loss: 0.10003611445426941\n",
      "Step: 359, Loss: 0.10013946890830994\n",
      "Step: 360, Loss: 0.10002493113279343\n",
      "Step: 361, Loss: 0.10018520057201385\n",
      "Step: 362, Loss: 0.09982316195964813\n",
      "Step: 363, Loss: 0.1001284196972847\n",
      "Step: 364, Loss: 0.10002555698156357\n",
      "Step: 365, Loss: 0.09970288723707199\n",
      "Step: 366, Loss: 0.09989278018474579\n",
      "Step: 367, Loss: 0.1003192737698555\n",
      "Step: 368, Loss: 0.10013553500175476\n",
      "Step: 369, Loss: 0.09986501932144165\n",
      "Step: 370, Loss: 0.10005918145179749\n",
      "Step: 371, Loss: 0.10015542060136795\n",
      "Step: 372, Loss: 0.09939827024936676\n",
      "Step: 373, Loss: 0.10003519803285599\n",
      "Step: 374, Loss: 0.0998372957110405\n",
      "Step: 375, Loss: 0.10035349428653717\n",
      "Step: 376, Loss: 0.09981882572174072\n",
      "Step: 377, Loss: 0.09971339255571365\n",
      "Step: 378, Loss: 0.09950613230466843\n",
      "Step: 379, Loss: 0.0999230444431305\n",
      "Step: 380, Loss: 0.10002359747886658\n",
      "Step: 381, Loss: 0.10012052953243256\n",
      "Step: 382, Loss: 0.10015081614255905\n",
      "Step: 383, Loss: 0.09993571788072586\n",
      "Step: 384, Loss: 0.10028791427612305\n",
      "Step: 385, Loss: 0.09959328919649124\n",
      "Step: 386, Loss: 0.09993324428796768\n",
      "Step: 387, Loss: 0.09984131157398224\n",
      "Step: 388, Loss: 0.09991424530744553\n",
      "Step: 389, Loss: 0.10016492754220963\n",
      "Step: 390, Loss: 0.09998331218957901\n",
      "Step: 391, Loss: 0.09986256062984467\n",
      "Step: 392, Loss: 0.09996448457241058\n",
      "Step: 393, Loss: 0.09943445771932602\n",
      "Step: 394, Loss: 0.09974827617406845\n",
      "Step: 395, Loss: 0.10029754787683487\n",
      "Step: 396, Loss: 0.10010799765586853\n",
      "Step: 397, Loss: 0.09959585964679718\n",
      "Step: 398, Loss: 0.09976475685834885\n",
      "Step: 399, Loss: 0.09986741095781326\n",
      "Step: 400, Loss: 0.09970524162054062\n",
      "Step: 401, Loss: 0.09987582266330719\n",
      "Step: 402, Loss: 0.09949187934398651\n",
      "Step: 403, Loss: 0.09993467479944229\n",
      "Step: 404, Loss: 0.10013909637928009\n",
      "Step: 405, Loss: 0.09980069845914841\n",
      "Step: 406, Loss: 0.10038300603628159\n",
      "Step: 407, Loss: 0.09998966008424759\n",
      "Step: 408, Loss: 0.09987952560186386\n",
      "Step: 409, Loss: 0.0997140109539032\n",
      "Step: 410, Loss: 0.09966640919446945\n",
      "Step: 411, Loss: 0.09958469867706299\n",
      "Step: 412, Loss: 0.10002169013023376\n",
      "Step: 413, Loss: 0.09982061386108398\n",
      "Step: 414, Loss: 0.09994416683912277\n",
      "Step: 415, Loss: 0.09970349818468094\n",
      "Step: 416, Loss: 0.10004322975873947\n",
      "Step: 417, Loss: 0.0996941328048706\n",
      "Step: 418, Loss: 0.09992799162864685\n",
      "Step: 419, Loss: 0.10003471374511719\n",
      "Step: 420, Loss: 0.09996713697910309\n",
      "Step: 421, Loss: 0.09973790496587753\n",
      "Step: 422, Loss: 0.09977637976408005\n",
      "Step: 423, Loss: 0.10000914335250854\n",
      "Step: 424, Loss: 0.10004518181085587\n",
      "Step: 425, Loss: 0.10023046284914017\n",
      "Step: 426, Loss: 0.09995432198047638\n",
      "Step: 427, Loss: 0.09925954788923264\n",
      "Step: 428, Loss: 0.09991728514432907\n",
      "Step: 429, Loss: 0.09946966916322708\n",
      "Step: 430, Loss: 0.09996330738067627\n",
      "Step: 431, Loss: 0.09970986098051071\n",
      "Step: 432, Loss: 0.09970537573099136\n",
      "Step: 433, Loss: 0.10017500817775726\n",
      "Step: 434, Loss: 0.0995701476931572\n",
      "Step: 435, Loss: 0.09996020793914795\n",
      "Step: 436, Loss: 0.09986601769924164\n",
      "Step: 437, Loss: 0.09947854280471802\n",
      "Step: 438, Loss: 0.09946966916322708\n",
      "Step: 439, Loss: 0.09941795468330383\n",
      "Step: 440, Loss: 0.0998176634311676\n",
      "Step: 441, Loss: 0.0996798649430275\n",
      "Step: 442, Loss: 0.0996241644024849\n",
      "Step: 443, Loss: 0.09985533356666565\n",
      "Step: 444, Loss: 0.09977034479379654\n",
      "Step: 445, Loss: 0.09954078495502472\n",
      "Step: 446, Loss: 0.09973353892564774\n",
      "Step: 447, Loss: 0.09921430796384811\n",
      "Step: 448, Loss: 0.09962459653615952\n",
      "Step: 449, Loss: 0.09963539242744446\n",
      "Step: 450, Loss: 0.09927268326282501\n",
      "Step: 451, Loss: 0.09973824769258499\n",
      "Step: 452, Loss: 0.10009041428565979\n",
      "Step: 453, Loss: 0.09966740012168884\n",
      "Step: 454, Loss: 0.10032093524932861\n",
      "Step: 455, Loss: 0.10001173615455627\n",
      "Step: 456, Loss: 0.09977711737155914\n",
      "Step: 457, Loss: 0.09990888833999634\n",
      "Step: 458, Loss: 0.10095333307981491\n",
      "Step: 459, Loss: 0.10029976814985275\n",
      "Step: 460, Loss: 0.10055486112833023\n",
      "Step: 461, Loss: 0.100510373711586\n",
      "Step: 462, Loss: 0.1001894623041153\n",
      "Step: 463, Loss: 0.1000116840004921\n",
      "Step: 464, Loss: 0.10017205774784088\n",
      "Step: 465, Loss: 0.09987734258174896\n",
      "Step: 466, Loss: 0.09993784874677658\n",
      "Step: 467, Loss: 0.10050167143344879\n",
      "Step: 468, Loss: 0.10080045461654663\n",
      "Step: 469, Loss: 0.10109151899814606\n",
      "Step: 470, Loss: 0.10154446959495544\n",
      "Step: 471, Loss: 0.101442851126194\n",
      "Step: 472, Loss: 0.10067947208881378\n",
      "Step: 473, Loss: 0.10112825036048889\n",
      "Step: 474, Loss: 0.10031715780496597\n",
      "Step: 475, Loss: 0.10064329952001572\n",
      "Step: 476, Loss: 0.1008189469575882\n",
      "Step: 477, Loss: 0.1007021814584732\n",
      "Step: 478, Loss: 0.10026750713586807\n",
      "Step: 479, Loss: 0.10025402158498764\n",
      "Step: 480, Loss: 0.10001331567764282\n",
      "Step: 481, Loss: 0.10018754005432129\n",
      "Step: 482, Loss: 0.10039862245321274\n",
      "Step: 483, Loss: 0.10031452029943466\n",
      "Step: 484, Loss: 0.10024953633546829\n",
      "Step: 485, Loss: 0.10004188120365143\n",
      "Step: 486, Loss: 0.09993920475244522\n",
      "Step: 487, Loss: 0.10009193420410156\n",
      "Step: 488, Loss: 0.09955208003520966\n",
      "Step: 489, Loss: 0.09983590990304947\n",
      "Step: 490, Loss: 0.10010526329278946\n",
      "Step: 491, Loss: 0.09993577003479004\n",
      "Step: 492, Loss: 0.10001025348901749\n",
      "Step: 493, Loss: 0.09999268501996994\n",
      "Step: 494, Loss: 0.1001076027750969\n",
      "Step: 495, Loss: 0.09974785894155502\n",
      "Step: 496, Loss: 0.10024365037679672\n",
      "Step: 497, Loss: 0.09986631572246552\n",
      "Step: 498, Loss: 0.09976784884929657\n",
      "Step: 499, Loss: 0.10009763389825821\n",
      "Step: 500, Loss: 0.0998651534318924\n",
      "Step: 501, Loss: 0.0996825098991394\n",
      "Step: 502, Loss: 0.0994672030210495\n",
      "Step: 503, Loss: 0.09935992956161499\n",
      "Step: 504, Loss: 0.09979315102100372\n",
      "Step: 505, Loss: 0.09958283603191376\n",
      "Step: 506, Loss: 0.09996163845062256\n",
      "Step: 507, Loss: 0.09993566572666168\n",
      "Step: 508, Loss: 0.1003762036561966\n",
      "Step: 509, Loss: 0.10008284449577332\n",
      "Step: 510, Loss: 0.09933522343635559\n",
      "Step: 511, Loss: 0.0995444506406784\n",
      "Step: 512, Loss: 0.10031702369451523\n",
      "Step: 513, Loss: 0.09976620972156525\n",
      "Step: 514, Loss: 0.09976449608802795\n",
      "Step: 515, Loss: 0.0998372808098793\n",
      "Step: 516, Loss: 0.0995604619383812\n",
      "Step: 517, Loss: 0.09973584115505219\n",
      "Step: 518, Loss: 0.09963072091341019\n",
      "Step: 519, Loss: 0.09962606430053711\n",
      "Step: 520, Loss: 0.09978165477514267\n",
      "Step: 521, Loss: 0.0993596613407135\n",
      "Step: 522, Loss: 0.09998148679733276\n",
      "Step: 523, Loss: 0.1000947579741478\n",
      "Step: 524, Loss: 0.09979057312011719\n",
      "Step: 525, Loss: 0.1000363752245903\n",
      "Step: 526, Loss: 0.099674291908741\n",
      "Step: 527, Loss: 0.09972982108592987\n",
      "Step: 528, Loss: 0.09998524188995361\n",
      "Step: 529, Loss: 0.09960104525089264\n",
      "Step: 530, Loss: 0.09983497858047485\n",
      "Step: 531, Loss: 0.09992524981498718\n",
      "Step: 532, Loss: 0.09966707229614258\n",
      "Step: 533, Loss: 0.09958945959806442\n",
      "Step: 534, Loss: 0.09980438649654388\n",
      "Step: 535, Loss: 0.0994650349020958\n",
      "Step: 536, Loss: 0.09983722120523453\n",
      "Step: 537, Loss: 0.09963468462228775\n",
      "Step: 538, Loss: 0.09971854090690613\n",
      "Step: 539, Loss: 0.09986277669668198\n",
      "Step: 540, Loss: 0.09945572912693024\n",
      "Step: 541, Loss: 0.10004832595586777\n",
      "Step: 542, Loss: 0.09977001696825027\n",
      "Step: 543, Loss: 0.09960256516933441\n",
      "Step: 544, Loss: 0.09927938133478165\n",
      "Step: 545, Loss: 0.09926994144916534\n",
      "Step: 546, Loss: 0.09978073835372925\n",
      "Step: 547, Loss: 0.09961432218551636\n",
      "Step: 548, Loss: 0.09936332702636719\n",
      "Step: 549, Loss: 0.09980623424053192\n",
      "Step: 550, Loss: 0.09983431547880173\n",
      "Step: 551, Loss: 0.09938172250986099\n",
      "Step: 552, Loss: 0.09943331778049469\n",
      "Step: 553, Loss: 0.09966979175806046\n",
      "Step: 554, Loss: 0.09966657310724258\n",
      "Step: 555, Loss: 0.09943845868110657\n",
      "Step: 556, Loss: 0.09954114258289337\n",
      "Step: 557, Loss: 0.09956977516412735\n",
      "Step: 558, Loss: 0.09957275539636612\n",
      "Step: 559, Loss: 0.09975379705429077\n",
      "Step: 560, Loss: 0.09955392777919769\n",
      "Step: 561, Loss: 0.09936036169528961\n",
      "Step: 562, Loss: 0.09943887591362\n",
      "Step: 563, Loss: 0.09995297342538834\n",
      "Step: 564, Loss: 0.0995626151561737\n",
      "Step: 565, Loss: 0.09926672279834747\n",
      "Step: 566, Loss: 0.09947508573532104\n",
      "Step: 567, Loss: 0.09905603528022766\n",
      "Step: 568, Loss: 0.09952205419540405\n",
      "Step: 569, Loss: 0.09928470104932785\n",
      "Step: 570, Loss: 0.09992332756519318\n",
      "Step: 571, Loss: 0.09943296760320663\n",
      "Step: 572, Loss: 0.09956399351358414\n",
      "Step: 573, Loss: 0.09931079298257828\n",
      "Step: 574, Loss: 0.09973365068435669\n",
      "Step: 575, Loss: 0.0994490459561348\n",
      "Step: 576, Loss: 0.09972744435071945\n",
      "Step: 577, Loss: 0.09966512769460678\n",
      "Step: 578, Loss: 0.09981945157051086\n",
      "Step: 579, Loss: 0.0996147096157074\n",
      "Step: 580, Loss: 0.09937168657779694\n",
      "Step: 581, Loss: 0.09961981326341629\n",
      "Step: 582, Loss: 0.09938687086105347\n",
      "Step: 583, Loss: 0.09917938709259033\n",
      "Step: 584, Loss: 0.09949074685573578\n",
      "Step: 585, Loss: 0.09972810000181198\n",
      "Step: 586, Loss: 0.09952537715435028\n",
      "Step: 587, Loss: 0.09971737116575241\n",
      "Step: 588, Loss: 0.09962929040193558\n",
      "Step: 589, Loss: 0.09988339990377426\n",
      "Step: 590, Loss: 0.09978655725717545\n",
      "Step: 591, Loss: 0.09918398410081863\n",
      "Step: 592, Loss: 0.09979411959648132\n",
      "Step: 593, Loss: 0.09905647486448288\n",
      "Step: 594, Loss: 0.09952230006456375\n",
      "Step: 595, Loss: 0.09919173270463943\n",
      "Step: 596, Loss: 0.09927141666412354\n",
      "Step: 597, Loss: 0.09943622350692749\n",
      "Step: 598, Loss: 0.09921099990606308\n",
      "Step: 599, Loss: 0.09971991926431656\n",
      "Final loss:  0.09971992\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/mixturelinear_tn_gev_twcrps_mean13.0_std4.0_epochs600_folds_1_3.pkl\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
