{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 09:07:25.628531: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-27 09:07:25.654847: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-27 09:07:25.654871: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-27 09:07:25.655546: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-27 09:07:25.659551: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-27 09:07:25.659999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 09:07:30.883172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.training.training import train_model, train_and_save, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_distribution = 'distr_mixture_linear'\n",
    "distribution_1 = 'distr_trunc_normal_features'\n",
    "distribution_2 = 'distr_gev'\n",
    "\n",
    "loss = 'loss_CRPS_sample' # options: loss_CRPS_sample, loss_twCRPS_sample, loss_log_likelihood\n",
    "\n",
    "chain_function = 'chain_function_normal_cdf_plus_constant' # options: chain_function_normal_cdf, chain_function_indicator, chain_function_normal_cdf_plus_constant\n",
    "chain_function_mean = 13\n",
    "chain_function_std = 2\n",
    "chain_function_threshold = 15 # 12 / 15\n",
    "chain_function_constant = 0.01\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.01\n",
    "folds = [1,2]\n",
    "parameter_names = ['wind_speed', 'press', 'kinetic', 'humid', 'geopot']\n",
    "neighbourhood_size = 11\n",
    "ignore = ['229', '285', '323']\n",
    "epochs = 600\n",
    "\n",
    "samples = 100\n",
    "printing = True\n",
    "pretrained = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution with features\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Final loss:  1.1039246\n",
      "Final loss:  1.2262671\n",
      "Using given parameters for Truncated Normal distribution with features\n",
      "Using given parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 1.0523077249526978\n",
      "Step: 1, Loss: 1.0347641706466675\n",
      "Step: 2, Loss: 1.024176836013794\n",
      "Step: 3, Loss: 1.0115128755569458\n",
      "Step: 4, Loss: 1.0017307996749878\n",
      "Step: 5, Loss: 0.9935532808303833\n",
      "Step: 6, Loss: 0.9808883666992188\n",
      "Step: 7, Loss: 0.9776032567024231\n",
      "Step: 8, Loss: 0.9710566997528076\n",
      "Step: 9, Loss: 0.9625041484832764\n",
      "Step: 10, Loss: 0.9589493274688721\n",
      "Step: 11, Loss: 0.9564645290374756\n",
      "Step: 12, Loss: 0.950151801109314\n",
      "Step: 13, Loss: 0.944703221321106\n",
      "Step: 14, Loss: 0.9423388242721558\n",
      "Step: 15, Loss: 0.9358922243118286\n",
      "Step: 16, Loss: 0.9342640042304993\n",
      "Step: 17, Loss: 0.9333038926124573\n",
      "Step: 18, Loss: 0.9299827218055725\n",
      "Step: 19, Loss: 0.9280868172645569\n",
      "Step: 20, Loss: 0.9284082055091858\n",
      "Step: 21, Loss: 0.9271278381347656\n",
      "Step: 22, Loss: 0.9252626895904541\n",
      "Step: 23, Loss: 0.9230402112007141\n",
      "Step: 24, Loss: 0.9219565391540527\n",
      "Step: 25, Loss: 0.919690728187561\n",
      "Step: 26, Loss: 0.9203134775161743\n",
      "Step: 27, Loss: 0.9186753630638123\n",
      "Step: 28, Loss: 0.9182583093643188\n",
      "Step: 29, Loss: 0.9160754680633545\n",
      "Step: 30, Loss: 0.9155185222625732\n",
      "Step: 31, Loss: 0.9161849617958069\n",
      "Step: 32, Loss: 0.9128785729408264\n",
      "Step: 33, Loss: 0.9120798110961914\n",
      "Step: 34, Loss: 0.9134885668754578\n",
      "Step: 35, Loss: 0.910651445388794\n",
      "Step: 36, Loss: 0.9120714068412781\n",
      "Step: 37, Loss: 0.9098155498504639\n",
      "Step: 38, Loss: 0.9108127355575562\n",
      "Step: 39, Loss: 0.9095788598060608\n",
      "Step: 40, Loss: 0.9088360071182251\n",
      "Step: 41, Loss: 0.9098357558250427\n",
      "Step: 42, Loss: 0.9080456495285034\n",
      "Step: 43, Loss: 0.9084450006484985\n",
      "Step: 44, Loss: 0.9064327478408813\n",
      "Step: 45, Loss: 0.9088099598884583\n",
      "Step: 46, Loss: 0.9065549373626709\n",
      "Step: 47, Loss: 0.9070103764533997\n",
      "Step: 48, Loss: 0.9070715308189392\n",
      "Step: 49, Loss: 0.9061805009841919\n",
      "Step: 50, Loss: 0.9067613482475281\n",
      "Step: 51, Loss: 0.9035922288894653\n",
      "Step: 52, Loss: 0.9039439558982849\n",
      "Step: 53, Loss: 0.9034287333488464\n",
      "Step: 54, Loss: 0.9036477208137512\n",
      "Step: 55, Loss: 0.9045712351799011\n",
      "Step: 56, Loss: 0.9051729440689087\n",
      "Step: 57, Loss: 0.9043079614639282\n",
      "Step: 58, Loss: 0.9037491679191589\n",
      "Step: 59, Loss: 0.903810441493988\n",
      "Step: 60, Loss: 0.9030384421348572\n",
      "Step: 61, Loss: 0.9043977856636047\n",
      "Step: 62, Loss: 0.9037480354309082\n",
      "Step: 63, Loss: 0.9019092321395874\n",
      "Step: 64, Loss: 0.9036909341812134\n",
      "Step: 65, Loss: 0.9046359062194824\n",
      "Step: 66, Loss: 0.9019211530685425\n",
      "Step: 67, Loss: 0.9039819836616516\n",
      "Step: 68, Loss: 0.9021344184875488\n",
      "Step: 69, Loss: 0.9043360948562622\n",
      "Step: 70, Loss: 0.9040904641151428\n",
      "Step: 71, Loss: 0.9005941152572632\n",
      "Step: 72, Loss: 0.9029499292373657\n",
      "Step: 73, Loss: 0.8999370336532593\n",
      "Step: 74, Loss: 0.9024175405502319\n",
      "Step: 75, Loss: 0.9022794961929321\n",
      "Step: 76, Loss: 0.9014209508895874\n",
      "Step: 77, Loss: 0.9016101956367493\n",
      "Step: 78, Loss: 0.9042445421218872\n",
      "Step: 79, Loss: 0.9028028249740601\n",
      "Step: 80, Loss: 0.9014464616775513\n",
      "Step: 81, Loss: 0.9012162685394287\n",
      "Step: 82, Loss: 0.9026756882667542\n",
      "Step: 83, Loss: 0.9026287794113159\n",
      "Step: 84, Loss: 0.9001997113227844\n",
      "Step: 85, Loss: 0.901151180267334\n",
      "Step: 86, Loss: 0.9019709229469299\n",
      "Step: 87, Loss: 0.9018757939338684\n",
      "Step: 88, Loss: 0.9008455872535706\n",
      "Step: 89, Loss: 0.9006982445716858\n",
      "Step: 90, Loss: 0.9018784761428833\n",
      "Step: 91, Loss: 0.8999630808830261\n",
      "Step: 92, Loss: 0.9022947549819946\n",
      "Step: 93, Loss: 0.9024344682693481\n",
      "Step: 94, Loss: 0.9001548886299133\n",
      "Step: 95, Loss: 0.9017689824104309\n",
      "Step: 96, Loss: 0.9003435373306274\n",
      "Step: 97, Loss: 0.9013558030128479\n",
      "Step: 98, Loss: 0.9000713229179382\n",
      "Step: 99, Loss: 0.9023182988166809\n",
      "Step: 100, Loss: 0.9014994502067566\n",
      "Step: 101, Loss: 0.9020119309425354\n",
      "Step: 102, Loss: 0.9008541703224182\n",
      "Step: 103, Loss: 0.8985621929168701\n",
      "Step: 104, Loss: 0.9008277058601379\n",
      "Step: 105, Loss: 0.9012799859046936\n",
      "Step: 106, Loss: 0.9007269144058228\n",
      "Step: 107, Loss: 0.8994349241256714\n",
      "Step: 108, Loss: 0.901317834854126\n",
      "Step: 109, Loss: 0.900678277015686\n",
      "Step: 110, Loss: 0.9016725420951843\n",
      "Step: 111, Loss: 0.901547372341156\n",
      "Step: 112, Loss: 0.9015580415725708\n",
      "Step: 113, Loss: 0.9026131629943848\n",
      "Step: 114, Loss: 0.901452898979187\n",
      "Step: 115, Loss: 0.9016331434249878\n",
      "Step: 116, Loss: 0.9018124938011169\n",
      "Step: 117, Loss: 0.8995354175567627\n",
      "Step: 118, Loss: 0.900916337966919\n",
      "Step: 119, Loss: 0.8991252183914185\n",
      "Step: 120, Loss: 0.9001058340072632\n",
      "Step: 121, Loss: 0.9007700085639954\n",
      "Step: 122, Loss: 0.900466799736023\n",
      "Step: 123, Loss: 0.9033887982368469\n",
      "Step: 124, Loss: 0.8996932506561279\n",
      "Step: 125, Loss: 0.9017894268035889\n",
      "Step: 126, Loss: 0.9011299014091492\n",
      "Step: 127, Loss: 0.9012623429298401\n",
      "Step: 128, Loss: 0.8995712995529175\n",
      "Step: 129, Loss: 0.9016345739364624\n",
      "Step: 130, Loss: 0.9002966284751892\n",
      "Step: 131, Loss: 0.9018552303314209\n",
      "Step: 132, Loss: 0.9027793407440186\n",
      "Step: 133, Loss: 0.8989405632019043\n",
      "Step: 134, Loss: 0.8997455835342407\n",
      "Step: 135, Loss: 0.8997758030891418\n",
      "Step: 136, Loss: 0.9017035365104675\n",
      "Step: 137, Loss: 0.9004520773887634\n",
      "Step: 138, Loss: 0.8996618986129761\n",
      "Step: 139, Loss: 0.9015101790428162\n",
      "Step: 140, Loss: 0.8984732627868652\n",
      "Step: 141, Loss: 0.8998516201972961\n",
      "Step: 142, Loss: 0.9005656838417053\n",
      "Step: 143, Loss: 0.8993247151374817\n",
      "Step: 144, Loss: 0.9007147550582886\n",
      "Step: 145, Loss: 0.9008479714393616\n",
      "Step: 146, Loss: 0.9006367921829224\n",
      "Step: 147, Loss: 0.901223361492157\n",
      "Step: 148, Loss: 0.8996350765228271\n",
      "Step: 149, Loss: 0.902847409248352\n",
      "Step: 150, Loss: 0.9005012512207031\n",
      "Step: 151, Loss: 0.9014497995376587\n",
      "Step: 152, Loss: 0.9005105495452881\n",
      "Step: 153, Loss: 0.9001097083091736\n",
      "Step: 154, Loss: 0.9001340270042419\n",
      "Step: 155, Loss: 0.8999946117401123\n",
      "Step: 156, Loss: 0.8982996940612793\n",
      "Step: 157, Loss: 0.8993040919303894\n",
      "Step: 158, Loss: 0.9011814594268799\n",
      "Step: 159, Loss: 0.900484025478363\n",
      "Step: 160, Loss: 0.9010838270187378\n",
      "Step: 161, Loss: 0.8992210626602173\n",
      "Step: 162, Loss: 0.9002507328987122\n",
      "Step: 163, Loss: 0.9016188979148865\n",
      "Step: 164, Loss: 0.9004754424095154\n",
      "Step: 165, Loss: 0.9000018239021301\n",
      "Step: 166, Loss: 0.9011783003807068\n",
      "Step: 167, Loss: 0.9004106521606445\n",
      "Step: 168, Loss: 0.9021856188774109\n",
      "Step: 169, Loss: 0.9005432724952698\n",
      "Step: 170, Loss: 0.8982852101325989\n",
      "Step: 171, Loss: 0.8992254137992859\n",
      "Step: 172, Loss: 0.8988773822784424\n",
      "Step: 173, Loss: 0.8995623588562012\n",
      "Step: 174, Loss: 0.9011481404304504\n",
      "Step: 175, Loss: 0.8997923135757446\n",
      "Step: 176, Loss: 0.9001621007919312\n",
      "Step: 177, Loss: 0.9000986218452454\n",
      "Step: 178, Loss: 0.8990694284439087\n",
      "Step: 179, Loss: 0.8992001414299011\n",
      "Step: 180, Loss: 0.9003987908363342\n",
      "Step: 181, Loss: 0.9010059833526611\n",
      "Step: 182, Loss: 0.9008703231811523\n",
      "Step: 183, Loss: 0.8997997641563416\n",
      "Step: 184, Loss: 0.9000873565673828\n",
      "Step: 185, Loss: 0.8994394540786743\n",
      "Step: 186, Loss: 0.900190532207489\n",
      "Step: 187, Loss: 0.9010222554206848\n",
      "Step: 188, Loss: 0.8993848562240601\n",
      "Step: 189, Loss: 0.8996496200561523\n",
      "Step: 190, Loss: 0.8995826244354248\n",
      "Step: 191, Loss: 0.8999443054199219\n",
      "Step: 192, Loss: 0.9009197354316711\n",
      "Step: 193, Loss: 0.9004982709884644\n",
      "Step: 194, Loss: 0.8989831209182739\n",
      "Step: 195, Loss: 0.899147093296051\n",
      "Step: 196, Loss: 0.9016872644424438\n",
      "Step: 197, Loss: 0.8980880379676819\n",
      "Step: 198, Loss: 0.9010958075523376\n",
      "Step: 199, Loss: 0.8989324569702148\n",
      "Step: 200, Loss: 0.8991125226020813\n",
      "Step: 201, Loss: 0.9004513025283813\n",
      "Step: 202, Loss: 0.8984268307685852\n",
      "Step: 203, Loss: 0.9000555872917175\n",
      "Step: 204, Loss: 0.8993852138519287\n",
      "Step: 205, Loss: 0.9001359939575195\n",
      "Step: 206, Loss: 0.9020890593528748\n",
      "Step: 207, Loss: 0.9000946879386902\n",
      "Step: 208, Loss: 0.9007382988929749\n",
      "Step: 209, Loss: 0.9007852673530579\n",
      "Step: 210, Loss: 0.900522768497467\n",
      "Step: 211, Loss: 0.9014794826507568\n",
      "Step: 212, Loss: 0.8996521234512329\n",
      "Step: 213, Loss: 0.8993002772331238\n",
      "Step: 214, Loss: 0.9003172516822815\n",
      "Step: 215, Loss: 0.8992353677749634\n",
      "Step: 216, Loss: 0.9005494117736816\n",
      "Step: 217, Loss: 0.9012673497200012\n",
      "Step: 218, Loss: 0.9006820917129517\n",
      "Step: 219, Loss: 0.900793194770813\n",
      "Step: 220, Loss: 0.8989067673683167\n",
      "Step: 221, Loss: 0.9005110263824463\n",
      "Step: 222, Loss: 0.9002330899238586\n",
      "Step: 223, Loss: 0.8988157510757446\n",
      "Step: 224, Loss: 0.8983420133590698\n",
      "Step: 225, Loss: 0.8999932408332825\n",
      "Step: 226, Loss: 0.8987704515457153\n",
      "Step: 227, Loss: 0.900410532951355\n",
      "Step: 228, Loss: 0.9007532596588135\n",
      "Step: 229, Loss: 0.8995000123977661\n",
      "Step: 230, Loss: 0.9005999565124512\n",
      "Step: 231, Loss: 0.900631844997406\n",
      "Step: 232, Loss: 0.9005823731422424\n",
      "Step: 233, Loss: 0.8993965983390808\n",
      "Step: 234, Loss: 0.9008573889732361\n",
      "Step: 235, Loss: 0.9006122350692749\n",
      "Step: 236, Loss: 0.8983483910560608\n",
      "Step: 237, Loss: 0.8993304371833801\n",
      "Step: 238, Loss: 0.8990552425384521\n",
      "Step: 239, Loss: 0.9008768796920776\n",
      "Step: 240, Loss: 0.8986313939094543\n",
      "Step: 241, Loss: 0.8987112045288086\n",
      "Step: 242, Loss: 0.9020315408706665\n",
      "Step: 243, Loss: 0.8996512293815613\n",
      "Step: 244, Loss: 0.9013918042182922\n",
      "Step: 245, Loss: 0.8988408446311951\n",
      "Step: 246, Loss: 0.9009150862693787\n",
      "Step: 247, Loss: 0.9005793333053589\n",
      "Step: 248, Loss: 0.8996731042861938\n",
      "Step: 249, Loss: 0.9006788730621338\n",
      "Step: 250, Loss: 0.9005213379859924\n",
      "Step: 251, Loss: 0.9004179835319519\n",
      "Step: 252, Loss: 0.8984524011611938\n",
      "Step: 253, Loss: 0.9015853404998779\n",
      "Step: 254, Loss: 0.9000540971755981\n",
      "Step: 255, Loss: 0.8995872139930725\n",
      "Step: 256, Loss: 0.8975405693054199\n",
      "Step: 257, Loss: 0.8993831872940063\n",
      "Step: 258, Loss: 0.900425910949707\n",
      "Step: 259, Loss: 0.8976421356201172\n",
      "Step: 260, Loss: 0.9009172320365906\n",
      "Step: 261, Loss: 0.9001001119613647\n",
      "Step: 262, Loss: 0.9002645015716553\n",
      "Step: 263, Loss: 0.8977929353713989\n",
      "Step: 264, Loss: 0.9002020955085754\n",
      "Step: 265, Loss: 0.9014288187026978\n",
      "Step: 266, Loss: 0.8974013924598694\n",
      "Step: 267, Loss: 0.8988221883773804\n",
      "Step: 268, Loss: 0.8984214663505554\n",
      "Step: 269, Loss: 0.9018768668174744\n",
      "Step: 270, Loss: 0.8995413780212402\n",
      "Step: 271, Loss: 0.8998600244522095\n",
      "Step: 272, Loss: 0.9012153148651123\n",
      "Step: 273, Loss: 0.898003101348877\n",
      "Step: 274, Loss: 0.9015907049179077\n",
      "Step: 275, Loss: 0.8997951149940491\n",
      "Step: 276, Loss: 0.8992025852203369\n",
      "Step: 277, Loss: 0.8991122841835022\n",
      "Step: 278, Loss: 0.8993954658508301\n",
      "Step: 279, Loss: 0.8983046412467957\n",
      "Step: 280, Loss: 0.9003592133522034\n",
      "Step: 281, Loss: 0.899562418460846\n",
      "Step: 282, Loss: 0.9012393951416016\n",
      "Step: 283, Loss: 0.8998451232910156\n",
      "Step: 284, Loss: 0.8999242186546326\n",
      "Step: 285, Loss: 0.9001075625419617\n",
      "Step: 286, Loss: 0.8995702862739563\n",
      "Step: 287, Loss: 0.8977002501487732\n",
      "Step: 288, Loss: 0.8972510695457458\n",
      "Step: 289, Loss: 0.9001019597053528\n",
      "Step: 290, Loss: 0.9002370834350586\n",
      "Step: 291, Loss: 0.8977246880531311\n",
      "Step: 292, Loss: 0.8987004160881042\n",
      "Step: 293, Loss: 0.9004359245300293\n",
      "Step: 294, Loss: 0.8990395665168762\n",
      "Step: 295, Loss: 0.8995524644851685\n",
      "Step: 296, Loss: 0.8997457027435303\n",
      "Step: 297, Loss: 0.9006498456001282\n",
      "Step: 298, Loss: 0.9023948311805725\n",
      "Step: 299, Loss: 0.9004325270652771\n",
      "Step: 300, Loss: 0.8984177112579346\n",
      "Step: 301, Loss: 0.9000315070152283\n",
      "Step: 302, Loss: 0.8995015025138855\n",
      "Step: 303, Loss: 0.9007411003112793\n",
      "Step: 304, Loss: 0.8985483646392822\n",
      "Step: 305, Loss: 0.8983457684516907\n",
      "Step: 306, Loss: 0.9000304937362671\n",
      "Step: 307, Loss: 0.8972917795181274\n",
      "Step: 308, Loss: 0.9003917574882507\n",
      "Step: 309, Loss: 0.8994394540786743\n",
      "Step: 310, Loss: 0.9006466269493103\n",
      "Step: 311, Loss: 0.8981745839118958\n",
      "Step: 312, Loss: 0.8989735245704651\n",
      "Step: 313, Loss: 0.8981663584709167\n",
      "Step: 314, Loss: 0.900679886341095\n",
      "Step: 315, Loss: 0.8990892171859741\n",
      "Step: 316, Loss: 0.8997834324836731\n",
      "Step: 317, Loss: 0.899308979511261\n",
      "Step: 318, Loss: 0.898991584777832\n",
      "Step: 319, Loss: 0.9009385704994202\n",
      "Step: 320, Loss: 0.8980712294578552\n",
      "Step: 321, Loss: 0.8975076675415039\n",
      "Step: 322, Loss: 0.9002257585525513\n",
      "Step: 323, Loss: 0.8999828696250916\n",
      "Step: 324, Loss: 0.9011337161064148\n",
      "Step: 325, Loss: 0.9003221392631531\n",
      "Step: 326, Loss: 0.89890456199646\n",
      "Step: 327, Loss: 0.9002067446708679\n",
      "Step: 328, Loss: 0.8989986777305603\n",
      "Step: 329, Loss: 0.8989417552947998\n",
      "Step: 330, Loss: 0.9015138745307922\n",
      "Step: 331, Loss: 0.8992274403572083\n",
      "Step: 332, Loss: 0.8992469310760498\n",
      "Step: 333, Loss: 0.8991112112998962\n",
      "Step: 334, Loss: 0.8985241651535034\n",
      "Step: 335, Loss: 0.9000282883644104\n",
      "Step: 336, Loss: 0.9004881978034973\n",
      "Step: 337, Loss: 0.8996865749359131\n",
      "Step: 338, Loss: 0.9000087976455688\n",
      "Step: 339, Loss: 0.9007441401481628\n",
      "Step: 340, Loss: 0.9006037712097168\n",
      "Step: 341, Loss: 0.9003419876098633\n",
      "Step: 342, Loss: 0.9012454748153687\n",
      "Step: 343, Loss: 0.9013097882270813\n",
      "Step: 344, Loss: 0.9015567898750305\n",
      "Step: 345, Loss: 0.9003797173500061\n",
      "Step: 346, Loss: 0.899690568447113\n",
      "Step: 347, Loss: 0.8999199867248535\n",
      "Step: 348, Loss: 0.89933842420578\n",
      "Step: 349, Loss: 0.8994542360305786\n",
      "Step: 350, Loss: 0.8977671265602112\n",
      "Step: 351, Loss: 0.9006717801094055\n",
      "Step: 352, Loss: 0.900467574596405\n",
      "Step: 353, Loss: 0.9003409743309021\n",
      "Step: 354, Loss: 0.900406002998352\n",
      "Step: 355, Loss: 0.8988772630691528\n",
      "Step: 356, Loss: 0.8999981880187988\n",
      "Step: 357, Loss: 0.9002118706703186\n",
      "Step: 358, Loss: 0.9007883667945862\n",
      "Step: 359, Loss: 0.8989431262016296\n",
      "Step: 360, Loss: 0.8976768851280212\n",
      "Step: 361, Loss: 0.8987828493118286\n",
      "Step: 362, Loss: 0.8999544978141785\n",
      "Step: 363, Loss: 0.9012848138809204\n",
      "Step: 364, Loss: 0.8997243046760559\n",
      "Step: 365, Loss: 0.899940013885498\n",
      "Step: 366, Loss: 0.9004656076431274\n",
      "Step: 367, Loss: 0.8999725580215454\n",
      "Step: 368, Loss: 0.8992984890937805\n",
      "Step: 369, Loss: 0.9000242948532104\n",
      "Step: 370, Loss: 0.8996487259864807\n",
      "Step: 371, Loss: 0.8988674283027649\n",
      "Step: 372, Loss: 0.9010563492774963\n",
      "Step: 373, Loss: 0.89823317527771\n",
      "Step: 374, Loss: 0.9011500477790833\n",
      "Step: 375, Loss: 0.899644136428833\n",
      "Step: 376, Loss: 0.9001240134239197\n",
      "Step: 377, Loss: 0.9005101919174194\n",
      "Step: 378, Loss: 0.9005247950553894\n",
      "Step: 379, Loss: 0.9011216759681702\n",
      "Step: 380, Loss: 0.8995124697685242\n",
      "Step: 381, Loss: 0.8999547362327576\n",
      "Step: 382, Loss: 0.899279773235321\n",
      "Step: 383, Loss: 0.8980119228363037\n",
      "Step: 384, Loss: 0.9003634452819824\n",
      "Step: 385, Loss: 0.8992791175842285\n",
      "Step: 386, Loss: 0.900839626789093\n",
      "Step: 387, Loss: 0.9001988172531128\n",
      "Step: 388, Loss: 0.8985883593559265\n",
      "Step: 389, Loss: 0.9006972312927246\n",
      "Step: 390, Loss: 0.8998882174491882\n",
      "Step: 391, Loss: 0.898840069770813\n",
      "Step: 392, Loss: 0.9004489779472351\n",
      "Step: 393, Loss: 0.8996926546096802\n",
      "Step: 394, Loss: 0.8978883028030396\n",
      "Step: 395, Loss: 0.9000130891799927\n",
      "Step: 396, Loss: 0.8976696729660034\n",
      "Step: 397, Loss: 0.8987229466438293\n",
      "Step: 398, Loss: 0.9014805555343628\n",
      "Step: 399, Loss: 0.9004030823707581\n",
      "Step: 400, Loss: 0.8993924260139465\n",
      "Step: 401, Loss: 0.898466944694519\n",
      "Step: 402, Loss: 0.9020327925682068\n",
      "Step: 403, Loss: 0.897334098815918\n",
      "Step: 404, Loss: 0.8998498916625977\n",
      "Step: 405, Loss: 0.8995177149772644\n",
      "Step: 406, Loss: 0.8991811275482178\n",
      "Step: 407, Loss: 0.8984256982803345\n",
      "Step: 408, Loss: 0.9015285968780518\n",
      "Step: 409, Loss: 0.900821328163147\n",
      "Step: 410, Loss: 0.8987129330635071\n",
      "Step: 411, Loss: 0.8995234966278076\n",
      "Step: 412, Loss: 0.9007081985473633\n",
      "Step: 413, Loss: 0.8992329239845276\n",
      "Step: 414, Loss: 0.9000590443611145\n",
      "Step: 415, Loss: 0.8981245756149292\n",
      "Step: 416, Loss: 0.8985651135444641\n",
      "Step: 417, Loss: 0.9015325903892517\n",
      "Step: 418, Loss: 0.8989505767822266\n",
      "Step: 419, Loss: 0.9006523489952087\n",
      "Step: 420, Loss: 0.8983721137046814\n",
      "Step: 421, Loss: 0.8996908664703369\n",
      "Step: 422, Loss: 0.9009937047958374\n",
      "Step: 423, Loss: 0.8993762731552124\n",
      "Step: 424, Loss: 0.9002411365509033\n",
      "Step: 425, Loss: 0.8987768888473511\n",
      "Step: 426, Loss: 0.9000355005264282\n",
      "Step: 427, Loss: 0.8978362679481506\n",
      "Step: 428, Loss: 0.9013641476631165\n",
      "Step: 429, Loss: 0.8986649513244629\n",
      "Step: 430, Loss: 0.8979785442352295\n",
      "Step: 431, Loss: 0.8999136090278625\n",
      "Step: 432, Loss: 0.8983247876167297\n",
      "Step: 433, Loss: 0.8982347846031189\n",
      "Step: 434, Loss: 0.900143027305603\n",
      "Step: 435, Loss: 0.8996429443359375\n",
      "Step: 436, Loss: 0.8985554575920105\n",
      "Step: 437, Loss: 0.8986287117004395\n",
      "Step: 438, Loss: 0.8993617296218872\n",
      "Step: 439, Loss: 0.9003947973251343\n",
      "Step: 440, Loss: 0.8998911380767822\n",
      "Step: 441, Loss: 0.8983407020568848\n",
      "Step: 442, Loss: 0.8987237215042114\n",
      "Step: 443, Loss: 0.9004648327827454\n",
      "Step: 444, Loss: 0.8973675966262817\n",
      "Step: 445, Loss: 0.9008529782295227\n",
      "Step: 446, Loss: 0.9016167521476746\n",
      "Step: 447, Loss: 0.8986526131629944\n",
      "Step: 448, Loss: 0.8991393446922302\n",
      "Step: 449, Loss: 0.8998909592628479\n",
      "Step: 450, Loss: 0.8998144865036011\n",
      "Step: 451, Loss: 0.8979812860488892\n",
      "Step: 452, Loss: 0.8992432355880737\n",
      "Step: 453, Loss: 0.8982711434364319\n",
      "Step: 454, Loss: 0.9022426009178162\n",
      "Step: 455, Loss: 0.900114893913269\n",
      "Step: 456, Loss: 0.8986328840255737\n",
      "Step: 457, Loss: 0.8997771143913269\n",
      "Step: 458, Loss: 0.8974928259849548\n",
      "Step: 459, Loss: 0.8991031646728516\n",
      "Step: 460, Loss: 0.8995054960250854\n",
      "Step: 461, Loss: 0.899709165096283\n",
      "Step: 462, Loss: 0.899848222732544\n",
      "Step: 463, Loss: 0.9001421928405762\n",
      "Step: 464, Loss: 0.9012636542320251\n",
      "Step: 465, Loss: 0.9016129970550537\n",
      "Step: 466, Loss: 0.8998270034790039\n",
      "Step: 467, Loss: 0.8996520042419434\n",
      "Step: 468, Loss: 0.9001972079277039\n",
      "Step: 469, Loss: 0.8996228575706482\n",
      "Step: 470, Loss: 0.9006340503692627\n",
      "Step: 471, Loss: 0.8986192345619202\n",
      "Step: 472, Loss: 0.9007115364074707\n",
      "Step: 473, Loss: 0.8986976742744446\n",
      "Step: 474, Loss: 0.8992775678634644\n",
      "Step: 475, Loss: 0.8986696004867554\n",
      "Step: 476, Loss: 0.9000691771507263\n",
      "Step: 477, Loss: 0.9007970094680786\n",
      "Step: 478, Loss: 0.8997215032577515\n",
      "Step: 479, Loss: 0.9001343250274658\n",
      "Step: 480, Loss: 0.9013388156890869\n",
      "Step: 481, Loss: 0.9003267884254456\n",
      "Step: 482, Loss: 0.9007882475852966\n",
      "Step: 483, Loss: 0.8993159532546997\n",
      "Step: 484, Loss: 0.8995894193649292\n",
      "Step: 485, Loss: 0.8976789712905884\n",
      "Step: 486, Loss: 0.9001902341842651\n",
      "Step: 487, Loss: 0.8980212807655334\n",
      "Step: 488, Loss: 0.901701033115387\n",
      "Step: 489, Loss: 0.8995735049247742\n",
      "Step: 490, Loss: 0.8979066610336304\n",
      "Step: 491, Loss: 0.8998575806617737\n",
      "Step: 492, Loss: 0.8982391357421875\n",
      "Step: 493, Loss: 0.8988445401191711\n",
      "Step: 494, Loss: 0.9018226265907288\n",
      "Step: 495, Loss: 0.8997827172279358\n",
      "Step: 496, Loss: 0.8996620178222656\n",
      "Step: 497, Loss: 0.9000792503356934\n",
      "Step: 498, Loss: 0.8983451128005981\n",
      "Step: 499, Loss: 0.8995764851570129\n",
      "Step: 500, Loss: 0.8995830416679382\n",
      "Step: 501, Loss: 0.8995159268379211\n",
      "Step: 502, Loss: 0.8998706340789795\n",
      "Step: 503, Loss: 0.901118278503418\n",
      "Step: 504, Loss: 0.8988866806030273\n",
      "Step: 505, Loss: 0.9000080227851868\n",
      "Step: 506, Loss: 0.8993212580680847\n",
      "Step: 507, Loss: 0.8996343612670898\n",
      "Step: 508, Loss: 0.9020401239395142\n",
      "Step: 509, Loss: 0.9001455903053284\n",
      "Step: 510, Loss: 0.8988796472549438\n",
      "Step: 511, Loss: 0.9009462594985962\n",
      "Step: 512, Loss: 0.8980793356895447\n",
      "Step: 513, Loss: 0.9001790881156921\n",
      "Step: 514, Loss: 0.8983951210975647\n",
      "Step: 515, Loss: 0.8997477889060974\n",
      "Step: 516, Loss: 0.9001370072364807\n",
      "Step: 517, Loss: 0.900462806224823\n",
      "Step: 518, Loss: 0.8995973467826843\n",
      "Step: 519, Loss: 0.9015334844589233\n",
      "Step: 520, Loss: 0.899088978767395\n",
      "Step: 521, Loss: 0.8997721672058105\n",
      "Step: 522, Loss: 0.8999336361885071\n",
      "Step: 523, Loss: 0.8984104990959167\n",
      "Step: 524, Loss: 0.8985603451728821\n",
      "Step: 525, Loss: 0.9012376070022583\n",
      "Step: 526, Loss: 0.9001221060752869\n",
      "Step: 527, Loss: 0.8992643356323242\n",
      "Step: 528, Loss: 0.8997767567634583\n",
      "Step: 529, Loss: 0.9006525278091431\n",
      "Step: 530, Loss: 0.8986706137657166\n",
      "Step: 531, Loss: 0.8990865349769592\n",
      "Step: 532, Loss: 0.9009236693382263\n",
      "Step: 533, Loss: 0.8990287780761719\n",
      "Step: 534, Loss: 0.9002422094345093\n",
      "Step: 535, Loss: 0.8984700441360474\n",
      "Step: 536, Loss: 0.9005114436149597\n",
      "Step: 537, Loss: 0.8976735472679138\n",
      "Step: 538, Loss: 0.9018583297729492\n",
      "Step: 539, Loss: 0.8969259262084961\n",
      "Step: 540, Loss: 0.9011538624763489\n",
      "Step: 541, Loss: 0.9003335237503052\n",
      "Step: 542, Loss: 0.8987705707550049\n",
      "Step: 543, Loss: 0.9002299904823303\n",
      "Step: 544, Loss: 0.9012559056282043\n",
      "Step: 545, Loss: 0.8998488783836365\n",
      "Step: 546, Loss: 0.8989418745040894\n",
      "Step: 547, Loss: 0.897783100605011\n",
      "Step: 548, Loss: 0.900234580039978\n",
      "Step: 549, Loss: 0.8980804085731506\n",
      "Step: 550, Loss: 0.8995789885520935\n",
      "Step: 551, Loss: 0.8993387818336487\n",
      "Step: 552, Loss: 0.8990517258644104\n",
      "Step: 553, Loss: 0.899288535118103\n",
      "Step: 554, Loss: 0.8981342911720276\n",
      "Step: 555, Loss: 0.9001978039741516\n",
      "Step: 556, Loss: 0.8993135094642639\n",
      "Step: 557, Loss: 0.8988810777664185\n",
      "Step: 558, Loss: 0.9006321430206299\n",
      "Step: 559, Loss: 0.8989365100860596\n",
      "Step: 560, Loss: 0.8995143175125122\n",
      "Step: 561, Loss: 0.9009788036346436\n",
      "Step: 562, Loss: 0.8993663787841797\n",
      "Step: 563, Loss: 0.90130615234375\n",
      "Step: 564, Loss: 0.8992856740951538\n",
      "Step: 565, Loss: 0.8989279866218567\n",
      "Step: 566, Loss: 0.9000242948532104\n",
      "Step: 567, Loss: 0.9004055857658386\n",
      "Step: 568, Loss: 0.8992060422897339\n",
      "Step: 569, Loss: 0.897774875164032\n",
      "Step: 570, Loss: 0.9001177549362183\n",
      "Step: 571, Loss: 0.8989637494087219\n",
      "Step: 572, Loss: 0.8997177481651306\n",
      "Step: 573, Loss: 0.9010669589042664\n",
      "Step: 574, Loss: 0.8992015719413757\n",
      "Step: 575, Loss: 0.8993141651153564\n",
      "Step: 576, Loss: 0.9003008604049683\n",
      "Step: 577, Loss: 0.8974171280860901\n",
      "Step: 578, Loss: 0.8992981910705566\n",
      "Step: 579, Loss: 0.8986865282058716\n",
      "Step: 580, Loss: 0.8989526629447937\n",
      "Step: 581, Loss: 0.8978024125099182\n",
      "Step: 582, Loss: 0.8998075127601624\n",
      "Step: 583, Loss: 0.8979665637016296\n",
      "Step: 584, Loss: 0.8993874192237854\n",
      "Step: 585, Loss: 0.9001368880271912\n",
      "Step: 586, Loss: 0.8997879028320312\n",
      "Step: 587, Loss: 0.8994342088699341\n",
      "Step: 588, Loss: 0.8990364074707031\n",
      "Step: 589, Loss: 0.900396466255188\n",
      "Step: 590, Loss: 0.9015294313430786\n",
      "Step: 591, Loss: 0.8980687260627747\n",
      "Step: 592, Loss: 0.8979317545890808\n",
      "Step: 593, Loss: 0.8996896147727966\n",
      "Step: 594, Loss: 0.9007447957992554\n",
      "Step: 595, Loss: 0.8978897929191589\n",
      "Step: 596, Loss: 0.9002820253372192\n",
      "Step: 597, Loss: 0.900032103061676\n",
      "Step: 598, Loss: 0.8993105888366699\n",
      "Step: 599, Loss: 0.9001222252845764\n",
      "Final loss:  0.9001222\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/mixturelinear_tnf_gev_crps__epochs600_folds_1_2.pkl\n"
     ]
    }
   ],
   "source": [
    "model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    parameter_names,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_constant = chain_function_constant,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters for truncated normal distribution\n",
      "Using default parameters for Generalized Extreme Value distribution\n",
      "Final loss:  0.10335002\n",
      "Final loss:  0.101744905\n",
      "Using given parameters for Truncated Normal distribution\n",
      "Using given parameters for Generalized Extreme Value distribution\n",
      "Using default weight parameters for weights in Mixture Linear distribution\n",
      "Step: 0, Loss: 0.10101192444562912\n",
      "Step: 1, Loss: 0.1045975387096405\n",
      "Step: 2, Loss: 0.10153059661388397\n",
      "Step: 3, Loss: 0.10202322900295258\n",
      "Step: 4, Loss: 0.1033298522233963\n",
      "Step: 5, Loss: 0.10229266434907913\n",
      "Step: 6, Loss: 0.1007765606045723\n",
      "Step: 7, Loss: 0.10148461908102036\n",
      "Step: 8, Loss: 0.10205074399709702\n",
      "Step: 9, Loss: 0.10181190073490143\n",
      "Step: 10, Loss: 0.10107484459877014\n",
      "Step: 11, Loss: 0.10078710317611694\n",
      "Step: 12, Loss: 0.1010366752743721\n",
      "Step: 13, Loss: 0.10091517120599747\n",
      "Step: 14, Loss: 0.1011858657002449\n",
      "Step: 15, Loss: 0.10120128095149994\n",
      "Step: 16, Loss: 0.10065849870443344\n",
      "Step: 17, Loss: 0.10093311220407486\n",
      "Step: 18, Loss: 0.10144681483507156\n",
      "Step: 19, Loss: 0.10081422328948975\n",
      "Step: 20, Loss: 0.100248321890831\n",
      "Step: 21, Loss: 0.10068947076797485\n",
      "Step: 22, Loss: 0.10042747110128403\n",
      "Step: 23, Loss: 0.10088712722063065\n",
      "Step: 24, Loss: 0.1005655974149704\n",
      "Step: 25, Loss: 0.10123177617788315\n",
      "Step: 26, Loss: 0.10017429292201996\n",
      "Step: 27, Loss: 0.10028838366270065\n",
      "Step: 28, Loss: 0.10079685598611832\n",
      "Step: 29, Loss: 0.1003103256225586\n",
      "Step: 30, Loss: 0.10113375633955002\n",
      "Step: 31, Loss: 0.10008753091096878\n",
      "Step: 32, Loss: 0.10060267895460129\n",
      "Step: 33, Loss: 0.09993112087249756\n",
      "Step: 34, Loss: 0.10006285458803177\n",
      "Step: 35, Loss: 0.10051048547029495\n",
      "Step: 36, Loss: 0.10016465932130814\n",
      "Step: 37, Loss: 0.1008554995059967\n",
      "Step: 38, Loss: 0.10040871053934097\n",
      "Step: 39, Loss: 0.10027137398719788\n",
      "Step: 40, Loss: 0.10040444135665894\n",
      "Step: 41, Loss: 0.10068489611148834\n",
      "Step: 42, Loss: 0.10042519867420197\n",
      "Step: 43, Loss: 0.1005309671163559\n",
      "Step: 44, Loss: 0.100456103682518\n",
      "Step: 45, Loss: 0.10045760124921799\n",
      "Step: 46, Loss: 0.1009252518415451\n",
      "Step: 47, Loss: 0.10039106011390686\n",
      "Step: 48, Loss: 0.10039254277944565\n",
      "Step: 49, Loss: 0.10032002627849579\n",
      "Step: 50, Loss: 0.10016867518424988\n",
      "Step: 51, Loss: 0.10017027705907822\n",
      "Step: 52, Loss: 0.10000988841056824\n",
      "Step: 53, Loss: 0.10024469345808029\n",
      "Step: 54, Loss: 0.09989137947559357\n",
      "Step: 55, Loss: 0.10024677962064743\n",
      "Step: 56, Loss: 0.0998087152838707\n",
      "Step: 57, Loss: 0.10010010749101639\n",
      "Step: 58, Loss: 0.10027453303337097\n",
      "Step: 59, Loss: 0.10038748383522034\n",
      "Step: 60, Loss: 0.09995193034410477\n",
      "Step: 61, Loss: 0.10020797699689865\n",
      "Step: 62, Loss: 0.10054070502519608\n",
      "Step: 63, Loss: 0.09990429133176804\n",
      "Step: 64, Loss: 0.10013564676046371\n",
      "Step: 65, Loss: 0.10048423707485199\n",
      "Step: 66, Loss: 0.10039737820625305\n",
      "Step: 67, Loss: 0.10024860501289368\n",
      "Step: 68, Loss: 0.10020972788333893\n",
      "Step: 69, Loss: 0.1000327616930008\n",
      "Step: 70, Loss: 0.10040455311536789\n",
      "Step: 71, Loss: 0.10010264068841934\n",
      "Step: 72, Loss: 0.100120410323143\n",
      "Step: 73, Loss: 0.10007312148809433\n",
      "Step: 74, Loss: 0.09965398907661438\n",
      "Step: 75, Loss: 0.10011614859104156\n",
      "Step: 76, Loss: 0.1004413589835167\n",
      "Step: 77, Loss: 0.10010948032140732\n",
      "Step: 78, Loss: 0.10064411908388138\n",
      "Step: 79, Loss: 0.10035866498947144\n",
      "Step: 80, Loss: 0.10028905421495438\n",
      "Step: 81, Loss: 0.10032825917005539\n",
      "Step: 82, Loss: 0.09956841170787811\n",
      "Step: 83, Loss: 0.1001385971903801\n",
      "Step: 84, Loss: 0.10013581067323685\n",
      "Step: 85, Loss: 0.10000420361757278\n",
      "Step: 86, Loss: 0.09996264427900314\n",
      "Step: 87, Loss: 0.10030550509691238\n",
      "Step: 88, Loss: 0.09974067658185959\n",
      "Step: 89, Loss: 0.10065500438213348\n",
      "Step: 90, Loss: 0.10021546483039856\n",
      "Step: 91, Loss: 0.10012366622686386\n",
      "Step: 92, Loss: 0.1002422347664833\n",
      "Step: 93, Loss: 0.09985672682523727\n",
      "Step: 94, Loss: 0.10030867159366608\n",
      "Step: 95, Loss: 0.09985712915658951\n",
      "Step: 96, Loss: 0.10033926367759705\n",
      "Step: 97, Loss: 0.10037868469953537\n",
      "Step: 98, Loss: 0.099961057305336\n",
      "Step: 99, Loss: 0.10020636022090912\n",
      "Step: 100, Loss: 0.09966293722391129\n",
      "Step: 101, Loss: 0.10012321919202805\n",
      "Step: 102, Loss: 0.10028219223022461\n",
      "Step: 103, Loss: 0.10012897104024887\n",
      "Step: 104, Loss: 0.10037192702293396\n",
      "Step: 105, Loss: 0.10018285363912582\n",
      "Step: 106, Loss: 0.10033261775970459\n",
      "Step: 107, Loss: 0.10020512342453003\n",
      "Step: 108, Loss: 0.09996239095926285\n",
      "Step: 109, Loss: 0.09985598176717758\n",
      "Step: 110, Loss: 0.10007429122924805\n",
      "Step: 111, Loss: 0.10003470629453659\n",
      "Step: 112, Loss: 0.10045289248228073\n",
      "Step: 113, Loss: 0.1001242995262146\n",
      "Step: 114, Loss: 0.1000967025756836\n",
      "Step: 115, Loss: 0.10052905231714249\n",
      "Step: 116, Loss: 0.10022064298391342\n",
      "Step: 117, Loss: 0.10005631297826767\n",
      "Step: 118, Loss: 0.1000601276755333\n",
      "Step: 119, Loss: 0.09992896765470505\n",
      "Step: 120, Loss: 0.10031336545944214\n",
      "Step: 121, Loss: 0.10004186630249023\n",
      "Step: 122, Loss: 0.09985711425542831\n",
      "Step: 123, Loss: 0.09995575249195099\n",
      "Step: 124, Loss: 0.0998339056968689\n",
      "Step: 125, Loss: 0.10030561685562134\n",
      "Step: 126, Loss: 0.10000582784414291\n",
      "Step: 127, Loss: 0.1001092940568924\n",
      "Step: 128, Loss: 0.09972014278173447\n",
      "Step: 129, Loss: 0.09994105994701385\n",
      "Step: 130, Loss: 0.1002492755651474\n",
      "Step: 131, Loss: 0.0996551588177681\n",
      "Step: 132, Loss: 0.10011706501245499\n",
      "Step: 133, Loss: 0.10062992572784424\n",
      "Step: 134, Loss: 0.10021651536226273\n",
      "Step: 135, Loss: 0.10009622573852539\n",
      "Step: 136, Loss: 0.09982169419527054\n",
      "Step: 137, Loss: 0.1003158912062645\n",
      "Step: 138, Loss: 0.10016825795173645\n",
      "Step: 139, Loss: 0.10030484199523926\n",
      "Step: 140, Loss: 0.09976093471050262\n",
      "Step: 141, Loss: 0.10006352514028549\n",
      "Step: 142, Loss: 0.1001281887292862\n",
      "Step: 143, Loss: 0.0998583659529686\n",
      "Step: 144, Loss: 0.09987346082925797\n",
      "Step: 145, Loss: 0.09980994462966919\n",
      "Step: 146, Loss: 0.0999317392706871\n",
      "Step: 147, Loss: 0.09982917457818985\n",
      "Step: 148, Loss: 0.09982779622077942\n",
      "Step: 149, Loss: 0.10061391443014145\n",
      "Step: 150, Loss: 0.09979795664548874\n",
      "Step: 151, Loss: 0.09970931708812714\n",
      "Step: 152, Loss: 0.09963594377040863\n",
      "Step: 153, Loss: 0.09963522106409073\n",
      "Step: 154, Loss: 0.10000116378068924\n",
      "Step: 155, Loss: 0.09996750950813293\n",
      "Step: 156, Loss: 0.09971670806407928\n",
      "Step: 157, Loss: 0.10017547011375427\n",
      "Step: 158, Loss: 0.10012545436620712\n",
      "Step: 159, Loss: 0.1003195270895958\n",
      "Step: 160, Loss: 0.09978775680065155\n",
      "Step: 161, Loss: 0.0996885895729065\n",
      "Step: 162, Loss: 0.09987661987543106\n",
      "Step: 163, Loss: 0.09971199184656143\n",
      "Step: 164, Loss: 0.09981833398342133\n",
      "Step: 165, Loss: 0.09961768984794617\n",
      "Step: 166, Loss: 0.099788136780262\n",
      "Step: 167, Loss: 0.09958240389823914\n",
      "Step: 168, Loss: 0.0994737520813942\n",
      "Step: 169, Loss: 0.09954477846622467\n",
      "Step: 170, Loss: 0.09974025934934616\n",
      "Step: 171, Loss: 0.09960589557886124\n",
      "Step: 172, Loss: 0.09965706616640091\n",
      "Step: 173, Loss: 0.10000422596931458\n",
      "Step: 174, Loss: 0.09968196600675583\n",
      "Step: 175, Loss: 0.09961492568254471\n",
      "Step: 176, Loss: 0.09962505102157593\n",
      "Step: 177, Loss: 0.09971467405557632\n",
      "Step: 178, Loss: 0.09962061792612076\n",
      "Step: 179, Loss: 0.10008540749549866\n",
      "Step: 180, Loss: 0.09984629601240158\n",
      "Step: 181, Loss: 0.09999776631593704\n",
      "Step: 182, Loss: 0.09971991181373596\n",
      "Step: 183, Loss: 0.09985431283712387\n",
      "Step: 184, Loss: 0.09943965077400208\n",
      "Step: 185, Loss: 0.09947750717401505\n",
      "Step: 186, Loss: 0.09945524483919144\n",
      "Step: 187, Loss: 0.09984668344259262\n",
      "Step: 188, Loss: 0.09993193298578262\n",
      "Step: 189, Loss: 0.10006702691316605\n",
      "Step: 190, Loss: 0.09995385259389877\n",
      "Step: 191, Loss: 0.09992042928934097\n",
      "Step: 192, Loss: 0.09999304264783859\n",
      "Step: 193, Loss: 0.09988008439540863\n",
      "Step: 194, Loss: 0.10006505995988846\n",
      "Step: 195, Loss: 0.0996040552854538\n",
      "Step: 196, Loss: 0.09975820779800415\n",
      "Step: 197, Loss: 0.0998237207531929\n",
      "Step: 198, Loss: 0.10023599117994308\n",
      "Step: 199, Loss: 0.09960661828517914\n",
      "Step: 200, Loss: 0.10003604739904404\n",
      "Step: 201, Loss: 0.09946393966674805\n",
      "Step: 202, Loss: 0.100131094455719\n",
      "Step: 203, Loss: 0.09991443157196045\n",
      "Step: 204, Loss: 0.09985866397619247\n",
      "Step: 205, Loss: 0.09977446496486664\n",
      "Step: 206, Loss: 0.09983466565608978\n",
      "Step: 207, Loss: 0.09991797804832458\n",
      "Step: 208, Loss: 0.10013588517904282\n",
      "Step: 209, Loss: 0.09971050173044205\n",
      "Step: 210, Loss: 0.09985184669494629\n",
      "Step: 211, Loss: 0.10012660920619965\n",
      "Step: 212, Loss: 0.10011982917785645\n",
      "Step: 213, Loss: 0.10008106380701065\n",
      "Step: 214, Loss: 0.0996464341878891\n",
      "Step: 215, Loss: 0.09952999651432037\n",
      "Step: 216, Loss: 0.09975336492061615\n",
      "Step: 217, Loss: 0.10034859925508499\n",
      "Step: 218, Loss: 0.10001468658447266\n",
      "Step: 219, Loss: 0.09998709708452225\n",
      "Step: 220, Loss: 0.09959638863801956\n",
      "Step: 221, Loss: 0.10007799416780472\n",
      "Step: 222, Loss: 0.09981153905391693\n",
      "Step: 223, Loss: 0.09976130723953247\n",
      "Step: 224, Loss: 0.09986881166696548\n",
      "Step: 225, Loss: 0.1004132404923439\n",
      "Step: 226, Loss: 0.10018006712198257\n",
      "Step: 227, Loss: 0.09992454946041107\n",
      "Step: 228, Loss: 0.09985758364200592\n",
      "Step: 229, Loss: 0.10003688186407089\n",
      "Step: 230, Loss: 0.09983201324939728\n",
      "Step: 231, Loss: 0.09984076023101807\n",
      "Step: 232, Loss: 0.09968853741884232\n",
      "Step: 233, Loss: 0.10021143406629562\n",
      "Step: 234, Loss: 0.10005252063274384\n",
      "Step: 235, Loss: 0.09953856468200684\n",
      "Step: 236, Loss: 0.09988480806350708\n",
      "Step: 237, Loss: 0.09968262910842896\n",
      "Step: 238, Loss: 0.09969595074653625\n",
      "Step: 239, Loss: 0.09975699335336685\n",
      "Step: 240, Loss: 0.09970810264348984\n",
      "Step: 241, Loss: 0.09993459284305573\n",
      "Step: 242, Loss: 0.10000336170196533\n",
      "Step: 243, Loss: 0.10025124996900558\n",
      "Step: 244, Loss: 0.09998960793018341\n",
      "Step: 245, Loss: 0.09986984729766846\n",
      "Step: 246, Loss: 0.09957825392484665\n",
      "Step: 247, Loss: 0.09991302341222763\n",
      "Step: 248, Loss: 0.09960884600877762\n",
      "Step: 249, Loss: 0.10005608946084976\n",
      "Step: 250, Loss: 0.10001975297927856\n",
      "Step: 251, Loss: 0.10007541626691818\n",
      "Step: 252, Loss: 0.09981565922498703\n",
      "Step: 253, Loss: 0.0997411459684372\n",
      "Step: 254, Loss: 0.09985959529876709\n",
      "Step: 255, Loss: 0.09977702796459198\n",
      "Step: 256, Loss: 0.09994734078645706\n",
      "Step: 257, Loss: 0.09973761439323425\n",
      "Step: 258, Loss: 0.09974502772092819\n",
      "Step: 259, Loss: 0.09975292533636093\n",
      "Step: 260, Loss: 0.0995725467801094\n",
      "Step: 261, Loss: 0.09971857070922852\n",
      "Step: 262, Loss: 0.09960460662841797\n",
      "Step: 263, Loss: 0.09983714669942856\n",
      "Step: 264, Loss: 0.0994969978928566\n",
      "Step: 265, Loss: 0.09993026405572891\n",
      "Step: 266, Loss: 0.10000336170196533\n",
      "Step: 267, Loss: 0.0997394397854805\n",
      "Step: 268, Loss: 0.0997757539153099\n",
      "Step: 269, Loss: 0.10010257363319397\n",
      "Step: 270, Loss: 0.10022033751010895\n",
      "Step: 271, Loss: 0.09982910007238388\n",
      "Step: 272, Loss: 0.1000540629029274\n",
      "Step: 273, Loss: 0.09944810718297958\n",
      "Step: 274, Loss: 0.09984477609395981\n",
      "Step: 275, Loss: 0.0992569848895073\n",
      "Step: 276, Loss: 0.09955143183469772\n",
      "Step: 277, Loss: 0.09981421381235123\n",
      "Step: 278, Loss: 0.09991719573736191\n",
      "Step: 279, Loss: 0.0994381308555603\n",
      "Step: 280, Loss: 0.09965036064386368\n",
      "Step: 281, Loss: 0.09970209002494812\n",
      "Step: 282, Loss: 0.10018698871135712\n",
      "Step: 283, Loss: 0.09987757354974747\n",
      "Step: 284, Loss: 0.0996500626206398\n",
      "Step: 285, Loss: 0.100091852247715\n",
      "Step: 286, Loss: 0.09989342093467712\n",
      "Step: 287, Loss: 0.10021798312664032\n",
      "Step: 288, Loss: 0.0999566838145256\n",
      "Step: 289, Loss: 0.0995660126209259\n",
      "Step: 290, Loss: 0.09934667497873306\n",
      "Step: 291, Loss: 0.0996696874499321\n",
      "Step: 292, Loss: 0.09985460340976715\n",
      "Step: 293, Loss: 0.09978452324867249\n",
      "Step: 294, Loss: 0.09970322996377945\n",
      "Step: 295, Loss: 0.09984125941991806\n",
      "Step: 296, Loss: 0.09951096773147583\n",
      "Step: 297, Loss: 0.09961052238941193\n",
      "Step: 298, Loss: 0.09987451136112213\n",
      "Step: 299, Loss: 0.09990452975034714\n",
      "Step: 300, Loss: 0.10009047389030457\n",
      "Step: 301, Loss: 0.09997782856225967\n",
      "Step: 302, Loss: 0.09924758970737457\n",
      "Step: 303, Loss: 0.09946586191654205\n",
      "Step: 304, Loss: 0.0998670756816864\n",
      "Step: 305, Loss: 0.10005007684230804\n",
      "Step: 306, Loss: 0.10001236945390701\n",
      "Step: 307, Loss: 0.09990290552377701\n",
      "Step: 308, Loss: 0.09959187358617783\n",
      "Step: 309, Loss: 0.1002177894115448\n",
      "Step: 310, Loss: 0.09970739483833313\n",
      "Step: 311, Loss: 0.09943041950464249\n",
      "Step: 312, Loss: 0.0995108038187027\n",
      "Step: 313, Loss: 0.09992336481809616\n",
      "Step: 314, Loss: 0.09930036962032318\n",
      "Step: 315, Loss: 0.09970066696405411\n",
      "Step: 316, Loss: 0.09972327202558517\n",
      "Step: 317, Loss: 0.09958726167678833\n",
      "Step: 318, Loss: 0.09994520246982574\n",
      "Step: 319, Loss: 0.09952209144830704\n",
      "Step: 320, Loss: 0.09948418289422989\n",
      "Step: 321, Loss: 0.09951978921890259\n",
      "Step: 322, Loss: 0.09943463653326035\n",
      "Step: 323, Loss: 0.09984159469604492\n",
      "Step: 324, Loss: 0.09962151199579239\n",
      "Step: 325, Loss: 0.0996653139591217\n",
      "Step: 326, Loss: 0.09927761554718018\n",
      "Step: 327, Loss: 0.09962236881256104\n",
      "Step: 328, Loss: 0.0997888371348381\n",
      "Step: 329, Loss: 0.09921903908252716\n",
      "Step: 330, Loss: 0.10006149858236313\n",
      "Step: 331, Loss: 0.0994364321231842\n",
      "Step: 332, Loss: 0.10026399791240692\n",
      "Step: 333, Loss: 0.09971509873867035\n",
      "Step: 334, Loss: 0.10039117187261581\n",
      "Step: 335, Loss: 0.09976818412542343\n",
      "Step: 336, Loss: 0.09957128018140793\n",
      "Step: 337, Loss: 0.10015113651752472\n",
      "Step: 338, Loss: 0.10032059252262115\n",
      "Step: 339, Loss: 0.09994091838598251\n",
      "Step: 340, Loss: 0.10010998696088791\n",
      "Step: 341, Loss: 0.09978180378675461\n",
      "Step: 342, Loss: 0.09978150576353073\n",
      "Step: 343, Loss: 0.10012223571538925\n",
      "Step: 344, Loss: 0.09989391267299652\n",
      "Step: 345, Loss: 0.09989049285650253\n",
      "Step: 346, Loss: 0.10021966695785522\n",
      "Step: 347, Loss: 0.10012882947921753\n",
      "Step: 348, Loss: 0.09990854561328888\n",
      "Step: 349, Loss: 0.09975525736808777\n",
      "Step: 350, Loss: 0.10012611001729965\n",
      "Step: 351, Loss: 0.09984101355075836\n",
      "Step: 352, Loss: 0.09986074268817902\n",
      "Step: 353, Loss: 0.09975353628396988\n",
      "Step: 354, Loss: 0.09982657432556152\n",
      "Step: 355, Loss: 0.09984082728624344\n",
      "Step: 356, Loss: 0.10024143010377884\n",
      "Step: 357, Loss: 0.10007154941558838\n",
      "Step: 358, Loss: 0.10003611445426941\n",
      "Step: 359, Loss: 0.10013946890830994\n",
      "Step: 360, Loss: 0.10002493113279343\n",
      "Step: 361, Loss: 0.10018520057201385\n",
      "Step: 362, Loss: 0.09982316195964813\n",
      "Step: 363, Loss: 0.1001284196972847\n",
      "Step: 364, Loss: 0.10002555698156357\n",
      "Step: 365, Loss: 0.09970288723707199\n",
      "Step: 366, Loss: 0.09989278018474579\n",
      "Step: 367, Loss: 0.1003192737698555\n",
      "Step: 368, Loss: 0.10013553500175476\n",
      "Step: 369, Loss: 0.09986501932144165\n",
      "Step: 370, Loss: 0.10005918145179749\n",
      "Step: 371, Loss: 0.10015542060136795\n",
      "Step: 372, Loss: 0.09939827024936676\n",
      "Step: 373, Loss: 0.10003519803285599\n",
      "Step: 374, Loss: 0.0998372957110405\n",
      "Step: 375, Loss: 0.10035349428653717\n",
      "Step: 376, Loss: 0.09981882572174072\n",
      "Step: 377, Loss: 0.09971339255571365\n",
      "Step: 378, Loss: 0.09950613230466843\n",
      "Step: 379, Loss: 0.0999230444431305\n",
      "Step: 380, Loss: 0.10002359747886658\n",
      "Step: 381, Loss: 0.10012052953243256\n",
      "Step: 382, Loss: 0.10015081614255905\n",
      "Step: 383, Loss: 0.09993571788072586\n",
      "Step: 384, Loss: 0.10028791427612305\n",
      "Step: 385, Loss: 0.09959328919649124\n",
      "Step: 386, Loss: 0.09993324428796768\n",
      "Step: 387, Loss: 0.09984131157398224\n",
      "Step: 388, Loss: 0.09991424530744553\n",
      "Step: 389, Loss: 0.10016492754220963\n",
      "Step: 390, Loss: 0.09998331218957901\n",
      "Step: 391, Loss: 0.09986256062984467\n",
      "Step: 392, Loss: 0.09996448457241058\n",
      "Step: 393, Loss: 0.09943445771932602\n",
      "Step: 394, Loss: 0.09974827617406845\n",
      "Step: 395, Loss: 0.10029754787683487\n",
      "Step: 396, Loss: 0.10010799765586853\n",
      "Step: 397, Loss: 0.09959585964679718\n",
      "Step: 398, Loss: 0.09976475685834885\n",
      "Step: 399, Loss: 0.09986741095781326\n",
      "Step: 400, Loss: 0.09970524162054062\n",
      "Step: 401, Loss: 0.09987582266330719\n",
      "Step: 402, Loss: 0.09949187934398651\n",
      "Step: 403, Loss: 0.09993467479944229\n",
      "Step: 404, Loss: 0.10013909637928009\n",
      "Step: 405, Loss: 0.09980069845914841\n",
      "Step: 406, Loss: 0.10038300603628159\n",
      "Step: 407, Loss: 0.09998966008424759\n",
      "Step: 408, Loss: 0.09987952560186386\n",
      "Step: 409, Loss: 0.0997140109539032\n",
      "Step: 410, Loss: 0.09966640919446945\n",
      "Step: 411, Loss: 0.09958469867706299\n",
      "Step: 412, Loss: 0.10002169013023376\n",
      "Step: 413, Loss: 0.09982061386108398\n",
      "Step: 414, Loss: 0.09994416683912277\n",
      "Step: 415, Loss: 0.09970349818468094\n",
      "Step: 416, Loss: 0.10004322975873947\n",
      "Step: 417, Loss: 0.0996941328048706\n",
      "Step: 418, Loss: 0.09992799162864685\n",
      "Step: 419, Loss: 0.10003471374511719\n",
      "Step: 420, Loss: 0.09996713697910309\n",
      "Step: 421, Loss: 0.09973790496587753\n",
      "Step: 422, Loss: 0.09977637976408005\n",
      "Step: 423, Loss: 0.10000914335250854\n",
      "Step: 424, Loss: 0.10004518181085587\n",
      "Step: 425, Loss: 0.10023046284914017\n",
      "Step: 426, Loss: 0.09995432198047638\n",
      "Step: 427, Loss: 0.09925954788923264\n",
      "Step: 428, Loss: 0.09991728514432907\n",
      "Step: 429, Loss: 0.09946966916322708\n",
      "Step: 430, Loss: 0.09996330738067627\n",
      "Step: 431, Loss: 0.09970986098051071\n",
      "Step: 432, Loss: 0.09970537573099136\n",
      "Step: 433, Loss: 0.10017500817775726\n",
      "Step: 434, Loss: 0.0995701476931572\n",
      "Step: 435, Loss: 0.09996020793914795\n",
      "Step: 436, Loss: 0.09986601769924164\n",
      "Step: 437, Loss: 0.09947854280471802\n",
      "Step: 438, Loss: 0.09946966916322708\n",
      "Step: 439, Loss: 0.09941795468330383\n",
      "Step: 440, Loss: 0.0998176634311676\n",
      "Step: 441, Loss: 0.0996798649430275\n",
      "Step: 442, Loss: 0.0996241644024849\n",
      "Step: 443, Loss: 0.09985533356666565\n",
      "Step: 444, Loss: 0.09977034479379654\n",
      "Step: 445, Loss: 0.09954078495502472\n",
      "Step: 446, Loss: 0.09973353892564774\n",
      "Step: 447, Loss: 0.09921430796384811\n",
      "Step: 448, Loss: 0.09962459653615952\n",
      "Step: 449, Loss: 0.09963539242744446\n",
      "Step: 450, Loss: 0.09927268326282501\n",
      "Step: 451, Loss: 0.09973824769258499\n",
      "Step: 452, Loss: 0.10009041428565979\n",
      "Step: 453, Loss: 0.09966740012168884\n",
      "Step: 454, Loss: 0.10032093524932861\n",
      "Step: 455, Loss: 0.10001173615455627\n",
      "Step: 456, Loss: 0.09977711737155914\n",
      "Step: 457, Loss: 0.09990888833999634\n",
      "Step: 458, Loss: 0.10095333307981491\n",
      "Step: 459, Loss: 0.10029976814985275\n",
      "Step: 460, Loss: 0.10055486112833023\n",
      "Step: 461, Loss: 0.100510373711586\n",
      "Step: 462, Loss: 0.1001894623041153\n",
      "Step: 463, Loss: 0.1000116840004921\n",
      "Step: 464, Loss: 0.10017205774784088\n",
      "Step: 465, Loss: 0.09987734258174896\n",
      "Step: 466, Loss: 0.09993784874677658\n",
      "Step: 467, Loss: 0.10050167143344879\n",
      "Step: 468, Loss: 0.10080045461654663\n",
      "Step: 469, Loss: 0.10109151899814606\n",
      "Step: 470, Loss: 0.10154446959495544\n",
      "Step: 471, Loss: 0.101442851126194\n",
      "Step: 472, Loss: 0.10067947208881378\n",
      "Step: 473, Loss: 0.10112825036048889\n",
      "Step: 474, Loss: 0.10031715780496597\n",
      "Step: 475, Loss: 0.10064329952001572\n",
      "Step: 476, Loss: 0.1008189469575882\n",
      "Step: 477, Loss: 0.1007021814584732\n",
      "Step: 478, Loss: 0.10026750713586807\n",
      "Step: 479, Loss: 0.10025402158498764\n",
      "Step: 480, Loss: 0.10001331567764282\n",
      "Step: 481, Loss: 0.10018754005432129\n",
      "Step: 482, Loss: 0.10039862245321274\n",
      "Step: 483, Loss: 0.10031452029943466\n",
      "Step: 484, Loss: 0.10024953633546829\n",
      "Step: 485, Loss: 0.10004188120365143\n",
      "Step: 486, Loss: 0.09993920475244522\n",
      "Step: 487, Loss: 0.10009193420410156\n",
      "Step: 488, Loss: 0.09955208003520966\n",
      "Step: 489, Loss: 0.09983590990304947\n",
      "Step: 490, Loss: 0.10010526329278946\n",
      "Step: 491, Loss: 0.09993577003479004\n",
      "Step: 492, Loss: 0.10001025348901749\n",
      "Step: 493, Loss: 0.09999268501996994\n",
      "Step: 494, Loss: 0.1001076027750969\n",
      "Step: 495, Loss: 0.09974785894155502\n",
      "Step: 496, Loss: 0.10024365037679672\n",
      "Step: 497, Loss: 0.09986631572246552\n",
      "Step: 498, Loss: 0.09976784884929657\n",
      "Step: 499, Loss: 0.10009763389825821\n",
      "Step: 500, Loss: 0.0998651534318924\n",
      "Step: 501, Loss: 0.0996825098991394\n",
      "Step: 502, Loss: 0.0994672030210495\n",
      "Step: 503, Loss: 0.09935992956161499\n",
      "Step: 504, Loss: 0.09979315102100372\n",
      "Step: 505, Loss: 0.09958283603191376\n",
      "Step: 506, Loss: 0.09996163845062256\n",
      "Step: 507, Loss: 0.09993566572666168\n",
      "Step: 508, Loss: 0.1003762036561966\n",
      "Step: 509, Loss: 0.10008284449577332\n",
      "Step: 510, Loss: 0.09933522343635559\n",
      "Step: 511, Loss: 0.0995444506406784\n",
      "Step: 512, Loss: 0.10031702369451523\n",
      "Step: 513, Loss: 0.09976620972156525\n",
      "Step: 514, Loss: 0.09976449608802795\n",
      "Step: 515, Loss: 0.0998372808098793\n",
      "Step: 516, Loss: 0.0995604619383812\n",
      "Step: 517, Loss: 0.09973584115505219\n",
      "Step: 518, Loss: 0.09963072091341019\n",
      "Step: 519, Loss: 0.09962606430053711\n",
      "Step: 520, Loss: 0.09978165477514267\n",
      "Step: 521, Loss: 0.0993596613407135\n",
      "Step: 522, Loss: 0.09998148679733276\n",
      "Step: 523, Loss: 0.1000947579741478\n",
      "Step: 524, Loss: 0.09979057312011719\n",
      "Step: 525, Loss: 0.1000363752245903\n",
      "Step: 526, Loss: 0.099674291908741\n",
      "Step: 527, Loss: 0.09972982108592987\n",
      "Step: 528, Loss: 0.09998524188995361\n",
      "Step: 529, Loss: 0.09960104525089264\n",
      "Step: 530, Loss: 0.09983497858047485\n",
      "Step: 531, Loss: 0.09992524981498718\n",
      "Step: 532, Loss: 0.09966707229614258\n",
      "Step: 533, Loss: 0.09958945959806442\n",
      "Step: 534, Loss: 0.09980438649654388\n",
      "Step: 535, Loss: 0.0994650349020958\n",
      "Step: 536, Loss: 0.09983722120523453\n",
      "Step: 537, Loss: 0.09963468462228775\n",
      "Step: 538, Loss: 0.09971854090690613\n",
      "Step: 539, Loss: 0.09986277669668198\n",
      "Step: 540, Loss: 0.09945572912693024\n",
      "Step: 541, Loss: 0.10004832595586777\n",
      "Step: 542, Loss: 0.09977001696825027\n",
      "Step: 543, Loss: 0.09960256516933441\n",
      "Step: 544, Loss: 0.09927938133478165\n",
      "Step: 545, Loss: 0.09926994144916534\n",
      "Step: 546, Loss: 0.09978073835372925\n",
      "Step: 547, Loss: 0.09961432218551636\n",
      "Step: 548, Loss: 0.09936332702636719\n",
      "Step: 549, Loss: 0.09980623424053192\n",
      "Step: 550, Loss: 0.09983431547880173\n",
      "Step: 551, Loss: 0.09938172250986099\n",
      "Step: 552, Loss: 0.09943331778049469\n",
      "Step: 553, Loss: 0.09966979175806046\n",
      "Step: 554, Loss: 0.09966657310724258\n",
      "Step: 555, Loss: 0.09943845868110657\n",
      "Step: 556, Loss: 0.09954114258289337\n",
      "Step: 557, Loss: 0.09956977516412735\n",
      "Step: 558, Loss: 0.09957275539636612\n",
      "Step: 559, Loss: 0.09975379705429077\n",
      "Step: 560, Loss: 0.09955392777919769\n",
      "Step: 561, Loss: 0.09936036169528961\n",
      "Step: 562, Loss: 0.09943887591362\n",
      "Step: 563, Loss: 0.09995297342538834\n",
      "Step: 564, Loss: 0.0995626151561737\n",
      "Step: 565, Loss: 0.09926672279834747\n",
      "Step: 566, Loss: 0.09947508573532104\n",
      "Step: 567, Loss: 0.09905603528022766\n",
      "Step: 568, Loss: 0.09952205419540405\n",
      "Step: 569, Loss: 0.09928470104932785\n",
      "Step: 570, Loss: 0.09992332756519318\n",
      "Step: 571, Loss: 0.09943296760320663\n",
      "Step: 572, Loss: 0.09956399351358414\n",
      "Step: 573, Loss: 0.09931079298257828\n",
      "Step: 574, Loss: 0.09973365068435669\n",
      "Step: 575, Loss: 0.0994490459561348\n",
      "Step: 576, Loss: 0.09972744435071945\n",
      "Step: 577, Loss: 0.09966512769460678\n",
      "Step: 578, Loss: 0.09981945157051086\n",
      "Step: 579, Loss: 0.0996147096157074\n",
      "Step: 580, Loss: 0.09937168657779694\n",
      "Step: 581, Loss: 0.09961981326341629\n",
      "Step: 582, Loss: 0.09938687086105347\n",
      "Step: 583, Loss: 0.09917938709259033\n",
      "Step: 584, Loss: 0.09949074685573578\n",
      "Step: 585, Loss: 0.09972810000181198\n",
      "Step: 586, Loss: 0.09952537715435028\n",
      "Step: 587, Loss: 0.09971737116575241\n",
      "Step: 588, Loss: 0.09962929040193558\n",
      "Step: 589, Loss: 0.09988339990377426\n",
      "Step: 590, Loss: 0.09978655725717545\n",
      "Step: 591, Loss: 0.09918398410081863\n",
      "Step: 592, Loss: 0.09979411959648132\n",
      "Step: 593, Loss: 0.09905647486448288\n",
      "Step: 594, Loss: 0.09952230006456375\n",
      "Step: 595, Loss: 0.09919173270463943\n",
      "Step: 596, Loss: 0.09927141666412354\n",
      "Step: 597, Loss: 0.09943622350692749\n",
      "Step: 598, Loss: 0.09921099990606308\n",
      "Step: 599, Loss: 0.09971991926431656\n",
      "Final loss:  0.09971992\n",
      "Model saved as /net/pc200239/nobackup/users/hakvoort/models/emos/mixture_linear/mixturelinear_tn_gev_twcrps_mean13.0_std4.0_epochs600_folds_1_3.pkl\n"
     ]
    }
   ],
   "source": [
    "chain_function_std = 4\n",
    "\n",
    "model = train_and_save(\n",
    "    forecast_distribution,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    folds,\n",
    "    parameter_names,\n",
    "    neighbourhood_size,\n",
    "    ignore,\n",
    "    epochs,\n",
    "\n",
    "    chain_function = chain_function,\n",
    "    chain_function_mean = chain_function_mean,\n",
    "    chain_function_std = chain_function_std,\n",
    "    chain_function_constant = chain_function_constant,\n",
    "    chain_function_threshold = chain_function_threshold,\n",
    "    samples = samples,\n",
    "    printing = printing,\n",
    "    distribution_1 = distribution_1,\n",
    "    distribution_2 = distribution_2,\n",
    "    pretrained = pretrained\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
